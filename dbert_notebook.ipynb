{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dbert_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ae0f63f7cbf6432aa4c436d1d440f8ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_20210c34f8834eafa7c36cb84ee0f5cb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78f8848a2ebe4da5ac40ed9b9220f225",
              "IPY_MODEL_63f82ebe69774bdcb1eee57c8f2f37a9",
              "IPY_MODEL_10baa54a5e664453bfd3f57084cf30c0"
            ]
          }
        },
        "20210c34f8834eafa7c36cb84ee0f5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78f8848a2ebe4da5ac40ed9b9220f225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ccb17741cf4749cab4c7e1b62225e15e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80726f1a87f6485c9fc16635580bb610"
          }
        },
        "63f82ebe69774bdcb1eee57c8f2f37a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f552c9fdfbd43e0acd62a05b4554891",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1093,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1093,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40a6ef0067f543adbe44ae7007457a33"
          }
        },
        "10baa54a5e664453bfd3f57084cf30c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b916d1020fff488facc75cdaf5a74643",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.09k/1.09k [00:00&lt;00:00, 26.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0608d49126e4ebda53c16dcef64ff99"
          }
        },
        "ccb17741cf4749cab4c7e1b62225e15e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80726f1a87f6485c9fc16635580bb610": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f552c9fdfbd43e0acd62a05b4554891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40a6ef0067f543adbe44ae7007457a33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b916d1020fff488facc75cdaf5a74643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0608d49126e4ebda53c16dcef64ff99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96cc6d4217034eb7a9fe84a000c81ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_006923490ad349a1a8d0311cf7deba1f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd23d87c73db4868ad5710efaad75231",
              "IPY_MODEL_7a1eccd49c9f48b191d9baca9418428a",
              "IPY_MODEL_27bbe050897b47138b1cfb6a9dd3cfdf"
            ]
          }
        },
        "006923490ad349a1a8d0311cf7deba1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd23d87c73db4868ad5710efaad75231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8abb61000e504bfeb439cf5c8ea5d221",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ef8500333bb4677a2d43347a4910c20"
          }
        },
        "7a1eccd49c9f48b191d9baca9418428a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ced4df5388974466878b1f79b5056626",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3680,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3680,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_87687f98b1214baa98fe93d65e62e224"
          }
        },
        "27bbe050897b47138b1cfb6a9dd3cfdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c03b138aea4e45e4bb995c916c71047c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3.68k/3.68k [00:00&lt;00:00, 95.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a7ecee154b9b44b3b08af0a9ab8c7255"
          }
        },
        "8abb61000e504bfeb439cf5c8ea5d221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ef8500333bb4677a2d43347a4910c20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ced4df5388974466878b1f79b5056626": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "87687f98b1214baa98fe93d65e62e224": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c03b138aea4e45e4bb995c916c71047c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a7ecee154b9b44b3b08af0a9ab8c7255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c2319f78d39e42618491b671bc7eae9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7ba714850e7144b19e1ca2bc3b2a7939",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a17d76c810ab4d76b3ed8750b66b10a4",
              "IPY_MODEL_5bbe6544005e441daa87e0c9059214a6",
              "IPY_MODEL_cb83423d60e74014b71e360756567735"
            ]
          }
        },
        "7ba714850e7144b19e1ca2bc3b2a7939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a17d76c810ab4d76b3ed8750b66b10a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_27ebc7bc118d43c68a5e9f7a5d9db4ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9824e1a90ff148af8582a4f0b908be04"
          }
        },
        "5bbe6544005e441daa87e0c9059214a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0e4043967de84adab1a90ed5fdc16b9f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 675,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 675,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_90f07fa7da47483b859eaab196f6e277"
          }
        },
        "cb83423d60e74014b71e360756567735": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_58c462ff16ed4502bbdb9c3b899a52fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 675/675 [00:00&lt;00:00, 16.1kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da381c36219149cea2edc95b46ad194a"
          }
        },
        "27ebc7bc118d43c68a5e9f7a5d9db4ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9824e1a90ff148af8582a4f0b908be04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e4043967de84adab1a90ed5fdc16b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "90f07fa7da47483b859eaab196f6e277": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58c462ff16ed4502bbdb9c3b899a52fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da381c36219149cea2edc95b46ad194a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da31ccbe0e274631899c25ab9f30828b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e4490056ad1d4e59a723abaf4d315ce6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e9b855d3496b4ca7860aa82f0dee847b",
              "IPY_MODEL_593e3d037ddb4e4587d069f98f834455",
              "IPY_MODEL_d278792dc34d41fe82bbc21936577a8e"
            ]
          }
        },
        "e4490056ad1d4e59a723abaf4d315ce6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e9b855d3496b4ca7860aa82f0dee847b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_58578eb8622a4deeb364dbdc86043f55",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6eb79bf51b8a4ca5af3fa4964f80c3d7"
          }
        },
        "593e3d037ddb4e4587d069f98f834455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_68cf032fc98d4d50ab988fd32076c756",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 122,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 122,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_93267d2b323c45319d11f0ca926ff937"
          }
        },
        "d278792dc34d41fe82bbc21936577a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_49ce014bd3ad4fcdad86e1395add951e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 122/122 [00:00&lt;00:00, 3.29kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1027a2258544fda9e32cc3cb8ed0d47"
          }
        },
        "58578eb8622a4deeb364dbdc86043f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6eb79bf51b8a4ca5af3fa4964f80c3d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68cf032fc98d4d50ab988fd32076c756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "93267d2b323c45319d11f0ca926ff937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "49ce014bd3ad4fcdad86e1395add951e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1027a2258544fda9e32cc3cb8ed0d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84a927dcf24a4b8aae89f238a725f69f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5bb3f3e07e7b4c23b888bf69253954f3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_57da66a6d9e14bbc93fd29038fdd596e",
              "IPY_MODEL_5db3c1834b14452aadd1e166ff34ef7e",
              "IPY_MODEL_80b3d9a0101c4f2ba678e4b2f2ddd3a9"
            ]
          }
        },
        "5bb3f3e07e7b4c23b888bf69253954f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57da66a6d9e14bbc93fd29038fdd596e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_199060989a3443b48be5a36d71ddf4a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_36b27741563d4ed797c659fb3fd57ab7"
          }
        },
        "5db3c1834b14452aadd1e166ff34ef7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_de30919ce8574fbb929b426b5c7232c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456356,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456356,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b26e130d77874b9097c9b7d180f390f6"
          }
        },
        "80b3d9a0101c4f2ba678e4b2f2ddd3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_92190cf8eb3547438e9dc253954b893a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 601kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_09e70e5161aa44609c0e2913464d121d"
          }
        },
        "199060989a3443b48be5a36d71ddf4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "36b27741563d4ed797c659fb3fd57ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de30919ce8574fbb929b426b5c7232c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b26e130d77874b9097c9b7d180f390f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "92190cf8eb3547438e9dc253954b893a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "09e70e5161aa44609c0e2913464d121d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd508c97105e42ffbf80e591d89c8e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_37c83327e4214601a3c12eaa51d1f771",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3d85e05853834207b5d15f17f65f962f",
              "IPY_MODEL_a5a7d4d5aecd4c64b1202211af77826d",
              "IPY_MODEL_3e5c2cfdc36c4fc78c011009a2571c65"
            ]
          }
        },
        "37c83327e4214601a3c12eaa51d1f771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d85e05853834207b5d15f17f65f962f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_001d04146a664567bd83cb66c0e7f9f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e96357656d174a03a5d2815f1cc048fb"
          }
        },
        "a5a7d4d5aecd4c64b1202211af77826d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_764512918ca34142b16772137220164e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 229,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 229,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9543d14567cf4db4bf955e10139dbb87"
          }
        },
        "3e5c2cfdc36c4fc78c011009a2571c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c632329b557c4fd59e064015cf1904eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 229/229 [00:00&lt;00:00, 5.05kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64291652dd004be7bf4fa110f0847d4e"
          }
        },
        "001d04146a664567bd83cb66c0e7f9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e96357656d174a03a5d2815f1cc048fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "764512918ca34142b16772137220164e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9543d14567cf4db4bf955e10139dbb87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c632329b557c4fd59e064015cf1904eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64291652dd004be7bf4fa110f0847d4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d77c605dbbe46ec9df8caaace913e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_daf1e5b9ba844d21b844503d211676bd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b270539fe0144fc7a393e5144b997b8c",
              "IPY_MODEL_0f4f1c77fc6143eeaa1583b5eac354dc",
              "IPY_MODEL_77227f02a8974b3dbb630d4793d5a901"
            ]
          }
        },
        "daf1e5b9ba844d21b844503d211676bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b270539fe0144fc7a393e5144b997b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9783e8b5f9b14b17a06acb463862f559",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4645ade2ad184106ad15c191830137e0"
          }
        },
        "0f4f1c77fc6143eeaa1583b5eac354dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f369d0e5d0e4459f9d1e2cab7e1eb224",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 498661169,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 498661169,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48e6cc4237ae4d1997e469d40333d13c"
          }
        },
        "77227f02a8974b3dbb630d4793d5a901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_283613c0584343a1bb0c8c3cd1877819",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 499M/499M [00:14&lt;00:00, 32.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12b036ae7d6a46d6bb96eda0c97a7b12"
          }
        },
        "9783e8b5f9b14b17a06acb463862f559": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4645ade2ad184106ad15c191830137e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f369d0e5d0e4459f9d1e2cab7e1eb224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48e6cc4237ae4d1997e469d40333d13c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "283613c0584343a1bb0c8c3cd1877819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12b036ae7d6a46d6bb96eda0c97a7b12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a63b14186a849f9a19e726351bd34a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7a836be512454b09af8f0dcc79ae07c7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d61db2fadc92447c8798eb4d1f4d9a0e",
              "IPY_MODEL_e3efe658016945099cea52a939ea92ac",
              "IPY_MODEL_9e33dcffdc664b7e8253e07a03cc5ffc"
            ]
          }
        },
        "7a836be512454b09af8f0dcc79ae07c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d61db2fadc92447c8798eb4d1f4d9a0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2224d00f46df4bd5abc5b00b7332e755",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac73493c9db5467e924e1671efc38730"
          }
        },
        "e3efe658016945099cea52a939ea92ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_be7019af8de64458adf121ab46a24141",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 52,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 52,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_73d861a1d11d4394874705e54b02193b"
          }
        },
        "9e33dcffdc664b7e8253e07a03cc5ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bcd069d45ad542c384096d555f0dbb0d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 52.0/52.0 [00:00&lt;00:00, 1.27kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_be6c7dbd554b45489d45513db62d5882"
          }
        },
        "2224d00f46df4bd5abc5b00b7332e755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac73493c9db5467e924e1671efc38730": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "be7019af8de64458adf121ab46a24141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "73d861a1d11d4394874705e54b02193b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bcd069d45ad542c384096d555f0dbb0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "be6c7dbd554b45489d45513db62d5882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8641d3b3bb2641d4b3338889975c07f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_320de4f2a17d4f348a37552fd8325049",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1cadc353bd7547ffb9960bd700a003b1",
              "IPY_MODEL_4c6653325e634339b85740c15475ca3f",
              "IPY_MODEL_d4822b90f7284f47993664a7d8635345"
            ]
          }
        },
        "320de4f2a17d4f348a37552fd8325049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cadc353bd7547ffb9960bd700a003b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_74ebc510d1c948b79c090bdec0dd3908",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f3411a9d0b54e2a8c8fcb9e692bc27b"
          }
        },
        "4c6653325e634339b85740c15475ca3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9fcdcde4c204954988f80c6ba380e35",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 239,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 239,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d74815e624649f8851caf3e0445da16"
          }
        },
        "d4822b90f7284f47993664a7d8635345": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40b0feb6514c464bae863f13fe41ebd3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 239/239 [00:00&lt;00:00, 3.77kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a0d651cdddf41f99aa208cbcccea025"
          }
        },
        "74ebc510d1c948b79c090bdec0dd3908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f3411a9d0b54e2a8c8fcb9e692bc27b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9fcdcde4c204954988f80c6ba380e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d74815e624649f8851caf3e0445da16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40b0feb6514c464bae863f13fe41ebd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a0d651cdddf41f99aa208cbcccea025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "afae024ad4a744f086c279bc0addfd7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_832b093a33604df69d9c017c165a876b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bf5a8f687f284006b58e8f1e33df9a5d",
              "IPY_MODEL_e2ae3734a37b4308b2fc8120909a6a1f",
              "IPY_MODEL_4b5a51161adc490280feb4392d37e0b4"
            ]
          }
        },
        "832b093a33604df69d9c017c165a876b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf5a8f687f284006b58e8f1e33df9a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b4c5fcfc14ac44ad8a85199db80d6d12",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f09658b1301047159483c944686e17f5"
          }
        },
        "e2ae3734a37b4308b2fc8120909a6a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3e6fde3167ad42e1aaafad4d3d40a210",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8192e0e7f4545bc95255fca3368bcc1"
          }
        },
        "4b5a51161adc490280feb4392d37e0b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_63406b79e4c448fc98148bcc2ea8d8e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:01&lt;00:00, 1.14MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2bece2e471d24feca1f133acf5c8f569"
          }
        },
        "b4c5fcfc14ac44ad8a85199db80d6d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f09658b1301047159483c944686e17f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e6fde3167ad42e1aaafad4d3d40a210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8192e0e7f4545bc95255fca3368bcc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63406b79e4c448fc98148bcc2ea8d8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2bece2e471d24feca1f133acf5c8f569": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1dbeeb82765b4453affbea67c74df59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_89f1ff5f853f4d85bc26419ecf46fc2c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2053df994e81408d9b134985fa7b372c",
              "IPY_MODEL_51e93537334345db8968773f5679aa37",
              "IPY_MODEL_703641060cc944d3a288241cc5cedbcc"
            ]
          }
        },
        "89f1ff5f853f4d85bc26419ecf46fc2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2053df994e81408d9b134985fa7b372c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7171fd8979b4446abe0dd978bf69594d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9809270c8dd74fcda186ddb4f6244733"
          }
        },
        "51e93537334345db8968773f5679aa37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1e7f85c569ee459298da60f8ec67982e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1111,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1111,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b2d3d7d117424fe79119c32bcaf3d655"
          }
        },
        "703641060cc944d3a288241cc5cedbcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db1f1d81f4b54815ac88c501c31b16fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.11k/1.11k [00:00&lt;00:00, 32.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_551b8ccb3a864fa8bc1d53ffe46effac"
          }
        },
        "7171fd8979b4446abe0dd978bf69594d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9809270c8dd74fcda186ddb4f6244733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e7f85c569ee459298da60f8ec67982e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b2d3d7d117424fe79119c32bcaf3d655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db1f1d81f4b54815ac88c501c31b16fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "551b8ccb3a864fa8bc1d53ffe46effac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f85d493f1a947c6a56222df951fdb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0f4a8bdefcd84cc6b65247909e5cb33b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f9a145b63ced456881ff4f1a64ef0980",
              "IPY_MODEL_52e442d79f614e6eb1faa04ec306ecef",
              "IPY_MODEL_13891a4ddd62434990cfc419aa3a9b5c"
            ]
          }
        },
        "0f4a8bdefcd84cc6b65247909e5cb33b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9a145b63ced456881ff4f1a64ef0980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_090d7b49d80e442196f237df8dc622a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_46dcf72c55044a47b401dc5eec5b733e"
          }
        },
        "52e442d79f614e6eb1faa04ec306ecef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d95876ac6be04c2e9818a384d5257d97",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798293,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798293,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99cebf859d724f408d92ae681d5c2f41"
          }
        },
        "13891a4ddd62434990cfc419aa3a9b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e5489964195e4894bc50ed99b17a161b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:00&lt;00:00, 749kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0d60cb8487f845ebae2855b008ed5015"
          }
        },
        "090d7b49d80e442196f237df8dc622a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "46dcf72c55044a47b401dc5eec5b733e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d95876ac6be04c2e9818a384d5257d97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99cebf859d724f408d92ae681d5c2f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5489964195e4894bc50ed99b17a161b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0d60cb8487f845ebae2855b008ed5015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25e91c8b969f45088138ef39dea908d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_af49cfdddf2c45028697f92382ee2c06",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_69a2f9b21654407f9c98811c20cfa97d",
              "IPY_MODEL_bccf27a412b84bfebe65c109cf33967d",
              "IPY_MODEL_35699d86094a4628a324f8ba63d491d2"
            ]
          }
        },
        "af49cfdddf2c45028697f92382ee2c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69a2f9b21654407f9c98811c20cfa97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7b17a2e6c14f4972891388492220fbdd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_970238362b3b48838db9a7a36acb4f66"
          }
        },
        "bccf27a412b84bfebe65c109cf33967d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c6df3c290ea14076bf65926e4d903388",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 190,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 190,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a45a0d3e36a54077a274cf524cfdf3f2"
          }
        },
        "35699d86094a4628a324f8ba63d491d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0e4c87d449847c7b114f0450356fb28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 190/190 [00:00&lt;00:00, 4.81kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed1b546f3005493b948dd46b74d4ea9e"
          }
        },
        "7b17a2e6c14f4972891388492220fbdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "970238362b3b48838db9a7a36acb4f66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6df3c290ea14076bf65926e4d903388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a45a0d3e36a54077a274cf524cfdf3f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e0e4c87d449847c7b114f0450356fb28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed1b546f3005493b948dd46b74d4ea9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b98111bd8d848bcae531df6f1af8716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e867661976ba40c58797969a55a960f9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a73a028a8f8b4ed3bebd42560d5a7228",
              "IPY_MODEL_d8925b99ff1445f2a7f7063eda8eebc3",
              "IPY_MODEL_b51e47938b744776b795cfd0943d30b9"
            ]
          }
        },
        "e867661976ba40c58797969a55a960f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a73a028a8f8b4ed3bebd42560d5a7228": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_244ae4cd74a642a78dcc50109902149b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9179f6a0113842e78d08d8ea4971a5bb"
          }
        },
        "d8925b99ff1445f2a7f7063eda8eebc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6617d9a47ac34f28993a1e2d5d816cb1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 675,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 675,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a6b51f003ab74a838bb60928c20302d3"
          }
        },
        "b51e47938b744776b795cfd0943d30b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2f34dffb9f654d3aa3f5b2e8de669071",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 675/675 [00:00&lt;00:00, 16.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c83afd64944445e48cbc76a752435110"
          }
        },
        "244ae4cd74a642a78dcc50109902149b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9179f6a0113842e78d08d8ea4971a5bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6617d9a47ac34f28993a1e2d5d816cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a6b51f003ab74a838bb60928c20302d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f34dffb9f654d3aa3f5b2e8de669071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c83afd64944445e48cbc76a752435110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "80c9f9521bdb46038a6fa33b12fd83bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_543c8f5357934c78a2937429da5b894c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2d5366ddde9a4672a7d3bc2a7cf6b2fb",
              "IPY_MODEL_fd22b73aa75d49acbfd6dcf8f836073b",
              "IPY_MODEL_0b9970a3b8ab494587dd1948bca2ba5a"
            ]
          }
        },
        "543c8f5357934c78a2937429da5b894c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d5366ddde9a4672a7d3bc2a7cf6b2fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b5e2d454b56b4321bc6063dc7fb44c22",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_969d4f70fffe4c6b92bcad5fff78c55b"
          }
        },
        "fd22b73aa75d49acbfd6dcf8f836073b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8a9ae141177445058a83e71acad6691a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 498661169,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 498661169,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e958093c3244618b303b8cd6e3c8cad"
          }
        },
        "0b9970a3b8ab494587dd1948bca2ba5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f505100c1adc48dfbac2714241e008b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 476M/476M [00:14&lt;00:00, 34.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e1d7952a4d4a454e982b1cf15a3d44ce"
          }
        },
        "b5e2d454b56b4321bc6063dc7fb44c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "969d4f70fffe4c6b92bcad5fff78c55b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a9ae141177445058a83e71acad6691a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e958093c3244618b303b8cd6e3c8cad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f505100c1adc48dfbac2714241e008b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e1d7952a4d4a454e982b1cf15a3d44ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56f32b2dbc4943bbbebb21510c8d4a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8e56dc32847e485c99b78e6f66d5ad4d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_16902df9c93e49b4b6815f7eac85e225",
              "IPY_MODEL_bf78fd3340e94cf887565c87c0bd7a11",
              "IPY_MODEL_8c99f6340d6e4a79ad0f881a54d525bb"
            ]
          }
        },
        "8e56dc32847e485c99b78e6f66d5ad4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "16902df9c93e49b4b6815f7eac85e225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0bcce180928245bd9b61658a06aaf332",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5f804d77993451a88a6df7057b65c3e"
          }
        },
        "bf78fd3340e94cf887565c87c0bd7a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_da7065bf89644d529d641cd963048a79",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1111,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1111,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f93c583de0614927ba095e78521ccbc9"
          }
        },
        "8c99f6340d6e4a79ad0f881a54d525bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a1d04e350a5140c0aff97080812cc821",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.08k/1.08k [00:00&lt;00:00, 30.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c6e70a4fc6024bd0b891f490f720bdca"
          }
        },
        "0bcce180928245bd9b61658a06aaf332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5f804d77993451a88a6df7057b65c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da7065bf89644d529d641cd963048a79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f93c583de0614927ba095e78521ccbc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1d04e350a5140c0aff97080812cc821": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c6e70a4fc6024bd0b891f490f720bdca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8761e570a38b4980a7afc0ea660579b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_88dbf5bdc9344decbfd244720c1e3be8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cd8822b4d46448468595bc97faabddff",
              "IPY_MODEL_7967715160564bfa8e7122e1f9d0b210",
              "IPY_MODEL_27ed448c62cd42e0b2dc61dd17c82417"
            ]
          }
        },
        "88dbf5bdc9344decbfd244720c1e3be8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd8822b4d46448468595bc97faabddff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0dcc922cb2954924a0648f58c5c0cc5b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58a8071b87404a7e874591b58ab6849e"
          }
        },
        "7967715160564bfa8e7122e1f9d0b210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a74de6af39014c179b2de7ef8b5416a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798293,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798293,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5d4c4fc147c6499ab845e03b6c63bdab"
          }
        },
        "27ed448c62cd42e0b2dc61dd17c82417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3bd2f24eb7a24f6688b41358d83a4d14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 780k/780k [00:00&lt;00:00, 625kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aaed72870a9247fda69b1f024037564d"
          }
        },
        "0dcc922cb2954924a0648f58c5c0cc5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58a8071b87404a7e874591b58ab6849e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a74de6af39014c179b2de7ef8b5416a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5d4c4fc147c6499ab845e03b6c63bdab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3bd2f24eb7a24f6688b41358d83a4d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aaed72870a9247fda69b1f024037564d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8d00a29e1d7449acb5e41cf06ce653f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d7cdf424c344421ac95d732a1e13891",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_220820e6f04c485eb078503f43f5f908",
              "IPY_MODEL_2917e1169a7143bdb66425b8f0e5f0dc",
              "IPY_MODEL_11d8c12d4a3f459d8b3e14ac768d2408"
            ]
          }
        },
        "1d7cdf424c344421ac95d732a1e13891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "220820e6f04c485eb078503f43f5f908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_48f0492c70e2404c9c117fc7e2a360aa",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48e7eaee3077458da0da039baf37d780"
          }
        },
        "2917e1169a7143bdb66425b8f0e5f0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3cd14bd9c2924e9f86e73d5b4f0fb615",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456356,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456356,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a0a971f03c540e9835ae575314df105"
          }
        },
        "11d8c12d4a3f459d8b3e14ac768d2408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b5148d848514b2da4e50dbdaf43c1e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 651kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_231e6f5fc7e64bffbad5ca00c88ea0a2"
          }
        },
        "48f0492c70e2404c9c117fc7e2a360aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48e7eaee3077458da0da039baf37d780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3cd14bd9c2924e9f86e73d5b4f0fb615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a0a971f03c540e9835ae575314df105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b5148d848514b2da4e50dbdaf43c1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "231e6f5fc7e64bffbad5ca00c88ea0a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a9fc419c8c384de6a1f11a1bdcc8e10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ba51c47f8480434c91cd60351e8b7eb6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e37462679b024153a21bc6e0d47c00ab",
              "IPY_MODEL_6803be8e35084b77892b996c5649edf1",
              "IPY_MODEL_e946ac659cc84088a2b5dc4cb95431c1"
            ]
          }
        },
        "ba51c47f8480434c91cd60351e8b7eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e37462679b024153a21bc6e0d47c00ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_084cadb0c0b246a5ba096d120d6b3ee6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_167b6f30aaa649658f33a60e149ad0e6"
          }
        },
        "6803be8e35084b77892b996c5649edf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7d1118ee3c954172ab7f3d577b4a2c2a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6cc4523ee4614369a31621b0909dd1c1"
          }
        },
        "e946ac659cc84088a2b5dc4cb95431c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40333f539b4c4a228d587904efe31292",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.29M/1.29M [00:01&lt;00:00, 1.27MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bd6044c9d293462990326ac12210fbd4"
          }
        },
        "084cadb0c0b246a5ba096d120d6b3ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "167b6f30aaa649658f33a60e149ad0e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d1118ee3c954172ab7f3d577b4a2c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6cc4523ee4614369a31621b0909dd1c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40333f539b4c4a228d587904efe31292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bd6044c9d293462990326ac12210fbd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6f4ac8a352244bcbb9cd4a4eb00d3a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ffa5069b198d494d932574f3363887e8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab05ce7c9cce407cb47117264e8f124c",
              "IPY_MODEL_c8cc98fe3b714f099959df858f113832",
              "IPY_MODEL_23bb1c8579ed4cb39849631e4468637e"
            ]
          }
        },
        "ffa5069b198d494d932574f3363887e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab05ce7c9cce407cb47117264e8f124c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d8efe82c35794fb2b7aab57300ae7a39",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f0a111a2a0348cc80ca2edfcb235947"
          }
        },
        "c8cc98fe3b714f099959df858f113832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e1e7ac7c2354f72941c749543cb6325",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 239,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 239,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3383710a39b04012b73ccd84dd834c8b"
          }
        },
        "23bb1c8579ed4cb39849631e4468637e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f59b51df31e4f1494f60936398a1bdf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 239/239 [00:00&lt;00:00, 6.06kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_704bc99c93a94dab833f76f30ff837a9"
          }
        },
        "d8efe82c35794fb2b7aab57300ae7a39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f0a111a2a0348cc80ca2edfcb235947": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2e1e7ac7c2354f72941c749543cb6325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3383710a39b04012b73ccd84dd834c8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f59b51df31e4f1494f60936398a1bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "704bc99c93a94dab833f76f30ff837a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaCF0hKHfqQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "976423d2-9d9f-4b2a-a4e2-642b4ac712a5"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentence-transformers\n",
        "!pip install pyyaml h5py tensorflow\n",
        "!pip install ipython-autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.23.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.43.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: ipython-autotime in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipython-autotime) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (57.4.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ipython-autotime) (0.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ipython-autotime) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipython-autotime) (0.7.0)\n",
            "time: 14.2 s (started: 2022-01-18 18:55:36 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Lo5Gv0ud2Sz",
        "outputId": "d2bae074-5919-43b6-ce3f-9e347ff375da"
      },
      "source": [
        "%load_ext autotime\n",
        "%load_ext google.colab.data_table\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "time: 5.22 s (started: 2022-01-18 18:55:50 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAjB68usgiRz",
        "outputId": "9df1d3d3-64c2-4bce-c06b-965abf26daf9"
      },
      "source": [
        "import scipy\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "from torch import nn\n",
        "#model = SentenceTransformer('stsb-roberta-large')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.3 ms (started: 2022-01-18 18:55:55 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ8gO-N7d7WJ"
      },
      "source": [
        "#model.save('/content/drive/My Drive/Colab Notebooks/models/stsb_roberta_large.h5')\n",
        "#in SavedModel format\n",
        "#model.save('/content/drive/My Drive/Colab Notebooks/models/stsb_roberta_large_saved_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this one for pure sbert: implemented for comparison only\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('stsb-roberta-base-v2')\n",
        "print(\"Max Sequence Length:\", model.max_seq_length)\n",
        "model.max_seq_length = 300\n",
        "print(\"Max Sequence Length:\", model.max_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485,
          "referenced_widgets": [
            "ae0f63f7cbf6432aa4c436d1d440f8ac",
            "20210c34f8834eafa7c36cb84ee0f5cb",
            "78f8848a2ebe4da5ac40ed9b9220f225",
            "63f82ebe69774bdcb1eee57c8f2f37a9",
            "10baa54a5e664453bfd3f57084cf30c0",
            "ccb17741cf4749cab4c7e1b62225e15e",
            "80726f1a87f6485c9fc16635580bb610",
            "9f552c9fdfbd43e0acd62a05b4554891",
            "40a6ef0067f543adbe44ae7007457a33",
            "b916d1020fff488facc75cdaf5a74643",
            "a0608d49126e4ebda53c16dcef64ff99",
            "96cc6d4217034eb7a9fe84a000c81ccf",
            "006923490ad349a1a8d0311cf7deba1f",
            "cd23d87c73db4868ad5710efaad75231",
            "7a1eccd49c9f48b191d9baca9418428a",
            "27bbe050897b47138b1cfb6a9dd3cfdf",
            "8abb61000e504bfeb439cf5c8ea5d221",
            "2ef8500333bb4677a2d43347a4910c20",
            "ced4df5388974466878b1f79b5056626",
            "87687f98b1214baa98fe93d65e62e224",
            "c03b138aea4e45e4bb995c916c71047c",
            "a7ecee154b9b44b3b08af0a9ab8c7255",
            "c2319f78d39e42618491b671bc7eae9e",
            "7ba714850e7144b19e1ca2bc3b2a7939",
            "a17d76c810ab4d76b3ed8750b66b10a4",
            "5bbe6544005e441daa87e0c9059214a6",
            "cb83423d60e74014b71e360756567735",
            "27ebc7bc118d43c68a5e9f7a5d9db4ed",
            "9824e1a90ff148af8582a4f0b908be04",
            "0e4043967de84adab1a90ed5fdc16b9f",
            "90f07fa7da47483b859eaab196f6e277",
            "58c462ff16ed4502bbdb9c3b899a52fb",
            "da381c36219149cea2edc95b46ad194a",
            "da31ccbe0e274631899c25ab9f30828b",
            "e4490056ad1d4e59a723abaf4d315ce6",
            "e9b855d3496b4ca7860aa82f0dee847b",
            "593e3d037ddb4e4587d069f98f834455",
            "d278792dc34d41fe82bbc21936577a8e",
            "58578eb8622a4deeb364dbdc86043f55",
            "6eb79bf51b8a4ca5af3fa4964f80c3d7",
            "68cf032fc98d4d50ab988fd32076c756",
            "93267d2b323c45319d11f0ca926ff937",
            "49ce014bd3ad4fcdad86e1395add951e",
            "b1027a2258544fda9e32cc3cb8ed0d47",
            "84a927dcf24a4b8aae89f238a725f69f",
            "5bb3f3e07e7b4c23b888bf69253954f3",
            "57da66a6d9e14bbc93fd29038fdd596e",
            "5db3c1834b14452aadd1e166ff34ef7e",
            "80b3d9a0101c4f2ba678e4b2f2ddd3a9",
            "199060989a3443b48be5a36d71ddf4a0",
            "36b27741563d4ed797c659fb3fd57ab7",
            "de30919ce8574fbb929b426b5c7232c9",
            "b26e130d77874b9097c9b7d180f390f6",
            "92190cf8eb3547438e9dc253954b893a",
            "09e70e5161aa44609c0e2913464d121d",
            "fd508c97105e42ffbf80e591d89c8e79",
            "37c83327e4214601a3c12eaa51d1f771",
            "3d85e05853834207b5d15f17f65f962f",
            "a5a7d4d5aecd4c64b1202211af77826d",
            "3e5c2cfdc36c4fc78c011009a2571c65",
            "001d04146a664567bd83cb66c0e7f9f0",
            "e96357656d174a03a5d2815f1cc048fb",
            "764512918ca34142b16772137220164e",
            "9543d14567cf4db4bf955e10139dbb87",
            "c632329b557c4fd59e064015cf1904eb",
            "64291652dd004be7bf4fa110f0847d4e",
            "4d77c605dbbe46ec9df8caaace913e9f",
            "daf1e5b9ba844d21b844503d211676bd",
            "b270539fe0144fc7a393e5144b997b8c",
            "0f4f1c77fc6143eeaa1583b5eac354dc",
            "77227f02a8974b3dbb630d4793d5a901",
            "9783e8b5f9b14b17a06acb463862f559",
            "4645ade2ad184106ad15c191830137e0",
            "f369d0e5d0e4459f9d1e2cab7e1eb224",
            "48e6cc4237ae4d1997e469d40333d13c",
            "283613c0584343a1bb0c8c3cd1877819",
            "12b036ae7d6a46d6bb96eda0c97a7b12",
            "8a63b14186a849f9a19e726351bd34a1",
            "7a836be512454b09af8f0dcc79ae07c7",
            "d61db2fadc92447c8798eb4d1f4d9a0e",
            "e3efe658016945099cea52a939ea92ac",
            "9e33dcffdc664b7e8253e07a03cc5ffc",
            "2224d00f46df4bd5abc5b00b7332e755",
            "ac73493c9db5467e924e1671efc38730",
            "be7019af8de64458adf121ab46a24141",
            "73d861a1d11d4394874705e54b02193b",
            "bcd069d45ad542c384096d555f0dbb0d",
            "be6c7dbd554b45489d45513db62d5882",
            "8641d3b3bb2641d4b3338889975c07f1",
            "320de4f2a17d4f348a37552fd8325049",
            "1cadc353bd7547ffb9960bd700a003b1",
            "4c6653325e634339b85740c15475ca3f",
            "d4822b90f7284f47993664a7d8635345",
            "74ebc510d1c948b79c090bdec0dd3908",
            "9f3411a9d0b54e2a8c8fcb9e692bc27b",
            "f9fcdcde4c204954988f80c6ba380e35",
            "9d74815e624649f8851caf3e0445da16",
            "40b0feb6514c464bae863f13fe41ebd3",
            "6a0d651cdddf41f99aa208cbcccea025",
            "afae024ad4a744f086c279bc0addfd7c",
            "832b093a33604df69d9c017c165a876b",
            "bf5a8f687f284006b58e8f1e33df9a5d",
            "e2ae3734a37b4308b2fc8120909a6a1f",
            "4b5a51161adc490280feb4392d37e0b4",
            "b4c5fcfc14ac44ad8a85199db80d6d12",
            "f09658b1301047159483c944686e17f5",
            "3e6fde3167ad42e1aaafad4d3d40a210",
            "d8192e0e7f4545bc95255fca3368bcc1",
            "63406b79e4c448fc98148bcc2ea8d8e6",
            "2bece2e471d24feca1f133acf5c8f569",
            "1dbeeb82765b4453affbea67c74df59e",
            "89f1ff5f853f4d85bc26419ecf46fc2c",
            "2053df994e81408d9b134985fa7b372c",
            "51e93537334345db8968773f5679aa37",
            "703641060cc944d3a288241cc5cedbcc",
            "7171fd8979b4446abe0dd978bf69594d",
            "9809270c8dd74fcda186ddb4f6244733",
            "1e7f85c569ee459298da60f8ec67982e",
            "b2d3d7d117424fe79119c32bcaf3d655",
            "db1f1d81f4b54815ac88c501c31b16fb",
            "551b8ccb3a864fa8bc1d53ffe46effac",
            "9f85d493f1a947c6a56222df951fdb64",
            "0f4a8bdefcd84cc6b65247909e5cb33b",
            "f9a145b63ced456881ff4f1a64ef0980",
            "52e442d79f614e6eb1faa04ec306ecef",
            "13891a4ddd62434990cfc419aa3a9b5c",
            "090d7b49d80e442196f237df8dc622a1",
            "46dcf72c55044a47b401dc5eec5b733e",
            "d95876ac6be04c2e9818a384d5257d97",
            "99cebf859d724f408d92ae681d5c2f41",
            "e5489964195e4894bc50ed99b17a161b",
            "0d60cb8487f845ebae2855b008ed5015",
            "25e91c8b969f45088138ef39dea908d6",
            "af49cfdddf2c45028697f92382ee2c06",
            "69a2f9b21654407f9c98811c20cfa97d",
            "bccf27a412b84bfebe65c109cf33967d",
            "35699d86094a4628a324f8ba63d491d2",
            "7b17a2e6c14f4972891388492220fbdd",
            "970238362b3b48838db9a7a36acb4f66",
            "c6df3c290ea14076bf65926e4d903388",
            "a45a0d3e36a54077a274cf524cfdf3f2",
            "e0e4c87d449847c7b114f0450356fb28",
            "ed1b546f3005493b948dd46b74d4ea9e"
          ]
        },
        "id": "RB6VAYZLrZ7U",
        "outputId": "61d1fb28-9dc8-48c3-c5fa-f68fe9112f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae0f63f7cbf6432aa4c436d1d440f8ac",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96cc6d4217034eb7a9fe84a000c81ccf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/3.68k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2319f78d39e42618491b671bc7eae9e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/675 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da31ccbe0e274631899c25ab9f30828b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84a927dcf24a4b8aae89f238a725f69f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd508c97105e42ffbf80e591d89c8e79",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d77c605dbbe46ec9df8caaace913e9f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a63b14186a849f9a19e726351bd34a1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8641d3b3bb2641d4b3338889975c07f1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afae024ad4a744f086c279bc0addfd7c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dbeeb82765b4453affbea67c74df59e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f85d493f1a947c6a56222df951fdb64",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25e91c8b969f45088138ef39dea908d6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sequence Length: 75\n",
            "Max Sequence Length: 300\n",
            "time: 40.9 s (started: 2022-01-18 18:41:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FZNcUGIhq0S"
      },
      "source": [
        "#model = keras.models.load_model('/content/drive/My Drive/Colab Notebooks/models/stsb_roberta_large.h5')\n",
        "#model = SentenceTransformer('/content/drive/My Drive/Colab Notebooks/models/stsb_roberta_large.h5')\n",
        "model = SentenceTransformer('/content/drive/My Drive/Colab Notebooks/models/stsb_roberta_large_saved_model')\n",
        "model.max_seq_length=300\n",
        "print(\"Max Sequence Length:\", model.max_seq_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "1b98111bd8d848bcae531df6f1af8716",
            "e867661976ba40c58797969a55a960f9",
            "a73a028a8f8b4ed3bebd42560d5a7228",
            "d8925b99ff1445f2a7f7063eda8eebc3",
            "b51e47938b744776b795cfd0943d30b9",
            "244ae4cd74a642a78dcc50109902149b",
            "9179f6a0113842e78d08d8ea4971a5bb",
            "6617d9a47ac34f28993a1e2d5d816cb1",
            "a6b51f003ab74a838bb60928c20302d3",
            "2f34dffb9f654d3aa3f5b2e8de669071",
            "c83afd64944445e48cbc76a752435110",
            "80c9f9521bdb46038a6fa33b12fd83bf",
            "543c8f5357934c78a2937429da5b894c",
            "2d5366ddde9a4672a7d3bc2a7cf6b2fb",
            "fd22b73aa75d49acbfd6dcf8f836073b",
            "0b9970a3b8ab494587dd1948bca2ba5a",
            "b5e2d454b56b4321bc6063dc7fb44c22",
            "969d4f70fffe4c6b92bcad5fff78c55b",
            "8a9ae141177445058a83e71acad6691a",
            "0e958093c3244618b303b8cd6e3c8cad",
            "f505100c1adc48dfbac2714241e008b3",
            "e1d7952a4d4a454e982b1cf15a3d44ce",
            "56f32b2dbc4943bbbebb21510c8d4a65",
            "8e56dc32847e485c99b78e6f66d5ad4d",
            "16902df9c93e49b4b6815f7eac85e225",
            "bf78fd3340e94cf887565c87c0bd7a11",
            "8c99f6340d6e4a79ad0f881a54d525bb",
            "0bcce180928245bd9b61658a06aaf332",
            "b5f804d77993451a88a6df7057b65c3e",
            "da7065bf89644d529d641cd963048a79",
            "f93c583de0614927ba095e78521ccbc9",
            "a1d04e350a5140c0aff97080812cc821",
            "c6e70a4fc6024bd0b891f490f720bdca",
            "8761e570a38b4980a7afc0ea660579b3",
            "88dbf5bdc9344decbfd244720c1e3be8",
            "cd8822b4d46448468595bc97faabddff",
            "7967715160564bfa8e7122e1f9d0b210",
            "27ed448c62cd42e0b2dc61dd17c82417",
            "0dcc922cb2954924a0648f58c5c0cc5b",
            "58a8071b87404a7e874591b58ab6849e",
            "a74de6af39014c179b2de7ef8b5416a1",
            "5d4c4fc147c6499ab845e03b6c63bdab",
            "3bd2f24eb7a24f6688b41358d83a4d14",
            "aaed72870a9247fda69b1f024037564d",
            "8d00a29e1d7449acb5e41cf06ce653f3",
            "1d7cdf424c344421ac95d732a1e13891",
            "220820e6f04c485eb078503f43f5f908",
            "2917e1169a7143bdb66425b8f0e5f0dc",
            "11d8c12d4a3f459d8b3e14ac768d2408",
            "48f0492c70e2404c9c117fc7e2a360aa",
            "48e7eaee3077458da0da039baf37d780",
            "3cd14bd9c2924e9f86e73d5b4f0fb615",
            "4a0a971f03c540e9835ae575314df105",
            "0b5148d848514b2da4e50dbdaf43c1e5",
            "231e6f5fc7e64bffbad5ca00c88ea0a2",
            "a9fc419c8c384de6a1f11a1bdcc8e10f",
            "ba51c47f8480434c91cd60351e8b7eb6",
            "e37462679b024153a21bc6e0d47c00ab",
            "6803be8e35084b77892b996c5649edf1",
            "e946ac659cc84088a2b5dc4cb95431c1",
            "084cadb0c0b246a5ba096d120d6b3ee6",
            "167b6f30aaa649658f33a60e149ad0e6",
            "7d1118ee3c954172ab7f3d577b4a2c2a",
            "6cc4523ee4614369a31621b0909dd1c1",
            "40333f539b4c4a228d587904efe31292",
            "bd6044c9d293462990326ac12210fbd4",
            "d6f4ac8a352244bcbb9cd4a4eb00d3a4",
            "ffa5069b198d494d932574f3363887e8",
            "ab05ce7c9cce407cb47117264e8f124c",
            "c8cc98fe3b714f099959df858f113832",
            "23bb1c8579ed4cb39849631e4468637e",
            "d8efe82c35794fb2b7aab57300ae7a39",
            "8f0a111a2a0348cc80ca2edfcb235947",
            "2e1e7ac7c2354f72941c749543cb6325",
            "3383710a39b04012b73ccd84dd834c8b",
            "7f59b51df31e4f1494f60936398a1bdf",
            "704bc99c93a94dab833f76f30ff837a9"
          ]
        },
        "id": "5ipzlkSijZJk",
        "outputId": "d80559b8-98a2-4e7b-e89e-030fada48cb0"
      },
      "source": [
        "word_embedding_model = models.Transformer('sentence-transformers/stsb-roberta-base-v2', max_seq_length=300)\n",
        "#print(word_embedding_model.get_word_embedding_dimension())\n",
        "#word_embedding_model = models.Transformer('allenai/scibert_scivocab_cased', max_seq_length=512)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
        "#pooling_model = models.Pooling(512)\n",
        "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=512, activation_function=nn.Tanh())\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
        "print(\"Max Sequence Length:\", model.max_seq_length)\n",
        "print(\"Sentence Embedding Dimension:\",model.get_sentence_embedding_dimension())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b98111bd8d848bcae531df6f1af8716",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/675 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80c9f9521bdb46038a6fa33b12fd83bf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/476M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56f32b2dbc4943bbbebb21510c8d4a65",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8761e570a38b4980a7afc0ea660579b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d00a29e1d7449acb5e41cf06ce653f3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9fc419c8c384de6a1f11a1bdcc8e10f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6f4ac8a352244bcbb9cd4a4eb00d3a4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Sequence Length: 300\n",
            "Sentence Embedding Dimension: 512\n",
            "time: 35 s (started: 2022-01-18 18:56:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2qUsu5fhUaN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339da6be-0b17-470a-e15e-c75a58c4febd"
      },
      "source": [
        "#reference 1\n",
        "text=\"\"\"\n",
        "\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Velit scelerisque in dictum non consectetur a erat. Sit amet justo donec enim diam vulputate. Id aliquet lectus proin nibh nisl condimentum id venenatis a. Eget gravida cum sociis natoque penatibus et magnis dis. Habitant morbi tristique senectus et netus et. Interdum consectetur libero id faucibus nisl tincidunt eget nullam. Aliquam purus sit amet luctus. Fringilla ut morbi tincidunt augue interdum velit. Neque sodales ut etiam sit. Quam viverra orci sagittis eu volutpat odio facilisis mauris. Ornare suspendisse sed nisi lacus sed. Iaculis at erat pellentesque adipiscing commodo elit at imperdiet dui. Quam nulla porttitor massa id neque aliquam vestibulum morbi. Dignissim diam quis enim lobortis scelerisque fermentum dui faucibus. Turpis egestas integer eget aliquet.\n",
        "\n",
        "In nisl nisi scelerisque eu ultrices vitae auctor eu. Dolor sit amet consectetur adipiscing elit duis. Tortor dignissim convallis aenean et tortor at. Iaculis at erat pellentesque adipiscing commodo. Viverra suspendisse potenti nullam ac tortor. Elementum nibh tellus molestie nunc non blandit massa enim. Ultricies integer quis auctor elit sed. Varius vel pharetra vel turpis nunc eget lorem dolor. Sit amet massa vitae tortor condimentum. Adipiscing elit ut aliquam purus sit amet luctus venenatis lectus. Nascetur ridiculus mus mauris vitae ultricies leo integer.\n",
        "\n",
        "Urna nunc id cursus metus. Id leo in vitae turpis massa. Blandit turpis cursus in hac habitasse platea. Feugiat sed lectus vestibulum mattis ullamcorper. Diam sit amet nisl suscipit adipiscing bibendum est. Enim nunc faucibus a pellentesque sit amet porttitor eget dolor. Enim eu turpis egestas pretium aenean pharetra. Amet mattis vulputate enim nulla aliquet. Tristique et egestas quis ipsum suspendisse ultrices gravida dictum fusce. Risus commodo viverra maecenas accumsan lacus vel. Eu mi bibendum neque egestas congue quisque egestas diam in. Fermentum odio eu feugiat pretium nibh ipsum consequat.\n",
        "\n",
        "Egestas egestas fringilla phasellus faucibus scelerisque. Sit amet dictum sit amet justo donec. Cum sociis natoque penatibus et magnis dis parturient montes. Habitasse platea dictumst quisque sagittis purus sit amet volutpat. Magna etiam tempor orci eu lobortis elementum nibh tellus molestie. Gravida arcu ac tortor dignissim convallis. Consequat nisl vel pretium lectus quam id leo in. Amet venenatis urna cursus eget. In est ante in nibh. Mauris commodo quis imperdiet massa. Pellentesque diam volutpat commodo sed egestas egestas fringilla phasellus.\n",
        "\n",
        "Laoreet suspendisse interdum consectetur libero id faucibus nisl tincidunt. Enim sit amet venenatis urna cursus eget. Tristique senectus et netus et. Ipsum suspendisse ultrices gravida dictum fusce ut. Velit ut tortor pretium viverra suspendisse potenti nullam ac tortor. Sapien eget mi proin sed libero enim sed faucibus turpis. Ullamcorper malesuada proin libero nunc consequat interdum varius. Suscipit adipiscing bibendum est ultricies integer quis. Libero volutpat sed cras ornare arcu. In ante metus dictum at. Sed augue lacus viverra vitae congue eu consequat. Mi eget mauris pharetra et ultrices neque ornare aenean. Pellentesque elit ullamcorper dignissim cras tincidunt lobortis feugiat vivamus at. Blandit cursus risus at ultrices mi tempus. Ultrices gravida dictum fusce ut placerat. Sit amet justo donec enim. Malesuada fames ac turpis egestas integer eget.\n",
        "\n",
        "Nibh praesent tristique magna sit amet purus gravida quis. Commodo viverra maecenas accumsan lacus vel facilisis volutpat est. Sed viverra tellus in hac habitasse. Eu augue ut lectus arcu bibendum at varius vel pharetra. Leo vel fringilla est ullamcorper eget nulla facilisi etiam dignissim. Gravida dictum fusce ut placerat orci nulla pellentesque dignissim. Varius sit amet mattis vulputate enim nulla aliquet porttitor. Egestas diam in arcu cursus euismod quis viverra nibh. Facilisi cras fermentum odio eu feugiat pretium nibh ipsum consequat. Enim nulla aliquet porttitor lacus luctus. At varius vel pharetra vel turpis.\n",
        "\n",
        "Eget mi proin sed libero enim sed faucibus turpis in. Sed odio morbi quis commodo odio aenean sed. Nibh mauris cursus mattis molestie a iaculis at. Tellus pellentesque eu tincidunt tortor. Massa vitae tortor condimentum lacinia quis. Scelerisque eu ultrices vitae auctor eu augue ut. Purus gravida quis blandit turpis cursus in hac habitasse platea. Ullamcorper sit amet risus nullam eget felis. Adipiscing vitae proin sagittis nisl rhoncus mattis rhoncus urna neque. Pellentesque habitant morbi tristique senectus et netus. Vehicula ipsum a arcu cursus vitae. Amet luctus venenatis lectus magna fringilla urna porttitor rhoncus. Tristique senectus et netus et malesuada. Placerat duis ultricies lacus sed turpis tincidunt id aliquet. Dolor purus non enim praesent elementum facilisis leo. Blandit libero volutpat sed cras ornare arcu. Neque vitae tempus quam pellentesque. Elit eget gravida cum sociis.\n",
        "\n",
        "Nunc lobortis mattis aliquam faucibus purus in massa tempor. Amet consectetur adipiscing elit pellentesque. Fermentum posuere urna nec tincidunt praesent semper feugiat nibh sed. Sed tempus urna et pharetra pharetra massa massa ultricies mi. Turpis egestas integer eget aliquet nibh praesent tristique magna sit. Pellentesque elit ullamcorper dignissim cras tincidunt lobortis feugiat vivamus. In est ante in nibh mauris. Vel facilisis volutpat est velit egestas. Elementum integer enim neque volutpat ac tincidunt vitae. Id velit ut tortor pretium viverra. Commodo viverra maecenas accumsan lacus. Mi bibendum neque egestas congue.\n",
        "\n",
        "Cras sed felis eget velit aliquet sagittis. Et sollicitudin ac orci phasellus egestas tellus rutrum tellus pellentesque. Elementum pulvinar etiam non quam lacus suspendisse faucibus interdum. Morbi tincidunt augue interdum velit euismod in. Vitae justo eget magna fermentum iaculis eu non. Pellentesque diam volutpat commodo sed. Cras ornare arcu dui vivamus arcu. Sed arcu non odio euismod lacinia at quis risus sed. Volutpat blandit aliquam etiam erat. Id neque aliquam vestibulum morbi blandit. Non sodales neque sodales ut etiam sit amet. Elit eget gravida cum sociis natoque penatibus et magnis. Eu feugiat pretium nibh ipsum consequat nisl vel pretium lectus. Suspendisse interdum consectetur libero id faucibus nisl tincidunt eget. Amet risus nullam eget felis eget nunc lobortis mattis. Volutpat consequat mauris nunc congue nisi vitae suscipit tellus.\n",
        "\n",
        "Elementum facilisis leo vel fringilla est ullamcorper eget nulla. Sit amet nulla facilisi morbi tempus iaculis urna id. Bibendum ut tristique et egestas quis. Eu feugiat pretium nibh ipsum consequat nisl vel pretium lectus. Volutpat commodo sed egestas egestas fringilla phasellus faucibus. Mauris a diam maecenas sed enim ut sem. Lacus vestibulum sed arcu non odio. In fermentum et sollicitudin ac orci phasellus egestas. Blandit aliquam etiam erat velit scelerisque. Sit amet nisl suscipit adipiscing. Feugiat sed lectus vestibulum mattis ullamcorper. Nisl suscipit adipiscing bibendum est ultricies integer. Nam libero justo laoreet sit amet cursus sit amet.\n",
        "\n",
        "Mi eget mauris pharetra et. A erat nam at lectus urna duis convallis. Sit amet porttitor eget dolor morbi non. Adipiscing at in tellus integer feugiat scelerisque. A erat nam at lectus urna duis. Sit amet porttitor eget dolor morbi. Massa tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada. Tellus orci ac auctor augue mauris augue neque. Suspendisse potenti nullam ac tortor vitae purus. Ut consequat semper viverra nam libero justo laoreet. Iaculis nunc sed augue lacus viverra vitae congue eu consequat.\n",
        "\n",
        "Accumsan lacus vel facilisis volutpat est velit egestas dui id. Diam sit amet nisl suscipit. Enim lobortis scelerisque fermentum dui faucibus in ornare quam viverra. Vitae auctor eu augue ut lectus. Nunc sed augue lacus viverra. Nibh ipsum consequat nisl vel pretium lectus. Tincidunt ornare massa eget egestas purus viverra accumsan in. Habitasse platea dictumst quisque sagittis purus sit amet volutpat. Turpis massa sed elementum tempus egestas sed. Pellentesque nec nam aliquam sem et tortor consequat id porta. Et netus et malesuada fames ac turpis egestas. Velit sed ullamcorper morbi tincidunt ornare massa. Tincidunt praesent semper feugiat nibh sed pulvinar.\n",
        "\n",
        "Ac tortor dignissim convallis aenean. Risus in hendrerit gravida rutrum quisque non tellus orci ac. In pellentesque massa placerat duis ultricies lacus sed turpis tincidunt. Bibendum enim facilisis gravida neque convallis a cras semper. Nec feugiat in fermentum posuere urna. Lacus suspendisse faucibus interdum posuere lorem ipsum dolor. Id diam vel quam elementum pulvinar etiam non quam lacus. Nibh tellus molestie nunc non blandit massa enim nec dui. Interdum posuere lorem ipsum dolor sit. Arcu bibendum at varius vel. Nibh tellus molestie nunc non blandit massa. Sit amet nisl suscipit adipiscing. Dui accumsan sit amet nulla facilisi morbi tempus. Adipiscing at in tellus integer feugiat scelerisque varius. Sit amet cursus sit amet. Consectetur libero id faucibus nisl tincidunt eget nullam non.\n",
        "\n",
        "Diam quam nulla porttitor massa. Quis viverra nibh cras pulvinar mattis nunc sed. Ultricies mi quis hendrerit dolor magna eget. Vitae congue eu consequat ac. Dolor magna eget est lorem ipsum dolor sit amet. Habitant morbi tristique senectus et netus et malesuada fames ac. Metus dictum at tempor commodo ullamcorper. Proin fermentum leo vel orci porta non pulvinar. In arcu cursus euismod quis. Eget lorem dolor sed viverra ipsum. Volutpat commodo sed egestas egestas fringilla phasellus faucibus scelerisque. Commodo sed egestas egestas fringilla phasellus faucibus scelerisque eleifend. Sem fringilla ut morbi tincidunt augue interdum. Lectus quam id leo in vitae turpis massa. Malesuada pellentesque elit eget gravida cum. Volutpat sed cras ornare arcu dui.\n",
        "\n",
        "Quam pellentesque nec nam aliquam sem. Viverra vitae congue eu consequat ac felis. Eu facilisis sed odio morbi. Sapien faucibus et molestie ac. Mauris nunc congue nisi vitae suscipit tellus mauris. Senectus et netus et malesuada fames. Risus in hendrerit gravida rutrum quisque non tellus orci ac. Volutpat blandit aliquam etiam erat velit scelerisque in dictum non. Varius morbi enim nunc faucibus. Risus pretium quam vulputate dignissim suspendisse. Sed euismod nisi porta lorem mollis aliquam ut.\n",
        "\n",
        "Id aliquet lectus proin nibh. Sed euismod nisi porta lorem. Mauris pharetra et ultrices neque ornare aenean euismod elementum nisi. Fringilla ut morbi tincidunt augue interdum velit euismod in pellentesque. Scelerisque varius morbi enim nunc faucibus a pellentesque. Convallis tellus id interdum velit. Quam pellentesque nec nam aliquam. Nisi vitae suscipit tellus mauris a diam maecenas sed enim. Euismod quis viverra nibh cras pulvinar mattis. Eget arcu dictum varius duis at consectetur lorem. Convallis aenean et tortor at risus. Ut morbi tincidunt augue interdum velit euismod.\n",
        "\n",
        "Duis ultricies lacus sed turpis tincidunt id aliquet. Nibh ipsum consequat nisl vel pretium lectus quam id leo. A cras semper auctor neque. Vitae turpis massa sed elementum tempus. Eget aliquet nibh praesent tristique magna sit amet. Amet venenatis urna cursus eget nunc scelerisque viverra mauris in. Ac tincidunt vitae semper quis lectus nulla at volutpat. Nulla facilisi nullam vehicula ipsum a arcu. Vel facilisis volutpat est velit egestas dui. Sed blandit libero volutpat sed cras ornare arcu dui vivamus. Morbi tincidunt augue interdum velit euismod in pellentesque massa. Tellus at urna condimentum mattis pellentesque id nibh. Lectus nulla at volutpat diam ut venenatis. Purus sit amet volutpat consequat mauris nunc congue. Mattis ullamcorper velit sed ullamcorper morbi tincidunt ornare massa eget. Hac habitasse platea dictumst quisque sagittis. Hendrerit gravida rutrum quisque non tellus orci ac auctor. Tellus at urna condimentum mattis.\n",
        "\n",
        "Mi ipsum faucibus vitae aliquet nec ullamcorper sit. Nisi quis eleifend quam adipiscing vitae proin sagittis. Dictum fusce ut placerat orci nulla pellentesque. Mauris pellentesque pulvinar pellentesque habitant morbi tristique senectus et. Sodales ut eu sem integer vitae justo eget magna. Orci ac auctor augue mauris augue neque gravida in fermentum. Nunc vel risus commodo viverra maecenas accumsan lacus. Euismod nisi porta lorem mollis. Ultrices dui sapien eget mi proin sed. Lobortis mattis aliquam faucibus purus in massa. Adipiscing bibendum est ultricies integer quis auctor elit sed vulputate. Ut faucibus pulvinar elementum integer. Bibendum ut tristique et egestas quis. Facilisis sed odio morbi quis commodo odio aenean sed. Augue ut lectus arcu bibendum at. Eget dolor morbi non arcu risus.\n",
        "\n",
        "Adipiscing bibendum est ultricies integer quis auctor elit sed vulputate. Quisque non tellus orci ac auctor. At auctor urna nunc id cursus metus aliquam. Eleifend mi in nulla posuere sollicitudin. Quisque sagittis purus sit amet volutpat consequat mauris nunc. Ut aliquam purus sit amet luctus venenatis lectus. Quis imperdiet massa tincidunt nunc. Ut ornare lectus sit amet. Semper quis lectus nulla at. Eget magna fermentum iaculis eu non. Faucibus et molestie ac feugiat sed. Eget lorem dolor sed viverra ipsum nunc aliquet bibendum.\n",
        "\n",
        "Tincidunt praesent semper feugiat nibh sed pulvinar proin gravida. Pellentesque diam volutpat commodo sed egestas. Maecenas accumsan lacus vel facilisis volutpat est velit egestas dui. Ultrices in iaculis nunc sed augue lacus viverra. Facilisi nullam vehicula ipsum a. Diam quam nulla porttitor massa id neque. Ornare massa eget egestas purus viverra accumsan in nisl. In arcu cursus euismod quis viverra nibh cras. Feugiat scelerisque varius morbi enim nunc faucibus a pellentesque. Nunc vel risus commodo viverra maecenas accumsan lacus vel. Lacus luctus accumsan tortor posuere ac.\n",
        "\n",
        "Nibh venenatis cras sed felis. Turpis egestas sed tempus urna. Lectus arcu bibendum at varius. Est lorem ipsum dolor sit amet consectetur adipiscing elit. Etiam non quam lacus suspendisse faucibus interdum posuere lorem. Facilisi nullam vehicula ipsum a arcu cursus vitae congue. Morbi leo urna molestie at elementum eu facilisis sed. Molestie a iaculis at erat pellentesque. Proin sagittis nisl rhoncus mattis rhoncus urna neque viverra. Justo nec ultrices dui sapien eget. Eget velit aliquet sagittis id consectetur. Suspendisse potenti nullam ac tortor vitae. At varius vel pharetra vel. Aliquam sem fringilla ut morbi tincidunt augue.\n",
        "\n",
        "Ante in nibh mauris cursus mattis molestie a iaculis at. Neque ornare aenean euismod elementum nisi quis eleifend. Consectetur lorem donec massa sapien faucibus et molestie. Netus et malesuada fames ac. Pharetra sit amet aliquam id diam. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget. Massa tincidunt dui ut ornare. Scelerisque varius morbi enim nunc faucibus a pellentesque sit amet. Imperdiet massa tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada. Varius quam quisque id diam vel quam. Dictum non consectetur a erat nam at lectus urna.\n",
        "\n",
        "In aliquam sem fringilla ut morbi tincidunt augue. Semper auctor neque vitae tempus quam pellentesque nec nam. Vestibulum rhoncus est pellentesque elit ullamcorper dignissim cras tincidunt. Magna eget est lorem ipsum dolor sit. Vestibulum lectus mauris ultrices eros in cursus turpis massa. Id ornare arcu odio ut sem nulla pharetra diam sit. Aliquet porttitor lacus luctus accumsan tortor posuere ac. Id venenatis a condimentum vitae sapien pellentesque habitant morbi tristique. Eget egestas purus viverra accumsan in nisl. Amet risus nullam eget felis eget nunc lobortis. Feugiat in ante metus dictum at tempor. Phasellus egestas tellus rutrum tellus pellentesque eu tincidunt tortor aliquam. Hac habitasse platea dictumst vestibulum rhoncus est pellentesque. Venenatis urna cursus eget nunc scelerisque viverra mauris. Morbi tincidunt augue interdum velit euismod in pellentesque.\n",
        "\n",
        "Amet dictum sit amet justo donec. Lorem donec massa sapien faucibus et molestie ac feugiat sed. In est ante in nibh mauris cursus. Pretium vulputate sapien nec sagittis aliquam malesuada. Fames ac turpis egestas sed. Commodo elit at imperdiet dui. Mi quis hendrerit dolor magna. Mi in nulla posuere sollicitudin aliquam. Dolor sed viverra ipsum nunc. Phasellus vestibulum lorem sed risus ultricies tristique. Integer quis auctor elit sed vulputate mi sit. Volutpat lacus laoreet non curabitur gravida arcu ac. Eget mi proin sed libero enim. At tempor commodo ullamcorper a. Elit ut aliquam purus sit amet.\n",
        "\n",
        "Nunc faucibus a pellentesque sit amet porttitor eget dolor morbi. Netus et malesuada fames ac turpis egestas. Morbi tristique senectus et netus. Fames ac turpis egestas maecenas pharetra convallis posuere morbi. Enim tortor at auctor urna nunc id cursus. Tempus iaculis urna id volutpat lacus laoreet non. Maecenas ultricies mi eget mauris pharetra et ultrices neque ornare. Accumsan lacus vel facilisis volutpat est velit egestas dui. At imperdiet dui accumsan sit amet nulla facilisi. Turpis tincidunt id aliquet risus feugiat in. Nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Integer quis auctor elit sed vulputate. Netus et malesuada fames ac turpis. Diam donec adipiscing tristique risus. Ornare quam viverra orci sagittis eu. Feugiat in fermentum posuere urna nec tincidunt praesent. Turpis cursus in hac habitasse platea. Aliquam eleifend mi in nulla posuere sollicitudin. Dui faucibus in ornare quam viverra orci sagittis. Scelerisque in dictum non consectetur a.\n",
        "\n",
        "Scelerisque mauris pellentesque pulvinar pellentesque habitant morbi. Dictumst quisque sagittis purus sit amet volutpat consequat mauris nunc. Id faucibus nisl tincidunt eget nullam non nisi est sit. Montes nascetur ridiculus mus mauris vitae. Ut tristique et egestas quis ipsum suspendisse ultrices gravida dictum. Pellentesque dignissim enim sit amet. Mauris in aliquam sem fringilla ut. Ut tellus elementum sagittis vitae. Faucibus et molestie ac feugiat sed lectus vestibulum mattis. Turpis egestas integer eget aliquet nibh praesent tristique magna sit. Lectus vestibulum mattis ullamcorper velit sed ullamcorper morbi tincidunt. Lorem sed risus ultricies tristique nulla.\n",
        "\n",
        "Molestie at elementum eu facilisis. In hendrerit gravida rutrum quisque non tellus orci. At auctor urna nunc id. Habitasse platea dictumst vestibulum rhoncus est pellentesque elit ullamcorper dignissim. Vestibulum rhoncus est pellentesque elit ullamcorper dignissim cras tincidunt lobortis. Nunc id cursus metus aliquam eleifend mi. Magna ac placerat vestibulum lectus mauris ultrices eros in. Non odio euismod lacinia at. At auctor urna nunc id cursus metus aliquam eleifend. Morbi non arcu risus quis. Dolor sit amet consectetur adipiscing elit. Aliquam nulla facilisi cras fermentum odio eu feugiat pretium nibh.\n",
        "\n",
        "At lectus urna duis convallis convallis tellus id interdum velit. Parturient montes nascetur ridiculus mus mauris vitae ultricies. At tempor commodo ullamcorper a lacus vestibulum sed. Proin nibh nisl condimentum id venenatis a. Accumsan tortor posuere ac ut consequat. Orci nulla pellentesque dignissim enim sit amet venenatis. Cras semper auctor neque vitae tempus quam. Elementum eu facilisis sed odio morbi quis. Aliquet risus feugiat in ante metus. Leo a diam sollicitudin tempor id eu nisl. Dui faucibus in ornare quam viverra orci sagittis eu. Aliquet nec ullamcorper sit amet risus. Eros in cursus turpis massa tincidunt. Pulvinar elementum integer enim neque volutpat ac tincidunt vitae semper.\n",
        "\n",
        "Velit aliquet sagittis id consectetur. Sed enim ut sem viverra aliquet eget sit amet tellus. Velit laoreet id donec ultrices. Iaculis urna id volutpat lacus laoreet non curabitur gravida arcu. Sit amet nisl suscipit adipiscing bibendum. Aliquam faucibus purus in massa tempor nec feugiat. Quam nulla porttitor massa id neque aliquam. Blandit massa enim nec dui. Amet volutpat consequat mauris nunc congue. Vel turpis nunc eget lorem dolor sed viverra ipsum. Adipiscing elit pellentesque habitant morbi. Eget aliquet nibh praesent tristique magna sit. In cursus turpis massa tincidunt dui ut ornare. Convallis aenean et tortor at risus viverra adipiscing at in. Pellentesque elit ullamcorper dignissim cras tincidunt lobortis feugiat.\n",
        "\n",
        "Fermentum dui faucibus in ornare quam viverra orci sagittis. Facilisi etiam dignissim diam quis enim lobortis scelerisque fermentum dui. Elit ut aliquam purus sit amet luctus venenatis lectus magna. Purus faucibus ornare suspendisse sed nisi lacus. Viverra nam libero justo laoreet. Ornare quam viverra orci sagittis eu volutpat odio facilisis mauris. Diam quam nulla porttitor massa. Cursus euismod quis viverra nibh cras pulvinar. Purus viverra accumsan in nisl nisi. Amet consectetur adipiscing elit ut aliquam. Sed odio morbi quis commodo. Eu mi bibendum neque egestas. In egestas erat imperdiet sed euismod nisi. Semper viverra nam libero justo laoreet sit amet. Tellus at urna condimentum mattis. In metus vulputate eu scelerisque felis imperdiet proin. Massa ultricies mi quis hendrerit dolor magna eget.\n",
        "\n",
        "Lectus mauris ultrices eros in. Mi sit amet mauris commodo quis imperdiet massa. Fusce ut placerat orci nulla pellentesque. Non diam phasellus vestibulum lorem sed risus ultricies. Risus feugiat in ante metus dictum at tempor commodo. Suspendisse ultrices gravida dictum fusce ut. Neque convallis a cras semper auctor neque vitae tempus quam. Pharetra massa massa ultricies mi. Arcu non odio euismod lacinia at quis risus. Eget velit aliquet sagittis id consectetur purus ut. Mattis molestie a iaculis at erat pellentesque adipiscing commodo elit. Lorem ipsum dolor sit amet. Enim facilisis gravida neque convallis a cras. Amet mattis vulputate enim nulla aliquet porttitor lacus luctus accumsan. Eros donec ac odio tempor. Tortor aliquam nulla facilisi cras fermentum odio. Erat pellentesque adipiscing commodo elit at imperdiet dui.\n",
        "\n",
        "Eget duis at tellus at urna. Gravida arcu ac tortor dignissim convallis aenean et. Vitae justo eget magna fermentum iaculis eu non diam phasellus. Mauris augue neque gravida in fermentum et sollicitudin. Auctor neque vitae tempus quam pellentesque. Lacus vestibulum sed arcu non odio euismod lacinia at quis. Viverra vitae congue eu consequat. Elementum tempus egestas sed sed. Fringilla urna porttitor rhoncus dolor purus non enim. Morbi tristique senectus et netus. Ut sem viverra aliquet eget. Ac placerat vestibulum lectus mauris ultrices eros in cursus turpis. Turpis massa sed elementum tempus egestas. Aliquam malesuada bibendum arcu vitae elementum curabitur vitae nunc sed. Cursus euismod quis viverra nibh. Ut sem nulla pharetra diam sit amet. Quis auctor elit sed vulputate. Gravida dictum fusce ut placerat orci nulla. Sem integer vitae justo eget. Consectetur adipiscing elit pellentesque habitant morbi tristique senectus et.\n",
        "\n",
        "Ultricies leo integer malesuada nunc vel risus commodo viverra. Lobortis scelerisque fermentum dui faucibus in ornare quam viverra. Suspendisse sed nisi lacus sed viverra tellus. Condimentum id venenatis a condimentum vitae sapien pellentesque. Dictum varius duis at consectetur. Ultrices tincidunt arcu non sodales neque sodales. Sed lectus vestibulum mattis ullamcorper velit sed ullamcorper morbi. Posuere ac ut consequat semper viverra nam. Aliquet nec ullamcorper sit amet risus nullam eget. Semper eget duis at tellus at urna condimentum. In metus vulputate eu scelerisque felis imperdiet proin. Fermentum dui faucibus in ornare quam viverra. Habitant morbi tristique senectus et netus et. Phasellus egestas tellus rutrum tellus pellentesque eu tincidunt tortor. Elementum sagittis vitae et leo duis. Tempor orci eu lobortis elementum. Nulla pellentesque dignissim enim sit. Auctor eu augue ut lectus arcu bibendum at varius. Donec et odio pellentesque diam volutpat commodo sed egestas.\n",
        "\n",
        "Gravida cum sociis natoque penatibus et magnis dis parturient. Scelerisque in dictum non consectetur a erat nam at. Mauris pellentesque pulvinar pellentesque habitant morbi tristique senectus et. Aliquet nibh praesent tristique magna sit amet purus. Tempus imperdiet nulla malesuada pellentesque. Posuere lorem ipsum dolor sit. Diam sit amet nisl suscipit adipiscing bibendum est ultricies integer. Suscipit adipiscing bibendum est ultricies integer quis auctor elit sed. Nunc consequat interdum varius sit amet mattis. Non enim praesent elementum facilisis. Luctus venenatis lectus magna fringilla urna porttitor. Netus et malesuada fames ac turpis egestas. Pharetra pharetra massa massa ultricies mi quis hendrerit.\n",
        "\n",
        "Dui vivamus arcu felis bibendum ut tristique et egestas. Nam at lectus urna duis convallis convallis tellus id. Malesuada nunc vel risus commodo viverra maecenas accumsan lacus vel. Odio aenean sed adipiscing diam donec adipiscing tristique risus nec. Mauris vitae ultricies leo integer. Tincidunt arcu non sodales neque sodales ut etiam sit amet. Quam id leo in vitae turpis massa sed elementum tempus. Ultrices dui sapien eget mi proin. Dui sapien eget mi proin sed libero. Quam lacus suspendisse faucibus interdum posuere lorem ipsum dolor. Purus semper eget duis at. Aliquet nec ullamcorper sit amet risus nullam eget felis eget. Egestas erat imperdiet sed euismod nisi porta. Tincidunt lobortis feugiat vivamus at. A pellentesque sit amet porttitor eget dolor morbi non.\n",
        "\n",
        "Egestas tellus rutrum tellus pellentesque eu tincidunt. Vehicula ipsum a arcu cursus vitae congue mauris rhoncus aenean. Blandit aliquam etiam erat velit scelerisque. Ut pharetra sit amet aliquam id diam maecenas. Ultrices eros in cursus turpis massa tincidunt dui ut ornare. Libero justo laoreet sit amet cursus sit amet dictum sit. Nisl rhoncus mattis rhoncus urna neque viverra justo nec ultrices. Eu turpis egestas pretium aenean pharetra. Mattis aliquam faucibus purus in. Mi bibendum neque egestas congue. Ipsum faucibus vitae aliquet nec. Consequat nisl vel pretium lectus quam id leo. Dolor sit amet consectetur adipiscing elit. Feugiat vivamus at augue eget arcu dictum. Mattis molestie a iaculis at. In eu mi bibendum neque egestas congue quisque egestas diam.\n",
        "\n",
        "Magna fermentum iaculis eu non diam phasellus vestibulum lorem sed. Ultrices gravida dictum fusce ut placerat orci nulla pellentesque. Viverra tellus in hac habitasse platea. Maecenas sed enim ut sem viverra aliquet. Pellentesque adipiscing commodo elit at imperdiet dui accumsan sit amet. Sociis natoque penatibus et magnis dis parturient montes. Vulputate odio ut enim blandit volutpat maecenas. Elementum pulvinar etiam non quam lacus suspendisse faucibus. Commodo odio aenean sed adipiscing. Risus feugiat in ante metus dictum at tempor commodo ullamcorper. Est ante in nibh mauris. Platea dictumst quisque sagittis purus sit amet volutpat consequat. Sagittis purus sit amet volutpat consequat mauris nunc congue nisi. At in tellus integer feugiat scelerisque varius.\n",
        "\n",
        "Tincidunt id aliquet risus feugiat. Metus dictum at tempor commodo ullamcorper a lacus vestibulum sed. Nunc congue nisi vitae suscipit tellus. Tristique senectus et netus et malesuada fames ac. Eget mauris pharetra et ultrices neque ornare. Fusce id velit ut tortor. Viverra vitae congue eu consequat ac felis donec et odio. Netus et malesuada fames ac. Convallis aenean et tortor at risus. Ornare massa eget egestas purus viverra accumsan in nisl nisi. Nulla posuere sollicitudin aliquam ultrices.\n",
        "\n",
        "Lorem dolor sed viverra ipsum. Aliquam sem fringilla ut morbi tincidunt augue. Scelerisque felis imperdiet proin fermentum. Proin fermentum leo vel orci porta non pulvinar. Nec feugiat in fermentum posuere urna nec. Habitant morbi tristique senectus et netus et malesuada fames. Faucibus pulvinar elementum integer enim neque. Metus vulputate eu scelerisque felis imperdiet. Lectus magna fringilla urna porttitor rhoncus. Id aliquet lectus proin nibh nisl condimentum id venenatis. Egestas maecenas pharetra convallis posuere morbi leo urna molestie at. Suspendisse in est ante in nibh mauris cursus mattis. Sed egestas egestas fringilla phasellus faucibus scelerisque. Sed vulputate mi sit amet mauris commodo quis. Ipsum dolor sit amet consectetur. Nibh praesent tristique magna sit. Massa sed elementum tempus egestas sed sed risus pretium quam.\n",
        "\n",
        "Pulvinar elementum integer enim neque volutpat. Eu feugiat pretium nibh ipsum consequat. Eu nisl nunc mi ipsum faucibus vitae. Nisi vitae suscipit tellus mauris a diam maecenas sed. Mattis nunc sed blandit libero volutpat sed. Ultricies mi eget mauris pharetra et ultrices neque. Ultrices mi tempus imperdiet nulla malesuada. Morbi quis commodo odio aenean sed adipiscing diam. Et pharetra pharetra massa massa ultricies mi quis. Dis parturient montes nascetur ridiculus mus mauris vitae ultricies. Sit amet dictum sit amet justo donec.\n",
        "\n",
        "Pellentesque habitant morbi tristique senectus et. Egestas erat imperdiet sed euismod nisi porta lorem mollis aliquam. Risus at ultrices mi tempus imperdiet nulla. Ac auctor augue mauris augue neque gravida in. Porttitor eget dolor morbi non arcu. Augue eget arcu dictum varius duis at consectetur. Amet consectetur adipiscing elit ut aliquam purus. Suspendisse interdum consectetur libero id faucibus. Quam elementum pulvinar etiam non quam lacus suspendisse faucibus. Aenean et tortor at risus viverra adipiscing at in tellus. Dolor sed viverra ipsum nunc aliquet. Enim tortor at auctor urna. Malesuada nunc vel risus commodo viverra.\n",
        "\n",
        "Sit amet volutpat consequat mauris nunc. Vulputate dignissim suspendisse in est ante in nibh mauris. Volutpat consequat mauris nunc congue. At tempor commodo ullamcorper a lacus. At imperdiet dui accumsan sit amet nulla. Urna duis convallis convallis tellus id. Quis enim lobortis scelerisque fermentum dui faucibus. Lobortis scelerisque fermentum dui faucibus in ornare quam viverra. Velit sed ullamcorper morbi tincidunt ornare massa. Faucibus vitae aliquet nec ullamcorper sit amet risus nullam. Arcu vitae elementum curabitur vitae nunc sed velit. Adipiscing commodo elit at imperdiet dui accumsan sit. Gravida rutrum quisque non tellus orci ac. Nibh mauris cursus mattis molestie a iaculis at. Elit pellentesque habitant morbi tristique senectus et netus et. Non tellus orci ac auctor augue mauris augue neque. Lacus vestibulum sed arcu non odio euismod lacinia at. Ac turpis egestas integer eget aliquet nibh praesent. Leo a diam sollicitudin tempor id eu. Leo in vitae turpis massa sed elementum tempus egestas sed.\n",
        "\n",
        "Mattis molestie a iaculis at erat pellentesque. Urna porttitor rhoncus dolor purus. Elementum integer enim neque volutpat ac tincidunt vitae. Integer malesuada nunc vel risus commodo. Eu tincidunt tortor aliquam nulla facilisi cras fermentum. Mauris commodo quis imperdiet massa tincidunt. Commodo quis imperdiet massa tincidunt nunc pulvinar sapien et. Auctor augue mauris augue neque. Justo donec enim diam vulputate. Nulla facilisi morbi tempus iaculis. Sit amet commodo nulla facilisi nullam vehicula ipsum a arcu. Sit amet nisl suscipit adipiscing bibendum est. Interdum varius sit amet mattis vulputate enim. Donec adipiscing tristique risus nec feugiat in. Integer malesuada nunc vel risus commodo viverra maecenas. Natoque penatibus et magnis dis parturient montes nascetur ridiculus.\n",
        "\n",
        "Gravida quis blandit turpis cursus in hac. Sed faucibus turpis in eu mi bibendum neque. At tempor commodo ullamcorper a lacus vestibulum sed. Amet commodo nulla facilisi nullam. A erat nam at lectus urna duis convallis convallis. Tortor pretium viverra suspendisse potenti nullam ac. Volutpat commodo sed egestas egestas fringilla phasellus. Elit eget gravida cum sociis natoque penatibus. Egestas sed sed risus pretium quam vulputate. Ultrices dui sapien eget mi proin. Vivamus arcu felis bibendum ut tristique et egestas quis. Vulputate odio ut enim blandit volutpat. Felis eget velit aliquet sagittis.\n",
        "\n",
        "Vestibulum rhoncus est pellentesque elit. Faucibus in ornare quam viverra orci sagittis eu. Nulla pellentesque dignissim enim sit amet venenatis urna. Turpis cursus in hac habitasse platea dictumst. Proin libero nunc consequat interdum varius sit amet mattis vulputate. Sed risus ultricies tristique nulla aliquet enim tortor. Turpis nunc eget lorem dolor sed viverra ipsum nunc aliquet. Odio aenean sed adipiscing diam donec adipiscing tristique risus. Volutpat sed cras ornare arcu dui. Elementum sagittis vitae et leo. A arcu cursus vitae congue mauris rhoncus. Nec ultrices dui sapien eget mi proin sed libero. Eget magna fermentum iaculis eu. Fringilla phasellus faucibus scelerisque eleifend donec pretium vulputate sapien. Ultricies mi eget mauris pharetra et ultrices. Elit sed vulputate mi sit amet. Platea dictumst quisque sagittis purus sit. Amet venenatis urna cursus eget. Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum. Feugiat nibh sed pulvinar proin gravida hendrerit.\n",
        "\n",
        "Sit amet tellus cras adipiscing enim eu turpis. In hac habitasse platea dictumst quisque. Elit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi. Eleifend mi in nulla posuere sollicitudin. Diam sollicitudin tempor id eu nisl nunc mi ipsum faucibus. Amet aliquam id diam maecenas. Volutpat commodo sed egestas egestas fringilla. A diam maecenas sed enim ut sem viverra aliquet. Aliquam sem et tortor consequat id porta nibh venenatis cras. Ac turpis egestas maecenas pharetra. Adipiscing elit ut aliquam purus sit. Iaculis urna id volutpat lacus laoreet non curabitur gravida. Egestas fringilla phasellus faucibus scelerisque eleifend donec pretium.\n",
        "\n",
        "Adipiscing tristique risus nec feugiat in fermentum posuere urna nec. Mauris commodo quis imperdiet massa tincidunt nunc pulvinar sapien. Egestas sed sed risus pretium quam vulputate. Etiam erat velit scelerisque in dictum. Elit pellentesque habitant morbi tristique senectus et netus. Cursus mattis molestie a iaculis. Eu feugiat pretium nibh ipsum consequat nisl. Sed vulputate odio ut enim blandit volutpat. Euismod nisi porta lorem mollis aliquam. Mauris cursus mattis molestie a iaculis. In aliquam sem fringilla ut morbi tincidunt augue. Non diam phasellus vestibulum lorem sed risus ultricies tristique. Tempus iaculis urna id volutpat lacus laoreet non curabitur gravida. Praesent elementum facilisis leo vel fringilla est. Netus et malesuada fames ac turpis. Enim nunc faucibus a pellentesque sit amet porttitor. Tortor consequat id porta nibh venenatis cras sed. Ut enim blandit volutpat maecenas volutpat blandit aliquam etiam.\n",
        "\n",
        "At quis risus sed vulputate odio ut enim blandit. Pharetra pharetra massa massa ultricies mi quis hendrerit dolor. Dictumst vestibulum rhoncus est pellentesque. Fames ac turpis egestas integer eget aliquet nibh praesent tristique. Dolor sit amet consectetur adipiscing elit duis tristique. Sed libero enim sed faucibus turpis in eu mi. Ut tortor pretium viverra suspendisse potenti nullam ac tortor vitae. Non nisi est sit amet facilisis. Est ullamcorper eget nulla facilisi etiam. Erat imperdiet sed euismod nisi porta lorem mollis. At erat pellentesque adipiscing commodo elit at imperdiet dui accumsan. Orci nulla pellentesque dignissim enim sit amet. Dignissim convallis aenean et tortor at risus. Sed adipiscing diam donec adipiscing tristique risus nec feugiat. Lorem ipsum dolor sit amet consectetur adipiscing elit pellentesque. In aliquam sem fringilla ut morbi tincidunt augue. Ut venenatis tellus in metus vulputate eu scelerisque. Morbi tincidunt augue interdum velit.\n",
        "\n",
        "Tristique magna sit amet purus gravida quis. Quis enim lobortis scelerisque fermentum dui. Eu mi bibendum neque egestas congue. Curabitur gravida arcu ac tortor dignissim convallis aenean. Amet mattis vulputate enim nulla. Bibendum est ultricies integer quis auctor elit. Ridiculus mus mauris vitae ultricies. Id ornare arcu odio ut. Pulvinar neque laoreet suspendisse interdum. Porttitor leo a diam sollicitudin. Vel facilisis volutpat est velit egestas. Diam vel quam elementum pulvinar etiam non quam lacus. Pretium vulputate sapien nec sagittis aliquam malesuada bibendum. Sed turpis tincidunt id aliquet risus feugiat in ante. Netus et malesuada fames ac. Imperdiet dui accumsan sit amet nulla facilisi. Imperdiet nulla malesuada pellentesque elit eget gravida. Amet volutpat consequat mauris nunc congue nisi vitae suscipit. Aliquet risus feugiat in ante.\n",
        "\n",
        "Diam vel quam elementum pulvinar etiam non quam. Volutpat ac tincidunt vitae semper quis lectus nulla at volutpat. Scelerisque fermentum dui faucibus in ornare quam. Lacinia quis vel eros donec ac odio tempor orci. Odio tempor orci dapibus ultrices in iaculis nunc sed augue. Montes nascetur ridiculus mus mauris vitae ultricies. Proin sagittis nisl rhoncus mattis rhoncus. Tortor pretium viverra suspendisse potenti nullam. Nisi porta lorem mollis aliquam ut porttitor leo a diam. Tortor at auctor urna nunc id. Volutpat blandit aliquam etiam erat velit scelerisque in dictum. Augue lacus viverra vitae congue eu consequat ac felis. Viverra adipiscing at in tellus integer feugiat scelerisque varius. Dui faucibus in ornare quam viverra orci. Aenean et tortor at risus viverra adipiscing. Suscipit adipiscing bibendum est ultricies integer quis auctor elit sed.\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Mattis pellentesque id nibh tortor. Venenatis tellus in metus vulputate eu scelerisque felis. In dictum non consectetur a. Morbi quis commodo odio aenean sed adipiscing. Amet volutpat consequat mauris nunc congue nisi vitae. Mauris in aliquam sem fringilla. Quis hendrerit dolor magna eget est. Eros in cursus turpis massa tincidunt dui ut ornare. Sed risus ultricies tristique nulla aliquet enim tortor. Nulla pellentesque dignissim enim sit. Eget velit aliquet sagittis id. Gravida cum sociis natoque penatibus et magnis dis parturient montes.\n",
        "\n",
        "At elementum eu facilisis sed odio morbi. Convallis aenean et tortor at risus viverra adipiscing at in. Mi proin sed libero enim. Amet est placerat in egestas erat. Volutpat sed cras ornare arcu. Platea dictumst quisque sagittis purus sit. Quis risus sed vulputate odio ut enim blandit volutpat. Quis hendrerit dolor magna eget. Morbi tristique senectus et netus et malesuada fames ac turpis. Netus et malesuada fames ac turpis egestas integer eget aliquet. Et pharetra pharetra massa massa ultricies mi. Et egestas quis ipsum suspendisse ultrices gravida dictum. Sapien et ligula ullamcorper malesuada proin libero nunc consequat interdum.\n",
        "\n",
        "Sit amet consectetur adipiscing elit duis tristique sollicitudin nibh. Morbi leo urna molestie at elementum. Gravida cum sociis natoque penatibus et magnis. Mauris commodo quis imperdiet massa tincidunt. Nunc consequat interdum varius sit amet mattis. Luctus venenatis lectus magna fringilla urna porttitor rhoncus dolor purus. Etiam non quam lacus suspendisse. Tincidunt praesent semper feugiat nibh. Tristique senectus et netus et. Velit dignissim sodales ut eu sem integer vitae. Ac auctor augue mauris augue. Dolor sit amet consectetur adipiscing elit ut. Et malesuada fames ac turpis egestas sed tempus urna et. Sit amet est placerat in. Odio eu feugiat pretium nibh ipsum consequat nisl vel pretium. Commodo elit at imperdiet dui accumsan sit. In eu mi bibendum neque egestas congue quisque. Sed viverra ipsum nunc aliquet. Tristique risus nec feugiat in. Turpis in eu mi bibendum neque egestas congue.\n",
        "\n",
        "Faucibus ornare suspendisse sed nisi lacus sed viverra tellus in. Feugiat pretium nibh ipsum consequat nisl vel pretium lectus. Vivamus at augue eget arcu dictum varius duis at. Aliquet sagittis id consectetur purus ut faucibus pulvinar elementum integer. Morbi quis commodo odio aenean sed. Duis ut diam quam nulla porttitor massa id neque. Cursus metus aliquam eleifend mi in. Est lorem ipsum dolor sit amet consectetur. Pharetra convallis posuere morbi leo urna molestie at elementum. Amet est placerat in egestas erat.\n",
        "\n",
        "Duis convallis convallis tellus id. In dictum non consectetur a erat nam at lectus. Sit amet volutpat consequat mauris nunc congue nisi vitae suscipit. Quis lectus nulla at volutpat diam ut venenatis tellus in. Quisque id diam vel quam elementum pulvinar. Malesuada pellentesque elit eget gravida cum sociis natoque penatibus. Vel fringilla est ullamcorper eget nulla facilisi etiam dignissim. Lectus sit amet est placerat in. Purus in massa tempor nec feugiat nisl. Eros in cursus turpis massa tincidunt dui ut ornare lectus. At urna condimentum mattis pellentesque id. Aliquet lectus proin nibh nisl condimentum id venenatis a condimentum. Augue ut lectus arcu bibendum at varius vel pharetra. Nec feugiat in fermentum posuere urna. Sed arcu non odio euismod lacinia at quis risus sed. Tristique sollicitudin nibh sit amet. Proin libero nunc consequat interdum varius sit amet mattis. Magna fringilla urna porttitor rhoncus.\n",
        "\n",
        "Tristique nulla aliquet enim tortor at auctor urna nunc id. Etiam dignissim diam quis enim lobortis scelerisque fermentum dui. Vitae sapien pellentesque habitant morbi tristique senectus et netus. Ultricies mi quis hendrerit dolor magna eget est lorem ipsum. Fringilla est ullamcorper eget nulla facilisi etiam. Donec pretium vulputate sapien nec sagittis aliquam malesuada bibendum. Quis blandit turpis cursus in hac habitasse. Felis eget nunc lobortis mattis aliquam faucibus purus in massa. Bibendum ut tristique et egestas quis ipsum. Vel turpis nunc eget lorem dolor sed viverra ipsum nunc. Leo in vitae turpis massa sed elementum tempus egestas. Molestie ac feugiat sed lectus vestibulum mattis ullamcorper velit.\n",
        "\n",
        "Nibh tellus molestie nunc non blandit massa enim. Nisi lacus sed viverra tellus in hac habitasse platea. Egestas sed tempus urna et pharetra pharetra. Non quam lacus suspendisse faucibus interdum posuere lorem ipsum. Augue lacus viverra vitae congue eu consequat ac. In eu mi bibendum neque. Id neque aliquam vestibulum morbi blandit cursus risus at ultrices. Libero enim sed faucibus turpis in eu. Enim diam vulputate ut pharetra. Purus sit amet luctus venenatis. At tellus at urna condimentum mattis pellentesque. Sapien eget mi proin sed. Augue ut lectus arcu bibendum at varius vel. Amet est placerat in egestas erat. Lacus sed viverra tellus in hac habitasse platea.\n",
        "\n",
        "Mattis nunc sed blandit libero. Viverra suspendisse potenti nullam ac tortor vitae purus faucibus. Non diam phasellus vestibulum lorem sed risus ultricies tristique nulla. Non consectetur a erat nam. Magna sit amet purus gravida quis. Tempor id eu nisl nunc mi. Cras adipiscing enim eu turpis egestas pretium. Rhoncus aenean vel elit scelerisque mauris. Odio eu feugiat pretium nibh. Pellentesque id nibh tortor id aliquet. Nisi quis eleifend quam adipiscing vitae proin sagittis nisl rhoncus. Arcu cursus vitae congue mauris rhoncus aenean vel. Praesent elementum facilisis leo vel fringilla est ullamcorper eget nulla. Amet cursus sit amet dictum sit amet justo donec enim. Condimentum mattis pellentesque id nibh.\n",
        "\n",
        "Posuere sollicitudin aliquam ultrices sagittis. Orci porta non pulvinar neque laoreet suspendisse interdum consectetur. Id aliquet risus feugiat in. Dignissim convallis aenean et tortor at risus viverra. Tincidunt dui ut ornare lectus. Dictum non consectetur a erat. Ligula ullamcorper malesuada proin libero. Maecenas volutpat blandit aliquam etiam erat velit. Sed viverra tellus in hac. Volutpat maecenas volutpat blandit aliquam etiam erat velit. Accumsan in nisl nisi scelerisque eu ultrices vitae auctor eu. Gravida quis blandit turpis cursus in hac habitasse platea. Tellus mauris a diam maecenas. Ac felis donec et odio pellentesque diam volutpat. Nisi est sit amet facilisis magna etiam tempor. Adipiscing at in tellus integer feugiat scelerisque varius. Mattis molestie a iaculis at. Sit amet justo donec enim diam vulputate ut.\n",
        "\n",
        "Ante metus dictum at tempor commodo ullamcorper a lacus. Pellentesque adipiscing commodo elit at imperdiet dui. Amet mattis vulputate enim nulla aliquet porttitor lacus luctus. Viverra aliquet eget sit amet tellus. Aliquam purus sit amet luctus venenatis. Imperdiet massa tincidunt nunc pulvinar sapien et ligula ullamcorper. Urna cursus eget nunc scelerisque viverra mauris. Eget est lorem ipsum dolor sit amet. In ante metus dictum at tempor. Parturient montes nascetur ridiculus mus mauris vitae ultricies. Nec dui nunc mattis enim ut tellus elementum. Eget mauris pharetra et ultrices neque ornare aenean euismod elementum. Id aliquet lectus proin nibh nisl condimentum id. Orci porta non pulvinar neque laoreet suspendisse interdum consectetur. Amet mauris commodo quis imperdiet massa tincidunt nunc. Adipiscing elit duis tristique sollicitudin. Viverra adipiscing at in tellus integer feugiat scelerisque varius.\n",
        "\n",
        "Et tortor at risus viverra adipiscing at in tellus. Magna etiam tempor orci eu lobortis elementum nibh. Leo integer malesuada nunc vel risus commodo. Quis eleifend quam adipiscing vitae proin. Quis viverra nibh cras pulvinar mattis. Nullam eget felis eget nunc lobortis mattis aliquam faucibus. Nibh sed pulvinar proin gravida hendrerit lectus. Ut consequat semper viverra nam. Augue ut lectus arcu bibendum at varius vel pharetra vel. Sit amet porttitor eget dolor morbi non. Sollicitudin nibh sit amet commodo. Morbi tristique senectus et netus et malesuada fames ac. Dolor sit amet consectetur adipiscing elit ut aliquam. Venenatis lectus magna fringilla urna porttitor rhoncus dolor.\n",
        "\n",
        "Eu volutpat odio facilisis mauris sit. In hendrerit gravida rutrum quisque non. Curabitur gravida arcu ac tortor dignissim convallis aenean et. Sit amet consectetur adipiscing elit ut aliquam purus sit. Facilisi etiam dignissim diam quis. Risus nec feugiat in fermentum posuere. A arcu cursus vitae congue mauris. Iaculis nunc sed augue lacus viverra vitae. Vulputate dignissim suspendisse in est ante in. Varius morbi enim nunc faucibus a pellentesque sit amet. Lectus nulla at volutpat diam ut venenatis tellus in metus. Cursus turpis massa tincidunt dui ut ornare. Bibendum at varius vel pharetra vel turpis. In hac habitasse platea dictumst vestibulum rhoncus est. Arcu odio ut sem nulla pharetra diam sit amet nisl. Consequat interdum varius sit amet mattis vulputate enim nulla. Potenti nullam ac tortor vitae. At auctor urna nunc id. Pellentesque pulvinar pellentesque habitant morbi tristique senectus et netus. At volutpat diam ut venenatis tellus in metus vulputate eu.\n",
        "\n",
        "Tellus id interdum velit laoreet id donec. Urna cursus eget nunc scelerisque. Risus quis varius quam quisque id diam. Lectus urna duis convallis convallis tellus. Sollicitudin aliquam ultrices sagittis orci a. Egestas tellus rutrum tellus pellentesque eu tincidunt tortor aliquam. Hendrerit gravida rutrum quisque non tellus orci ac. Lacinia at quis risus sed vulputate odio. Lobortis feugiat vivamus at augue eget. Lacus vestibulum sed arcu non odio euismod lacinia at quis. Amet purus gravida quis blandit turpis cursus in. Eleifend quam adipiscing vitae proin sagittis nisl rhoncus. Sed nisi lacus sed viverra tellus. Vitae semper quis lectus nulla at volutpat diam ut. Egestas erat imperdiet sed euismod nisi. Ut etiam sit amet nisl purus. Id consectetur purus ut faucibus pulvinar. Amet mattis vulputate enim nulla aliquet porttitor lacus.\n",
        "\n",
        "Nibh mauris cursus mattis molestie a iaculis at erat. Lacus sed turpis tincidunt id aliquet risus feugiat in. Dui id ornare arcu odio ut sem nulla. Feugiat nibh sed pulvinar proin gravida hendrerit. Sit amet consectetur adipiscing elit. Nulla at volutpat diam ut venenatis tellus. Eget mauris pharetra et ultrices neque ornare aenean euismod elementum. Amet purus gravida quis blandit turpis cursus. Tortor consequat id porta nibh venenatis cras sed. Feugiat in ante metus dictum at tempor commodo. Eget duis at tellus at urna condimentum mattis. Commodo nulla facilisi nullam vehicula. Consectetur lorem donec massa sapien faucibus et. Tellus cras adipiscing enim eu turpis.\n",
        "\n",
        "Elementum eu facilisis sed odio morbi quis. Id ornare arcu odio ut sem nulla pharetra. Sem integer vitae justo eget magna fermentum iaculis. Nisl condimentum id venenatis a. Dui ut ornare lectus sit. Convallis convallis tellus id interdum. Lorem dolor sed viverra ipsum nunc aliquet bibendum enim. In iaculis nunc sed augue lacus viverra vitae congue. Orci nulla pellentesque dignissim enim sit amet venenatis urna cursus. Ac feugiat sed lectus vestibulum mattis. Dignissim convallis aenean et tortor at. Senectus et netus et malesuada. Adipiscing tristique risus nec feugiat in fermentum posuere urna. Ut venenatis tellus in metus vulputate. Amet volutpat consequat mauris nunc congue nisi vitae suscipit.\n",
        "\n",
        "Accumsan sit amet nulla facilisi morbi. Quis enim lobortis scelerisque fermentum dui faucibus in ornare quam. Tristique et egestas quis ipsum suspendisse. Feugiat in fermentum posuere urna nec tincidunt praesent semper feugiat. Et tortor consequat id porta nibh venenatis cras. Dignissim convallis aenean et tortor. Fames ac turpis egestas sed. Magnis dis parturient montes nascetur ridiculus. Non blandit massa enim nec dui nunc mattis enim ut. Sit amet consectetur adipiscing elit ut aliquam purus. Elit sed vulputate mi sit amet mauris commodo. Et molestie ac feugiat sed lectus vestibulum mattis ullamcorper velit. Ultricies leo integer malesuada nunc vel risus commodo viverra.\n",
        "\n",
        "Eros in cursus turpis massa tincidunt dui ut. Tempus imperdiet nulla malesuada pellentesque elit eget gravida cum sociis. Pharetra convallis posuere morbi leo urna. Lectus proin nibh nisl condimentum id venenatis a condimentum vitae. Lectus proin nibh nisl condimentum id venenatis. Sit amet luctus venenatis lectus magna fringilla. Tristique nulla aliquet enim tortor at auctor urna nunc. Suspendisse sed nisi lacus sed viverra. Non quam lacus suspendisse faucibus interdum posuere. Eu augue ut lectus arcu bibendum. Amet cursus sit amet dictum sit. Felis eget nunc lobortis mattis aliquam faucibus. Tristique nulla aliquet enim tortor at auctor urna nunc id. Neque aliquam vestibulum morbi blandit cursus risus at ultrices mi. Velit euismod in pellentesque massa placerat duis ultricies. Imperdiet sed euismod nisi porta lorem mollis aliquam. Aliquam purus sit amet luctus venenatis lectus magna.\n",
        "\n",
        "Nibh praesent tristique magna sit amet. Nisl purus in mollis nunc sed id semper risus in. Non quam lacus suspendisse faucibus interdum. Suspendisse ultrices gravida dictum fusce ut placerat orci. Nunc congue nisi vitae suscipit tellus mauris a diam. Semper eget duis at tellus at urna condimentum mattis. Felis imperdiet proin fermentum leo vel orci porta non pulvinar. Etiam dignissim diam quis enim lobortis. Diam in arcu cursus euismod quis. Suscipit tellus mauris a diam maecenas. Porta nibh venenatis cras sed felis eget velit aliquet. In ante metus dictum at. Elit sed vulputate mi sit amet mauris commodo quis. Arcu ac tortor dignissim convallis aenean et tortor at risus. Turpis nunc eget lorem dolor sed. Luctus accumsan tortor posuere ac ut consequat semper viverra nam. Ipsum dolor sit amet consectetur adipiscing elit.\n",
        "\n",
        "Justo nec ultrices dui sapien eget mi proin sed. Odio facilisis mauris sit amet massa vitae tortor condimentum. Magnis dis parturient montes nascetur ridiculus mus mauris vitae ultricies. Vitae tempus quam pellentesque nec. Cras tincidunt lobortis feugiat vivamus at augue eget arcu. Sit amet purus gravida quis blandit. Augue mauris augue neque gravida in fermentum et sollicitudin ac. Viverra suspendisse potenti nullam ac tortor vitae purus faucibus. Blandit turpis cursus in hac habitasse platea dictumst. Ultricies integer quis auctor elit sed vulputate. Gravida arcu ac tortor dignissim convallis aenean et tortor at. Sodales ut eu sem integer vitae justo eget. Nisl suscipit adipiscing bibendum est ultricies integer quis auctor elit.\n",
        "\n",
        "Egestas purus viverra accumsan in. Fermentum odio eu feugiat pretium nibh ipsum. Venenatis cras sed felis eget. Purus gravida quis blandit turpis cursus in hac habitasse platea. Viverra nibh cras pulvinar mattis nunc sed blandit libero volutpat. Nunc faucibus a pellentesque sit. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Nisl condimentum id venenatis a condimentum vitae sapien pellentesque. Augue eget arcu dictum varius. Suspendisse in est ante in. Tincidunt ornare massa eget egestas purus viverra accumsan in. Venenatis tellus in metus vulputate eu scelerisque felis imperdiet. Vitae tortor condimentum lacinia quis vel eros. Vel pharetra vel turpis nunc eget lorem dolor sed viverra. Risus at ultrices mi tempus imperdiet nulla malesuada pellentesque elit. Ipsum faucibus vitae aliquet nec ullamcorper. Est sit amet facilisis magna etiam tempor orci eu. Vel orci porta non pulvinar neque laoreet suspendisse interdum consectetur. Sit amet facilisis magna etiam. Amet aliquam id diam maecenas ultricies.\n",
        "\n",
        "Quis vel eros donec ac odio. Dui vivamus arcu felis bibendum ut tristique et egestas quis. Egestas sed sed risus pretium quam. Turpis massa tincidunt dui ut ornare lectus. Dictumst vestibulum rhoncus est pellentesque. Sit amet cursus sit amet dictum sit amet justo donec. Eros donec ac odio tempor orci dapibus ultrices. Donec et odio pellentesque diam volutpat commodo sed egestas egestas. Ultrices tincidunt arcu non sodales. Risus quis varius quam quisque id diam. Vestibulum lorem sed risus ultricies tristique nulla. Lorem sed risus ultricies tristique nulla aliquet. Tortor dignissim convallis aenean et tortor at risus viverra. Tortor dignissim convallis aenean et tortor at risus. Maecenas pharetra convallis posuere morbi leo urna molestie at. Tincidunt arcu non sodales neque. Id aliquet risus feugiat in. Congue quisque egestas diam in arcu cursus euismod.\n",
        "\n",
        "Porttitor rhoncus dolor purus non. Facilisis leo vel fringilla est ullamcorper eget nulla facilisi. Massa id neque aliquam vestibulum morbi. Sit amet consectetur adipiscing elit ut aliquam. Orci phasellus egestas tellus rutrum. Ac turpis egestas sed tempus urna et pharetra pharetra massa. Dignissim cras tincidunt lobortis feugiat vivamus at augue. Mi tempus imperdiet nulla malesuada. Senectus et netus et malesuada fames ac turpis. At erat pellentesque adipiscing commodo elit. A iaculis at erat pellentesque. Nunc sed blandit libero volutpat sed cras ornare. Facilisis magna etiam tempor orci eu. Metus aliquam eleifend mi in nulla posuere. Tempor orci eu lobortis elementum nibh tellus molestie nunc. Amet nulla facilisi morbi tempus iaculis. Massa vitae tortor condimentum lacinia quis vel eros donec.\n",
        "\n",
        "Sit amet aliquam id diam maecenas. Donec et odio pellentesque diam volutpat commodo. Tristique senectus et netus et malesuada fames ac turpis egestas. Consequat semper viverra nam libero justo laoreet sit. Aenean et tortor at risus viverra adipiscing at in. Nascetur ridiculus mus mauris vitae ultricies leo. Feugiat in ante metus dictum. Nisl purus in mollis nunc sed. Suspendisse potenti nullam ac tortor vitae purus. Velit egestas dui id ornare arcu. Lacus luctus accumsan tortor posuere ac ut consequat semper.\n",
        "\n",
        "Massa id neque aliquam vestibulum morbi blandit cursus risus at. Odio facilisis mauris sit amet massa vitae tortor condimentum. A iaculis at erat pellentesque. Blandit aliquam etiam erat velit scelerisque in dictum. Velit euismod in pellentesque massa placerat duis ultricies lacus. Felis bibendum ut tristique et egestas quis ipsum suspendisse. Venenatis urna cursus eget nunc scelerisque. Sodales ut etiam sit amet. Id semper risus in hendrerit. Faucibus a pellentesque sit amet porttitor eget dolor. Eget est lorem ipsum dolor sit amet consectetur adipiscing elit. Pretium quam vulputate dignissim suspendisse in est ante in nibh. Odio ut enim blandit volutpat maecenas volutpat blandit aliquam etiam. Ullamcorper a lacus vestibulum sed arcu non. Ipsum a arcu cursus vitae congue mauris rhoncus aenean.\n",
        "\n",
        "Massa eget egestas purus viverra. Scelerisque varius morbi enim nunc faucibus a pellentesque. Vel facilisis volutpat est velit egestas dui id ornare. Donec ac odio tempor orci dapibus ultrices in iaculis. In hac habitasse platea dictumst quisque sagittis purus. Id diam vel quam elementum. Tempor orci eu lobortis elementum nibh tellus molestie. Ut lectus arcu bibendum at varius. Aenean sed adipiscing diam donec adipiscing tristique risus nec feugiat. Arcu bibendum at varius vel pharetra vel turpis nunc eget. Et ligula ullamcorper malesuada proin libero nunc consequat interdum. Diam quam nulla porttitor massa id neque aliquam vestibulum morbi. At tempor commodo ullamcorper a lacus vestibulum sed arcu. Pellentesque habitant morbi tristique senectus et netus et malesuada fames. Porttitor eget dolor morbi non.\n",
        "\n",
        "Sed augue lacus viverra vitae congue eu consequat ac. Lorem dolor sed viverra ipsum nunc aliquet bibendum enim. Tincidunt lobortis feugiat vivamus at augue eget arcu dictum. Lacinia at quis risus sed vulputate odio ut enim. Duis convallis convallis tellus id interdum velit laoreet. Volutpat diam ut venenatis tellus. Mi quis hendrerit dolor magna eget est. Massa vitae tortor condimentum lacinia quis vel. Egestas sed tempus urna et pharetra pharetra massa. Dictum fusce ut placerat orci nulla pellentesque dignissim. Posuere ac ut consequat semper. Mi bibendum neque egestas congue quisque egestas. Nunc mi ipsum faucibus vitae aliquet nec ullamcorper sit.\n",
        "\n",
        "Odio euismod lacinia at quis. Sem nulla pharetra diam sit amet nisl suscipit adipiscing bibendum. Eget lorem dolor sed viverra ipsum nunc. In nisl nisi scelerisque eu ultrices vitae auctor eu. Diam in arcu cursus euismod quis viverra nibh cras pulvinar. Nibh tellus molestie nunc non blandit massa enim nec. Neque volutpat ac tincidunt vitae semper quis. Platea dictumst vestibulum rhoncus est pellentesque elit ullamcorper. Enim tortor at auctor urna nunc id cursus metus. Sit amet nisl suscipit adipiscing bibendum est ultricies integer quis. Pellentesque dignissim enim sit amet venenatis urna cursus. Magnis dis parturient montes nascetur ridiculus.\n",
        "\n",
        "Interdum velit euismod in pellentesque massa. Dictum fusce ut placerat orci nulla pellentesque dignissim enim. Elementum integer enim neque volutpat ac tincidunt. Consequat interdum varius sit amet mattis. Laoreet id donec ultrices tincidunt arcu non sodales neque. Dignissim diam quis enim lobortis scelerisque fermentum dui. Metus aliquam eleifend mi in. Tortor vitae purus faucibus ornare suspendisse sed nisi lacus. Orci phasellus egestas tellus rutrum. Interdum varius sit amet mattis vulputate enim nulla aliquet porttitor. Tellus rutrum tellus pellentesque eu. Morbi tincidunt ornare massa eget egestas. Congue mauris rhoncus aenean vel elit scelerisque. Vitae et leo duis ut diam quam nulla porttitor.\n",
        "\n",
        "Sed turpis tincidunt id aliquet risus feugiat in ante. Quis auctor elit sed vulputate mi sit amet. Phasellus vestibulum lorem sed risus ultricies tristique nulla aliquet enim. Fermentum dui faucibus in ornare quam viverra. Fermentum et sollicitudin ac orci phasellus egestas. Leo vel orci porta non pulvinar. Tellus integer feugiat scelerisque varius morbi enim nunc faucibus a. Ut ornare lectus sit amet est placerat. Egestas quis ipsum suspendisse ultrices gravida dictum fusce ut placerat. Egestas egestas fringilla phasellus faucibus scelerisque eleifend. Et pharetra pharetra massa massa ultricies mi quis hendrerit dolor. Venenatis lectus magna fringilla urna porttitor rhoncus. Leo integer malesuada nunc vel risus commodo viverra maecenas accumsan. Sit amet luctus venenatis lectus magna.\n",
        "\n",
        "In fermentum posuere urna nec tincidunt. Maecenas volutpat blandit aliquam etiam erat velit. Felis imperdiet proin fermentum leo vel orci porta non. Faucibus in ornare quam viverra orci sagittis. Diam quam nulla porttitor massa id neque aliquam vestibulum. Vitae sapien pellentesque habitant morbi tristique senectus et netus. Maecenas sed enim ut sem viverra. Mauris pellentesque pulvinar pellentesque habitant morbi tristique. Amet massa vitae tortor condimentum lacinia quis vel eros. Viverra nibh cras pulvinar mattis nunc. Sit amet mattis vulputate enim nulla aliquet porttitor. Placerat vestibulum lectus mauris ultrices eros in. Dui vivamus arcu felis bibendum ut. Eros in cursus turpis massa tincidunt dui ut ornare. Neque convallis a cras semper auctor. Sagittis purus sit amet volutpat consequat mauris. Vestibulum morbi blandit cursus risus at ultrices. Sagittis orci a scelerisque purus semper eget duis at tellus.\n",
        "\n",
        "Viverra orci sagittis eu volutpat odio. Luctus venenatis lectus magna fringilla. Lacus vestibulum sed arcu non. Dui accumsan sit amet nulla. Ullamcorper dignissim cras tincidunt lobortis. Quis lectus nulla at volutpat diam. A diam sollicitudin tempor id eu nisl nunc. Morbi tristique senectus et netus et malesuada fames ac turpis. Dignissim enim sit amet venenatis urna cursus. Porttitor lacus luctus accumsan tortor posuere ac. Amet justo donec enim diam vulputate ut pharetra. Nunc sed velit dignissim sodales ut eu sem integer vitae.\n",
        "\n",
        "Vestibulum lorem sed risus ultricies tristique nulla aliquet enim tortor. Dictumst vestibulum rhoncus est pellentesque elit ullamcorper dignissim cras tincidunt. Sed vulputate mi sit amet mauris. Ut morbi tincidunt augue interdum velit euismod. In ornare quam viverra orci sagittis eu volutpat. Pellentesque dignissim enim sit amet venenatis urna. Viverra aliquet eget sit amet tellus. Aliquam purus sit amet luctus venenatis lectus magna fringilla urna. Nunc faucibus a pellentesque sit amet porttitor eget. Dui vivamus arcu felis bibendum ut tristique et. Tempor id eu nisl nunc. Vivamus at augue eget arcu dictum varius duis at consectetur. Risus in hendrerit gravida rutrum quisque non tellus. Morbi enim nunc faucibus a pellentesque. Pellentesque eu tincidunt tortor aliquam nulla facilisi cras. At augue eget arcu dictum. Tellus elementum sagittis vitae et. Vitae elementum curabitur vitae nunc sed velit. Tellus id interdum velit laoreet id donec ultrices tincidunt arcu.\n",
        "\n",
        "Placerat vestibulum lectus mauris ultrices eros in. Nunc non blandit massa enim nec dui nunc mattis. Mi ipsum faucibus vitae aliquet nec. Rhoncus urna neque viverra justo nec ultrices dui sapien eget. Quis viverra nibh cras pulvinar mattis nunc sed. Tellus id interdum velit laoreet id. Dictum fusce ut placerat orci nulla pellentesque dignissim enim. Libero enim sed faucibus turpis in eu mi. Ornare lectus sit amet est placerat in egestas. Facilisi etiam dignissim diam quis. Orci dapibus ultrices in iaculis nunc sed augue lacus. Amet luctus venenatis lectus magna fringilla urna porttitor rhoncus dolor. Nisi est sit amet facilisis magna etiam tempor orci eu. At auctor urna nunc id cursus metus aliquam. Massa massa ultricies mi quis hendrerit dolor magna eget. Mauris sit amet massa vitae. Id ornare arcu odio ut sem nulla pharetra diam sit. Odio ut enim blandit volutpat maecenas.\n",
        "\n",
        "Sit amet justo donec enim diam vulputate ut pharetra sit. Velit euismod in pellentesque massa placerat duis ultricies lacus. Egestas integer eget aliquet nibh praesent tristique magna. Vitae congue eu consequat ac felis donec. Porttitor eget dolor morbi non arcu. Tortor dignissim convallis aenean et tortor. Enim nec dui nunc mattis enim ut tellus. Amet nisl suscipit adipiscing bibendum est. Rhoncus urna neque viverra justo nec ultrices dui sapien eget. Tincidunt eget nullam non nisi est sit. Ac odio tempor orci dapibus ultrices in iaculis nunc.\n",
        "\n",
        "Arcu odio ut sem nulla. Nunc lobortis mattis aliquam faucibus. Nisl nunc mi ipsum faucibus vitae aliquet nec ullamcorper. Nulla aliquet enim tortor at auctor urna nunc. Ultricies tristique nulla aliquet enim tortor at auctor urna nunc. Eget nunc lobortis mattis aliquam faucibus purus in. Mauris augue neque gravida in fermentum et sollicitudin ac. Convallis convallis tellus id interdum velit laoreet id donec ultrices. Purus faucibus ornare suspendisse sed nisi lacus sed. Viverra accumsan in nisl nisi scelerisque eu ultrices. Imperdiet nulla malesuada pellentesque elit eget. A arcu cursus vitae congue mauris rhoncus aenean vel elit. In nisl nisi scelerisque eu ultrices vitae auctor eu augue. Habitant morbi tristique senectus et netus et. Varius sit amet mattis vulputate enim. Nisl nisi scelerisque eu ultrices vitae auctor eu augue. Nulla aliquet porttitor lacus luctus accumsan tortor posuere.\n",
        "\n",
        "Eget mi proin sed libero enim sed. Cursus vitae congue mauris rhoncus aenean vel elit scelerisque. Arcu ac tortor dignissim convallis aenean et tortor at risus. Faucibus nisl tincidunt eget nullam non nisi est sit. Ut etiam sit amet nisl purus. Praesent elementum facilisis leo vel fringilla est ullamcorper eget. Massa ultricies mi quis hendrerit dolor magna eget est. Purus semper eget duis at tellus. Vitae turpis massa sed elementum tempus egestas sed sed. Vestibulum mattis ullamcorper velit sed ullamcorper. Ut sem nulla pharetra diam sit. Pellentesque elit eget gravida cum sociis natoque. Dolor sit amet consectetur adipiscing. Volutpat est velit egestas dui id ornare arcu odio. Neque laoreet suspendisse interdum consectetur libero. Adipiscing enim eu turpis egestas. Nulla facilisi morbi tempus iaculis urna.\n",
        "\n",
        "Congue quisque egestas diam in arcu cursus. In iaculis nunc sed augue lacus viverra. Semper quis lectus nulla at. Ut venenatis tellus in metus. Aliquet lectus proin nibh nisl condimentum id venenatis. Turpis in eu mi bibendum neque. Ipsum consequat nisl vel pretium lectus quam. Risus sed vulputate odio ut enim blandit volutpat maecenas. Aliquet risus feugiat in ante metus. Aliquam nulla facilisi cras fermentum odio eu. Egestas purus viverra accumsan in. Hendrerit dolor magna eget est lorem ipsum dolor. Pharetra convallis posuere morbi leo urna molestie at. Pellentesque id nibh tortor id aliquet. Aenean vel elit scelerisque mauris pellentesque pulvinar pellentesque habitant.\n",
        "\n",
        "Ultricies mi quis hendrerit dolor magna eget est. Turpis egestas maecenas pharetra convallis posuere morbi leo urna molestie. Eget aliquet nibh praesent tristique magna sit amet. Tincidunt vitae semper quis lectus nulla at volutpat diam ut. Tellus pellentesque eu tincidunt tortor aliquam nulla facilisi cras fermentum. Massa ultricies mi quis hendrerit dolor magna eget est lorem. Feugiat nisl pretium fusce id velit ut tortor. Amet risus nullam eget felis eget nunc lobortis. Sit amet porttitor eget dolor morbi. Vestibulum morbi blandit cursus risus. Dignissim enim sit amet venenatis urna cursus. Lectus arcu bibendum at varius vel pharetra vel turpis nunc. Arcu bibendum at varius vel pharetra vel turpis. Ut morbi tincidunt augue interdum velit.\n",
        "\n",
        "Sit amet dictum sit amet justo. Dolor morbi non arcu risus quis varius quam quisque. Fusce id velit ut tortor pretium viverra suspendisse potenti. Lobortis feugiat vivamus at augue. Tellus at urna condimentum mattis pellentesque id nibh tortor id. Quam quisque id diam vel quam. Quisque egestas diam in arcu cursus euismod quis. Consectetur libero id faucibus nisl. Felis donec et odio pellentesque. In hac habitasse platea dictumst quisque sagittis.\n",
        "\n",
        "Et tortor at risus viverra adipiscing at in tellus integer. Lobortis elementum nibh tellus molestie nunc non. Mauris sit amet massa vitae tortor condimentum lacinia quis vel. Erat pellentesque adipiscing commodo elit at imperdiet. Quis viverra nibh cras pulvinar mattis nunc sed blandit. Ut ornare lectus sit amet est. Elementum integer enim neque volutpat ac tincidunt. Enim neque volutpat ac tincidunt vitae semper quis lectus. Massa ultricies mi quis hendrerit dolor magna eget. Dictum at tempor commodo ullamcorper. Nulla pharetra diam sit amet nisl. Dignissim suspendisse in est ante in nibh. Mattis rhoncus urna neque viverra justo nec ultrices dui sapien. Luctus accumsan tortor posuere ac ut consequat semper. In arcu cursus euismod quis. Tincidunt augue interdum velit euismod in.\n",
        "\n",
        "Eleifend donec pretium vulputate sapien nec. Nec sagittis aliquam malesuada bibendum arcu vitae. Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra. Viverra vitae congue eu consequat ac felis donec. Hac habitasse platea dictumst quisque sagittis. Egestas dui id ornare arcu. Tortor condimentum lacinia quis vel eros. Elementum integer enim neque volutpat ac tincidunt vitae semper. Viverra vitae congue eu consequat ac felis donec et odio. Massa id neque aliquam vestibulum morbi blandit cursus risus. Aliquet lectus proin nibh nisl condimentum. Ac tortor vitae purus faucibus ornare. Cursus eget nunc scelerisque viverra mauris in. Orci a scelerisque purus semper. Ut diam quam nulla porttitor massa id neque aliquam vestibulum. Lorem ipsum dolor sit amet. Massa vitae tortor condimentum lacinia quis vel. Ultrices mi tempus imperdiet nulla malesuada. Posuere ac ut consequat semper viverra. Odio facilisis mauris sit amet massa vitae tortor.\n",
        "\n",
        "Porta lorem mollis aliquam ut porttitor leo a diam. Est ante in nibh mauris. Fringilla ut morbi tincidunt augue interdum velit euismod in. Phasellus faucibus scelerisque eleifend donec pretium vulputate sapien nec. Non pulvinar neque laoreet suspendisse interdum. At auctor urna nunc id cursus. In nisl nisi scelerisque eu ultrices vitae auctor. Quis eleifend quam adipiscing vitae. Laoreet id donec ultrices tincidunt arcu non sodales. Condimentum vitae sapien pellentesque habitant morbi tristique senectus et. Maecenas sed enim ut sem viverra aliquet. Amet consectetur adipiscing elit ut aliquam purus sit amet.\n",
        "\n",
        "In nisl nisi scelerisque eu ultrices vitae auctor eu augue. Urna cursus eget nunc scelerisque viverra mauris in. Enim sed faucibus turpis in eu. Aliquet lectus proin nibh nisl. Sed lectus vestibulum mattis ullamcorper velit sed ullamcorper morbi tincidunt. Rutrum quisque non tellus orci ac auctor augue. Tellus at urna condimentum mattis pellentesque. Massa tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada. Semper feugiat nibh sed pulvinar proin gravida hendrerit lectus. Et malesuada fames ac turpis. Sagittis nisl rhoncus mattis rhoncus. Faucibus in ornare quam viverra. Turpis massa sed elementum tempus egestas sed. Nec ultrices dui sapien eget mi proin sed. Mi sit amet mauris commodo quis imperdiet massa. Viverra ipsum nunc aliquet bibendum enim. A pellentesque sit amet porttitor eget dolor. Dui vivamus arcu felis bibendum ut tristique et. Augue eget arcu dictum varius duis at consectetur. Diam sollicitudin tempor id eu nisl nunc mi.\n",
        "\n",
        "Congue nisi vitae suscipit tellus. Blandit turpis cursus in hac habitasse platea dictumst quisque sagittis. Est lorem ipsum dolor sit amet consectetur. Egestas fringilla phasellus faucibus scelerisque eleifend donec pretium vulputate. Aliquet lectus proin nibh nisl condimentum id venenatis. Augue neque gravida in fermentum et. Sem integer vitae justo eget. Vitae et leo duis ut diam quam nulla porttitor. Maecenas ultricies mi eget mauris pharetra. Scelerisque in dictum non consectetur a. Sit amet facilisis magna etiam. A pellentesque sit amet porttitor eget dolor. Arcu risus quis varius quam quisque. Nisl suscipit adipiscing bibendum est ultricies. A condimentum vitae sapien pellentesque habitant morbi tristique senectus et. Ipsum consequat nisl vel pretium lectus quam id leo. Risus in hendrerit gravida rutrum quisque non. Ultrices gravida dictum fusce ut placerat.\n",
        "\n",
        "Convallis posuere morbi leo urna molestie. Ac tincidunt vitae semper quis lectus nulla. Semper feugiat nibh sed pulvinar proin gravida hendrerit lectus. Eros donec ac odio tempor orci dapibus ultrices in iaculis. Amet porttitor eget dolor morbi. Sit amet mauris commodo quis imperdiet. Nec ultrices dui sapien eget. Mi tempus imperdiet nulla malesuada pellentesque elit eget. Fringilla ut morbi tincidunt augue interdum velit euismod in pellentesque. Arcu non odio euismod lacinia at. Aliquam faucibus purus in massa tempor nec feugiat nisl. Nulla pharetra diam sit amet nisl. Pharetra sit amet aliquam id diam.\n",
        "\n",
        "Turpis egestas integer eget aliquet nibh praesent tristique. Id diam vel quam elementum pulvinar. Faucibus scelerisque eleifend donec pretium. Accumsan in nisl nisi scelerisque. Urna duis convallis convallis tellus id interdum. Massa id neque aliquam vestibulum. Hendrerit gravida rutrum quisque non tellus orci ac. Risus in hendrerit gravida rutrum quisque. Lacus vel facilisis volutpat est velit egestas. Quisque egestas diam in arcu cursus euismod quis viverra. Molestie nunc non blandit massa enim nec dui nunc mattis. Orci phasellus egestas tellus rutrum tellus pellentesque eu tincidunt tortor.\n",
        "\n",
        "Nunc faucibus a pellentesque sit amet porttitor eget. Lacus vel facilisis volutpat est. Aliquet eget sit amet tellus cras adipiscing. Viverra tellus in hac habitasse platea dictumst vestibulum rhoncus est. Lobortis elementum nibh tellus molestie nunc non blandit massa. Duis at consectetur lorem donec massa sapien faucibus et. Quam viverra orci sagittis eu volutpat. Cursus eget nunc scelerisque viverra mauris in aliquam. Risus nec feugiat in fermentum posuere urna. Tincidunt id aliquet risus feugiat in ante. Massa massa ultricies mi quis hendrerit dolor magna eget. Nunc aliquet bibendum enim facilisis.\n",
        "\n",
        "Mi bibendum neque egestas congue quisque egestas diam in arcu. Massa massa ultricies mi quis hendrerit dolor magna eget est. Cursus in hac habitasse platea. Quam vulputate dignissim suspendisse in est. Semper feugiat nibh sed pulvinar. Nam at lectus urna duis convallis convallis tellus. Potenti nullam ac tortor vitae purus faucibus. Leo in vitae turpis massa sed elementum tempus. Est ullamcorper eget nulla facilisi etiam dignissim diam quis enim. Vehicula ipsum a arcu cursus vitae congue. Commodo sed egestas egestas fringilla phasellus faucibus scelerisque eleifend donec. Non pulvinar neque laoreet suspendisse. Semper eget duis at tellus. Erat imperdiet sed euismod nisi porta lorem mollis.\n",
        "\n",
        "Mi proin sed libero enim. Lacus luctus accumsan tortor posuere ac ut consequat semper viverra. Viverra orci sagittis eu volutpat odio facilisis mauris sit. Mattis enim ut tellus elementum sagittis. Non odio euismod lacinia at quis. Viverra ipsum nunc aliquet bibendum enim. Egestas fringilla phasellus faucibus scelerisque eleifend. Dolor sed viverra ipsum nunc aliquet. Nec feugiat in fermentum posuere urna. Consequat interdum varius sit amet mattis. Amet volutpat consequat mauris nunc congue nisi vitae suscipit. Viverra justo nec ultrices dui sapien eget. Neque convallis a cras semper auctor. Netus et malesuada fames ac turpis egestas integer. Ut porttitor leo a diam sollicitudin. Porttitor lacus luctus accumsan tortor posuere ac ut. Varius vel pharetra vel turpis nunc eget lorem dolor. Est placerat in egestas erat imperdiet sed. Laoreet non curabitur gravida arcu ac tortor dignissim convallis.\n",
        "\n",
        "Ut tortor pretium viverra suspendisse potenti nullam ac. Et tortor at risus viverra adipiscing. Accumsan in nisl nisi scelerisque. Faucibus vitae aliquet nec ullamcorper sit amet risus nullam. Eget mi proin sed libero enim sed faucibus turpis in. Fringilla est ullamcorper eget nulla. Faucibus scelerisque eleifend donec pretium vulputate sapien. Facilisi nullam vehicula ipsum a arcu cursus vitae congue. Gravida in fermentum et sollicitudin ac orci phasellus. Rutrum tellus pellentesque eu tincidunt. Nunc aliquet bibendum enim facilisis gravida. Quam nulla porttitor massa id neque. Aliquam purus sit amet luctus. Luctus accumsan tortor posuere ac ut consequat semper viverra. Facilisi cras fermentum odio eu feugiat pretium nibh ipsum. Cras tincidunt lobortis feugiat vivamus at augue eget. Semper eget duis at tellus at urna condimentum. Ipsum a arcu cursus vitae congue mauris rhoncus.\n",
        "\n",
        "Amet purus gravida quis blandit turpis cursus in. A condimentum vitae sapien pellentesque habitant morbi tristique. Lacus luctus accumsan tortor posuere ac ut consequat semper viverra. Dolor purus non enim praesent elementum. Sed odio morbi quis commodo odio. Scelerisque varius morbi enim nunc faucibus a pellentesque sit. Ipsum suspendisse ultrices gravida dictum. Sagittis nisl rhoncus mattis rhoncus urna neque viverra. Leo vel fringilla est ullamcorper eget nulla facilisi etiam. Semper auctor neque vitae tempus quam pellentesque. At quis risus sed vulputate odio ut enim blandit. Id ornare arcu odio ut sem nulla. Dictumst quisque sagittis purus sit amet volutpat. Sollicitudin ac orci phasellus egestas tellus rutrum tellus. Eleifend donec pretium vulputate sapien nec sagittis aliquam. Justo laoreet sit amet cursus sit amet. Hac habitasse platea dictumst vestibulum rhoncus est pellentesque. Scelerisque eleifend donec pretium vulputate sapien nec. Donec enim diam vulputate ut pharetra sit.\n",
        "\n",
        "In vitae turpis massa sed elementum tempus. Vitae suscipit tellus mauris a diam. Sed id semper risus in hendrerit gravida. Ultrices in iaculis nunc sed. Convallis posuere morbi leo urna molestie at elementum eu. Pulvinar mattis nunc sed blandit libero volutpat sed. Ornare aenean euismod elementum nisi quis eleifend quam. Vitae justo eget magna fermentum iaculis eu non diam phasellus. Fusce id velit ut tortor pretium viverra suspendisse. Maecenas ultricies mi eget mauris pharetra et. Risus at ultrices mi tempus imperdiet nulla malesuada.\n",
        "\n",
        "Non consectetur a erat nam at lectus. Vel facilisis volutpat est velit egestas dui id. Porttitor lacus luctus accumsan tortor. Aliquam id diam maecenas ultricies mi eget. Praesent tristique magna sit amet purus gravida quis blandit turpis. Dolor sed viverra ipsum nunc aliquet bibendum. Interdum varius sit amet mattis vulputate enim. Volutpat est velit egestas dui id ornare arcu odio ut. Pellentesque id nibh tortor id aliquet lectus proin nibh nisl. Eu feugiat pretium nibh ipsum consequat. Nisi porta lorem mollis aliquam ut porttitor leo a diam. Faucibus nisl tincidunt eget nullam non nisi est sit. Tempor commodo ullamcorper a lacus vestibulum sed arcu non odio.\n",
        "\n",
        "Quam adipiscing vitae proin sagittis nisl rhoncus mattis rhoncus urna. In ornare quam viverra orci sagittis eu. In egestas erat imperdiet sed euismod nisi porta lorem. Nulla at volutpat diam ut venenatis tellus. Tortor vitae purus faucibus ornare suspendisse sed nisi lacus. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Velit euismod in pellentesque massa placerat duis ultricies. Sapien pellentesque habitant morbi tristique senectus. Risus nec feugiat in fermentum posuere urna. Sem fringilla ut morbi tincidunt augue interdum velit euismod. Cum sociis natoque penatibus et magnis dis. A lacus vestibulum sed arcu non odio euismod. Elementum nibh tellus molestie nunc non blandit massa enim nec. Iaculis urna id volutpat lacus laoreet. Neque convallis a cras semper auctor neque vitae tempus. Orci porta non pulvinar neque laoreet suspendisse. Molestie a iaculis at erat pellentesque adipiscing commodo elit. Quis risus sed vulputate odio ut enim.\n",
        "\n",
        "Nam at lectus urna duis convallis. Adipiscing elit pellentesque habitant morbi tristique senectus. Sed lectus vestibulum mattis ullamcorper velit sed ullamcorper. Blandit volutpat maecenas volutpat blandit aliquam etiam erat velit scelerisque. Ornare quam viverra orci sagittis eu volutpat. Posuere ac ut consequat semper viverra nam. Mollis nunc sed id semper risus in. Lectus proin nibh nisl condimentum. Tincidunt augue interdum velit euismod in pellentesque massa. Commodo elit at imperdiet dui accumsan sit amet nulla. Ultrices mi tempus imperdiet nulla malesuada pellentesque elit eget. Pellentesque diam volutpat commodo sed egestas egestas fringilla phasellus faucibus. Etiam non quam lacus suspendisse faucibus. Consectetur libero id faucibus nisl. Sit amet est placerat in egestas erat imperdiet sed. Neque laoreet suspendisse interdum consectetur libero id. Id cursus metus aliquam eleifend mi in. Dignissim cras tincidunt lobortis feugiat vivamus at augue eget. Id aliquet risus feugiat in. Tortor aliquam nulla facilisi cras fermentum odio.\n",
        "\n",
        "Consectetur lorem donec massa sapien. Id velit ut tortor pretium viverra suspendisse. Massa sed elementum tempus egestas sed. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Eget velit aliquet sagittis id consectetur purus. Etiam erat velit scelerisque in dictum non consectetur a. Dignissim suspendisse in est ante in nibh. Tincidunt praesent semper feugiat nibh sed pulvinar proin gravida hendrerit. Convallis aenean et tortor at risus viverra adipiscing at. Lacinia quis vel eros donec ac odio tempor orci. Interdum varius sit amet mattis vulputate enim. Eget felis eget nunc lobortis mattis aliquam faucibus. Quisque egestas diam in arcu cursus euismod quis viverra nibh. Sed viverra tellus in hac habitasse platea. Nulla pharetra diam sit amet nisl suscipit adipiscing. Consequat nisl vel pretium lectus quam.\n",
        "\n",
        "Habitant morbi tristique senectus et. Pretium nibh ipsum consequat nisl vel pretium. Egestas tellus rutrum tellus pellentesque eu tincidunt tortor aliquam nulla. Pellentesque adipiscing commodo elit at imperdiet dui accumsan sit. Habitant morbi tristique senectus et netus et malesuada fames. Vitae suscipit tellus mauris a diam maecenas sed. Lorem ipsum dolor sit amet. Mollis nunc sed id semper risus in hendrerit. Lectus arcu bibendum at varius vel pharetra vel turpis. Enim ut sem viverra aliquet eget sit amet. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Interdum velit euismod in pellentesque. Enim sed faucibus turpis in eu mi bibendum neque egestas. Eu sem integer vitae justo. Curabitur gravida arcu ac tortor dignissim. Donec pretium vulputate sapien nec sagittis. Nulla posuere sollicitudin aliquam ultrices sagittis. Habitant morbi tristique senectus et netus et.\n",
        "\n",
        "Feugiat scelerisque varius morbi enim nunc faucibus. Hac habitasse platea dictumst vestibulum rhoncus est pellentesque elit. Posuere urna nec tincidunt praesent. Sed turpis tincidunt id aliquet risus feugiat in. Sapien et ligula ullamcorper malesuada proin libero nunc consequat. Sem viverra aliquet eget sit amet tellus cras. Tellus id interdum velit laoreet id. Ipsum faucibus vitae aliquet nec ullamcorper sit. Mi tempus imperdiet nulla malesuada pellentesque elit eget gravida. Fusce id velit ut tortor. Sed enim ut sem viverra aliquet eget sit amet tellus. Risus nullam eget felis eget nunc. Nec sagittis aliquam malesuada bibendum arcu vitae elementum curabitur vitae. Vitae auctor eu augue ut lectus arcu bibendum at varius. Et malesuada fames ac turpis. Est pellentesque elit ullamcorper dignissim cras.\n",
        "\n",
        "Ornare massa eget egestas purus viverra. Bibendum est ultricies integer quis auctor elit sed vulputate mi. In fermentum et sollicitudin ac orci. Scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique. Venenatis cras sed felis eget velit. At lectus urna duis convallis convallis. Diam donec adipiscing tristique risus nec. Erat nam at lectus urna duis convallis convallis. Diam quis enim lobortis scelerisque fermentum dui faucibus in ornare. Convallis tellus id interdum velit laoreet id donec ultrices tincidunt. Tincidunt eget nullam non nisi.\n",
        "\n",
        "Pellentesque nec nam aliquam sem. Ipsum dolor sit amet consectetur adipiscing elit pellentesque habitant. Vulputate sapien nec sagittis aliquam malesuada. In metus vulputate eu scelerisque felis imperdiet proin fermentum. Malesuada bibendum arcu vitae elementum curabitur vitae nunc sed velit. Nec ultrices dui sapien eget mi proin. Eu facilisis sed odio morbi quis commodo odio. Sit amet luctus venenatis lectus magna. Ac odio tempor orci dapibus ultrices in iaculis. At tempor commodo ullamcorper a. Eu mi bibendum neque egestas congue quisque egestas. Donec adipiscing tristique risus nec feugiat in fermentum. Semper risus in hendrerit gravida rutrum quisque non. Elementum nibh tellus molestie nunc non blandit massa.\n",
        "\n",
        "Viverra nibh cras pulvinar mattis nunc sed blandit. Dignissim enim sit amet venenatis. Adipiscing bibendum est ultricies integer quis auctor. Morbi leo urna molestie at elementum eu facilisis sed odio. Eget est lorem ipsum dolor sit amet. Ut venenatis tellus in metus vulputate eu scelerisque felis imperdiet. Quam pellentesque nec nam aliquam sem et tortor consequat id. Est ante in nibh mauris. Viverra suspendisse potenti nullam ac tortor vitae. Nulla facilisi nullam vehicula ipsum a arcu. At volutpat diam ut venenatis tellus. Convallis a cras semper auctor neque vitae. Vitae semper quis lectus nulla.\n",
        "\n",
        "In fermentum et sollicitudin ac orci phasellus. Justo donec enim diam vulputate ut pharetra sit. Mauris vitae ultricies leo integer malesuada nunc. Eget egestas purus viverra accumsan. Pellentesque pulvinar pellentesque habitant morbi. Aliquam sem et tortor consequat id. Cum sociis natoque penatibus et. Volutpat maecenas volutpat blandit aliquam. Urna nec tincidunt praesent semper feugiat nibh sed pulvinar proin. Eget sit amet tellus cras adipiscing. Sed adipiscing diam donec adipiscing tristique risus.\n",
        "\n",
        "Morbi blandit cursus risus at ultrices mi tempus. Viverra nibh cras pulvinar mattis nunc sed blandit libero volutpat. Justo donec enim diam vulputate ut pharetra sit amet. Nunc consequat interdum varius sit. Adipiscing tristique risus nec feugiat in. Pretium nibh ipsum consequat nisl vel. Amet nulla facilisi morbi tempus iaculis urna id volutpat lacus. Non odio euismod lacinia at quis. Elit ullamcorper dignissim cras tincidunt lobortis feugiat. Sit amet porttitor eget dolor. Malesuada pellentesque elit eget gravida cum. Viverra accumsan in nisl nisi scelerisque.\n",
        "\n",
        "Lectus magna fringilla urna porttitor rhoncus dolor purus non. Vel risus commodo viverra maecenas accumsan. Purus in massa tempor nec feugiat. Volutpat diam ut venenatis tellus. Mi eget mauris pharetra et. Morbi enim nunc faucibus a. Vel orci porta non pulvinar neque laoreet suspendisse. Eget sit amet tellus cras adipiscing enim eu. Ac felis donec et odio pellentesque diam. Amet mauris commodo quis imperdiet massa tincidunt nunc pulvinar. Sit amet mauris commodo quis imperdiet massa tincidunt nunc. Orci dapibus ultrices in iaculis nunc sed augue lacus. Integer eget aliquet nibh praesent. \n",
        "\n",
        "Research Article\n",
        "Deep Learning Based Abstractive Text Summarization:\n",
        "Approaches, Datasets, Evaluation Measures, and Challenges\n",
        "Dima Suleiman and Arafat Awajan\n",
        "Princess Sumaya University for Technology, Amman, Jordan\n",
        "Correspondence should be addressed to Dima Suleiman; d.suleiman@psut.edu.jo\n",
        "Received 24 April 2020; Revised 1 July 2020; Accepted 25 July 2020; Published 24 August 2020\n",
        "Academic Editor: Dimitris Mourtzis\n",
        "Copyright © 2020 Dima Suleiman and Arafat Awajan. *is is an open access article distributed under the Creative Commons\n",
        "Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is\n",
        "properly cited.\n",
        "In recent years, the volume of textual data has rapidly increased, which has generated a valuable resource for extracting and\n",
        "analysing information. To retrieve useful knowledge within a reasonable time period, this information must be summarised. *is\n",
        "paper reviews recent approaches for abstractive text summarisation using deep learning models. In addition, existing datasets for\n",
        "training and validating these approaches are reviewed, and their features and limitations are presented. *e Gigaword dataset is\n",
        "commonly employed for single-sentence summary approaches, while the Cable News Network (CNN)/Daily Mail dataset is\n",
        "commonly employed for multisentence summary approaches. Furthermore, the measures that are utilised to evaluate the quality\n",
        "of summarisation are investigated, and Recall-Oriented Understudy for Gisting Evaluation 1 (ROUGE1), ROUGE2, and ROUGEL are determined to be the most commonly applied metrics. *e challenges that are encountered during the summarisation\n",
        "process and the solutions proposed in each approach are analysed. *e analysis of the several approaches shows that recurrent\n",
        "neural networks with an attention mechanism and long short-term memory (LSTM) are the most prevalent techniques for\n",
        "abstractive text summarisation. *e experimental results show that text summarisation with a pretrained encoder model achieved\n",
        "the highest values for ROUGE1, ROUGE2, and ROUGE-L (43.85, 20.34, and 39.9, respectively). Furthermore, it was determined\n",
        "that most abstractive text summarisation models faced challenges such as the unavailability of a golden token at testing time, outof-vocabulary (OOV) words, summary sentence repetition, inaccurate sentences, and fake facts.\n",
        "1. Introduction\n",
        "Currently, there are vast quantities of textual data available,\n",
        "including online documents, articles, news, and reviews that\n",
        "contain long strings of text that need to be summarised [1].\n",
        "*e importance of text summarisation is due to several\n",
        "reasons, including the retrieval of significant information\n",
        "from a long text within a short period, easy and rapid loading\n",
        "of the most important information, and resolution of the\n",
        "problems associated with the criteria needed for summary\n",
        "evaluation [2]. Due to the evolution and growth of automatic\n",
        "text summarisation methods, which have provided significant results in many languages, these methods need to be\n",
        "reviewed and summarised. *erefore, in this review, we\n",
        "surveyed the most recent methods and focused on the\n",
        "techniques, datasets, evaluation measures, and challenges of\n",
        "each approach, in addition to the manner in which each\n",
        "method addressed challenges.\n",
        "Applications such as search engines and news websites\n",
        "use text summarisation [1]. In search engines, previews are\n",
        "produced as snippets, and news websites generate headlines\n",
        "to describe the news to facilitate knowledge retrieval [3, 4].\n",
        "Text summarisation can be divided into several categories\n",
        "based on function, genre, summary context, type of summarizer, and number of documents [5]; one specific text\n",
        "summarisation classification approach divides the summarisation process into extractive and abstractive categories [6].\n",
        "Extractive summarisation extracts or copies some parts\n",
        "from the original text based on scores computed using either\n",
        "statistical features or linguistic features, while abstractive\n",
        "summarisation rephrases the original text to generate new\n",
        "phrases that may not be in the original text, which is\n",
        "Hindawi\n",
        "Mathematical Problems in Engineering\n",
        "Volume 2020, Article ID 9365340, 29 pages\n",
        "https://doi.org/10.1155/2020/9365340considered a difficult task for a computer. As abstractive text\n",
        "summarisation requires an understanding of the document\n",
        "to generate the summary, advanced machine learning\n",
        "techniques and extensive natural language processing (NLP)\n",
        "are required. *us, abstractive summarisation is harder than\n",
        "extractive summarisation since abstractive summarisation\n",
        "requires real-word knowledge and semantic class analysis [7].\n",
        "However, abstractive summarisation is also better than extractive summarisation since the summary is an approximate\n",
        "representation of a human-generated summary, which makes\n",
        "it more meaningful [8]. For both types, acceptable summarisation should have the following: sentences that maintain\n",
        "the order of the main ideas and concepts presented in the\n",
        "original text, minimal to no repetition, sentences that are\n",
        "consistent and coherent, and the ability to remember the\n",
        "meaning of the text, even for long sentences [7]. In addition,\n",
        "the generated summary must be compact while conveying\n",
        "important information about the original text [2, 9].\n",
        "Abstractive text summarisation approaches include\n",
        "structured and semantic-based approaches. Structured approaches encode the crucial features of documents using\n",
        "several types of schemas, including tree, ontology, lead and\n",
        "body phrases, and template and rule-based schemas, while\n",
        "semantic-based approaches are more concerned with the\n",
        "semantics of the text and thus rely on the information\n",
        "representation of the document to summarise the text.\n",
        "Semantic-based approaches include the multimodal semantic method, information item method, and semantic\n",
        "graph-based method [10–17].\n",
        "Deep learning techniques were employed in abstractive\n",
        "text summarisation for the first time in 2015 [18], and the\n",
        "proposed model was based on the encoder-decoder architecture. For these applications, deep learning techniques\n",
        "have provided excellent results and have been extensively\n",
        "employed in recent years.\n",
        "Raphal et al. surveyed several abstractive text summarisation processes in general [19]. *eir study differentiated\n",
        "between different model architectures, such as reinforcement learning (RL), supervised learning, and attention\n",
        "mechanism. In addition, comparisons in terms of word\n",
        "embedding, data processing, training, and validation had\n",
        "been performed. However, there are no comparisons of the\n",
        "quality of several models that generated summaries.\n",
        "Furthermore, both extractive and abstractive summarisation models were summarised in [20, 21]. In [20], the\n",
        "classification of summarisation tasks was based on three factors:\n",
        "input factors, purpose factors, and output factors. Dong and\n",
        "Mahajani et al. surveyed only five abstractive summarisation\n",
        "models each. On the other hand, Mahajani et al. focused on the\n",
        "datasets and training techniques in addition to the architecture\n",
        "of several abstractive summarisation models [21]. However, the\n",
        "quality of the generated summary of the different techniques\n",
        "and the evaluation measures were not discussed.\n",
        "Shi et al. presented a comprehensive survey of several\n",
        "abstractive text summarisation models, which are based on\n",
        "sequence-to-sequence encoder-decoder architecture for\n",
        "convolutional and RNN seq2seq models. *e focus was the\n",
        "structure of the network, training strategy, and the algorithms\n",
        "employed to generate the summary [22]. Although several\n",
        "papers have analysed abstractive summarisation models, few\n",
        "papers have performed a comprehensive study [23]. Moreover, most of the previous surveys covered the techniques\n",
        "until 2018, even though surveys were published in 2019 and\n",
        "2020, such as [20, 21]. In this review, we addressed most of the\n",
        "recent deep learning-based RNN abstractive text summarisation models. Furthermore, this survey is the first to address recent techniques applied in abstractive summarisation,\n",
        "such as Transformer.\n",
        "*is paper provides an overview of the approaches,\n",
        "datasets, evaluation measures, and challenges of deep learningbased abstractive text summarisation, and each topic is\n",
        "discussed and analysed. We classified the approaches based on\n",
        "the output type into: single-sentence summary and multisentence summary approaches. Also, within each classification,\n",
        "we compared between the approaches in terms of architecture,\n",
        "dataset, dataset preprocessing, evaluation, and results. *e\n",
        "remainder of this paper is organised as follows: Section 2\n",
        "introduces a background of several deep learning models and\n",
        "techniques, such as the recurrent neural network (RNN),\n",
        "bidirectional RNN, attention mechanisms, long short-term\n",
        "memory (LSTM), gated recurrent unit (GRU), and sequenceto-sequence models. Section 3 describes the most recent singlesentence summarisation approaches, while the multisentence\n",
        "summarisation approaches are covered in Section 4. Section 5\n",
        "and Section 6 investigate datasets and evaluation measures,\n",
        "respectively. Section 7 discusses the challenges of the summarisation process and solutions to these challenges. Conclusions and discussion are provided in Section 8.\n",
        "2. Background\n",
        "Deep learning analyses complex problems to facilitate the\n",
        "decision-making process. Deep learning attempts to imitate\n",
        "what the human brain can achieve by extracting features at\n",
        "different levels of abstraction. Typically, higher-level layers\n",
        "have fewer details than lower-level layers [24]. *e output\n",
        "layer will produce an output by nonlinearly transforming the\n",
        "input from the input layer. *e hierarchical structure of deep\n",
        "learning can support learning. *e level of abstraction of a\n",
        "certain layer will determine the level of abstraction of the\n",
        "next layer since the output of one layer will be the input of\n",
        "the next layer. In addition, the number of layers determines\n",
        "the deepness, which affects the level of learning [25].\n",
        "Deep learning is applied in several NLP tasks since it\n",
        "facilitates the learning of multilevel hierarchal representations of data using several data processing layers of nonlinear\n",
        "units [24, 26–28]. Various deep learning models have been\n",
        "employed for abstractive summarisation, including RNNs,\n",
        "convolutional neural networks (CNNs), and sequence-tosequence models. We will cover deep learning models in\n",
        "more detail in this section.\n",
        "2.1. RNN Encoder-Decoder Summarization. RNN encoderdecoder architecture is based on the sequence-to-sequence\n",
        "model. *e sequence-to-sequence model maps the input\n",
        "sequence in the neural network to a similar sequence that\n",
        "consists of characters, words, or phrases. *is model is\n",
        "2 Mathematical Problems in Engineeringutilised in several NLP applications, such as machine\n",
        "translation and text summarisation. In text summarisation,\n",
        "the input sequence is the document that needs to be\n",
        "summarised, and the output is the summary [29, 30], as\n",
        "shown in Figure 1.\n",
        "An RNN is a deep learning model that is applied to\n",
        "process data in sequential order such that the input of a\n",
        "certain state depends on the output of the previous state\n",
        "[31, 32]. For example, in a sentence, the meaning of a word is\n",
        "closely related to the meaning of the previous words. An\n",
        "RNN consists of a set of hidden states that are learned by the\n",
        "neural network. An RNN may consist of several layers of\n",
        "hidden states, where states and layers learn different features.\n",
        "*e last state of each layer represents the whole inputs of the\n",
        "layer since it accumulates the values of all previous states [5].\n",
        "For example, the first layer and its state can be employed for\n",
        "part-of-speech tagging, while the second layer learns to\n",
        "create phrases. In text summarisation, the input for the RNN\n",
        "is the embedding of words, phrases, or sentences, and the\n",
        "output is the word embedding of the summary [5].\n",
        "In the RNN encoder-decoder model, at the encoder side,\n",
        "at certain hidden states, the vector representation of the\n",
        "current input word and the output of the hidden states of all\n",
        "previous words are combined and fed to the next hidden\n",
        "state. As shown in Figure 1, the vector representation of the\n",
        "word W3 and the output of the hidden states he1 and he2 are\n",
        "combined and fed as input to the hidden states he3. After\n",
        "feeding all the words of the input string, the output generated from the last hidden state of the encoder are fed to the\n",
        "decoder as a vector referred to as the context vector [29]. In\n",
        "addition to the context vector, which is fed to the first hidden\n",
        "state of the decoder, the start-of-sequence symbol 〈SOS〉 is\n",
        "fed to generate the first word of the summary from the\n",
        "headline (assume W5, as shown in Figure 1). In this case, W5\n",
        "is fed as the input to the next decoder hidden state. Each\n",
        "generated word is passed as an input to the next decoder\n",
        "hidden state to generate the next word of the summary. *e\n",
        "last generated word is the end-of-sequence symbol 〈EOS〉.\n",
        "Before generating the summary, each output from the decoder will take the form of a distributed representation\n",
        "before it is sent to the softmax layer and attention mechanism to generate the next summary [29].\n",
        "2.2.Bidirectional RNN. Bidirectional RNN consists of forward\n",
        "RNNs and backward RNNs. Forward RNNs generate a sequence of hidden states after reading the input sequence from\n",
        "left to right. On the other hand, the backward RNNs generate a\n",
        "sequence of hidden states after reading the input sequence from\n",
        "right to left. *e representation of the input sequence is the\n",
        "concatenation of the forward and backward RNNs [33].\n",
        "*erefore, the representation of each word depends on the\n",
        "representation of the preceding (past) and following (future)\n",
        "words. In this case, the context will contain the words to the left\n",
        "and the words to the right of the current word [34].\n",
        "Using bidirectional RNN enhances the performance. For\n",
        "example, if we have the following input text “Sara ate a\n",
        "delicious pizza at dinner tonight,” in this case, assume that\n",
        "we want to predict the representation of the word “dinner,”\n",
        "using bidirectional RNN and the forward LSTMs represent\n",
        "“Sara ate a delicious pizza at” while the backward LSTM\n",
        "represents “tonight.” Considering the word “tonight” when\n",
        "representing the word “dinner” provides better results.\n",
        "On the other hand, using the bidirectional RNN at the\n",
        "decoder size minimizes the probability of the wrong prediction. *e reason for this is that the unidirectional RNN\n",
        "only considers the previous prediction and reason only\n",
        "about the past. *erefore, if there is an error in previous\n",
        "prediction, the error will accumulate in all subsequent\n",
        "predictions, and this problem can be addressed using the\n",
        "bidirectional RNN [35].\n",
        "2.3. Gated Recurrent Neural Networks (LSTM and GRU).\n",
        "Gated RNNs are employed to solve the problem of vanishing\n",
        "gradients, which occurs when training a long sequence using\n",
        "an RNN. *is problem can be solved by allowing the gradients to backpropagate along a linear path using gates,\n",
        "where each gate has a weight and a bias. Gates can control\n",
        "and modify the amount of information that flows between\n",
        "hidden states. During training, the weights and biases of the\n",
        "gates are updated. *e most popular gated RNNs are LSTM\n",
        "[36] and GRU [37], which are two variants of an RNN.\n",
        "2.3.1. Long Short-Term Memory (LSTM). *e repeating unit\n",
        "of the LSTM architecture consists of input/read, memory/\n",
        "update, forget, and output gates [5, 7], but the chaining\n",
        "structure is the same as that of an RNN. *e four gates share\n",
        "information with each other; thus, information can flow in\n",
        "loops for a long period of time. *e four gates of each LSTM\n",
        "unit, which are shown in Figures 2 and 3, are discussed here.\n",
        "(1) Input Gate. In the first timestep, the input is a vector that is\n",
        "initialised randomly, while in subsequent steps, the input of the\n",
        "current step is the output (content of the memory cell) of the\n",
        "previous step. In all cases, the input is subject to element-wise\n",
        "multiplication with the output of the forget gate. *e multiplication result is added to the current memory gate output.\n",
        "(2) Forget Gate. A forget gate is a neural network with one\n",
        "layer and a sigmoid activation function. *e value of the\n",
        "sigmoid function will determine if the information of the\n",
        "previous state should be forgotten or remembered. If the\n",
        "sigmoid value is 1, then the previous state will be remembered, but if the sigmoid value is 0, then the previous state\n",
        "will be forgotten. In language modelling, for example, the\n",
        "forget gate remembers the gender of the subject to produce\n",
        "the proper pronouns until it finds a new subject. *ere are\n",
        "four inputs for the forget gate: the output of the previous\n",
        "block, the input vector, the remembered information from\n",
        "the previous block, and the bias.\n",
        "(3) Memory Gate. *e memory gate controls the effect of the\n",
        "remembered information on the new information. *e\n",
        "memory gate consists of two neural networks. *e first\n",
        "network has the same structure as the forget gate but a\n",
        "different bias, and the second neural network has a tanh\n",
        "activation function and is utilised to generate the new\n",
        "Mathematical Problems in Engineering 3+ + + +\n",
        "Ct–1 X + Ct\n",
        "Ht\n",
        "–1\n",
        "X\n",
        "σ t\n",
        "σ σ\n",
        "X\n",
        "Tanh\n",
        "Tanh\n",
        "X\n",
        "Ht\n",
        "Ht\n",
        "Input\n",
        "vector\n",
        "Memory\n",
        "from\n",
        "previous\n",
        "block\n",
        "Output\n",
        "of\n",
        "previous\n",
        "block\n",
        "Output\n",
        "of\n",
        "current\n",
        "block\n",
        "Element-wise\n",
        "multiplication\n",
        "Element-wise\n",
        "summation/\n",
        "concatenation\n",
        "Memory\n",
        "from\n",
        "current\n",
        "block\n",
        "Sigmoid\n",
        "Hyperbolic\n",
        "tangent\n",
        "Bias\n",
        "0 1 2 3\n",
        "Figure 2: LSTM unit architecture [5].\n",
        "he1\n",
        "W1 W2 W3 W4\n",
        "he2 he3 he4 hd1\n",
        "<SOS> W5\n",
        "W5 W6 W7 <EOS>\n",
        "W6 W7\n",
        "hd2 hd3 hd4\n",
        "Context\n",
        "vector\n",
        "Encoder Decoder\n",
        "Figure 1: Sequence-to-sequence; the last hidden state of the encoder is fed as input to the decoder with the symbol EOS [51].\n",
        "+ + + +\n",
        "X\n",
        "σ σ\n",
        "Ht\n",
        "Ct–1 + Ct\n",
        "Ht\n",
        "–1\n",
        "X\n",
        "t\n",
        "0 1 2 3\n",
        "σ Tanh\n",
        "Tanh\n",
        "X\n",
        "Ht\n",
        "X\n",
        "(a)\n",
        "+ + + +\n",
        "X\n",
        "σ σ\n",
        "Ht\n",
        "Ct–1 + Ct\n",
        "Ht\n",
        "–1\n",
        "X\n",
        "t\n",
        "0 1 2 3\n",
        "σ Tanh\n",
        "Tanh\n",
        "X\n",
        "Ht\n",
        "X\n",
        "(b)\n",
        "Figure 3: Continued.\n",
        "4 Mathematical Problems in Engineeringinformation. *e new information is formed by adding the\n",
        "old information to the result of the element-wise multiplication of the output of the two memory gate neural\n",
        "networks.\n",
        "(4) Output Gate. *e output gates control the amount of new\n",
        "information that is forwarded to the next LSTM unit. *e\n",
        "output gate is a neural network with a sigmoid activation\n",
        "function that considers the input vector, the previous hidden\n",
        "state, the new information, and the bias as input. *e output\n",
        "of the sigmoid function is multiplied by the tanh of the new\n",
        "information to produce the output of the current block.\n",
        "2.3.2. Gated Recurrent Unit (GRU). A GRU is a simplified\n",
        "LSTM with two gates, a reset gate and an update gate, and\n",
        "there is no explicit memory. *e previous hidden state information is forgotten when all the reset gate elements approach zero; then, only the input vector affects the candidate\n",
        "hidden state. In this case, the update gate acts as a forget gate.\n",
        "LSTM and GRU are commonly employed for abstractive\n",
        "summarisation since LSTM has a memory unit that provides\n",
        "extra control; however, the computation time of the GRU is\n",
        "reduced [38]. In addition, while it is easier to tune the parameters with LSTM, the GRU takes less time to train [30].\n",
        "2.4. Attention Mechanism. *e attention mechanism was\n",
        "employed for neural machine translation [33] before being\n",
        "utilised for NLP tasks such as text summarisation [18]. A\n",
        "basic encoder-decoder architecture may fail when given long\n",
        "sentences since the size of encoding is fixed for the input\n",
        "string; thus, it cannot consider all the elements of a long\n",
        "input. To remember the input that has a significant impact\n",
        "on the summary, the attention mechanism was introduced\n",
        "[29]. *e attention mechanism is employed at each output\n",
        "word to calculate the weight between the output word and\n",
        "every input word; the weights add to one. *e advantage of\n",
        "using weights is to show which input word must receive\n",
        "attention with respect to the output word. *e weighted\n",
        "average of the last hidden layers of the decoder in the current\n",
        "step is calculated after passing each input word and fed to the\n",
        "softmax layer along the last hidden layers [39].\n",
        "2.5. Beam Search. Beam search and greedy search are very\n",
        "similar; however, while greedy search considers only the best\n",
        "hypothesis, beam search considers b hypotheses, where b\n",
        "represents the beam width or beam size [5]. In text summarisation tasks, the decoder utilises the final encoder\n",
        "representation to generate the summary from the target\n",
        "vocabulary. In each step, the output of the decoder is a\n",
        "probability distribution over the target word. *us, to obtain\n",
        "the output word from the learned probability, several\n",
        "methods can be applied, including (1) greedy sampling,\n",
        "which selects the distribution mode, (2) 1-best or beam\n",
        "search, which selects the best output, and (3) n-best or beam\n",
        "search, which select several outputs. When n-best beam\n",
        "search is employed, the top b most relevant target words are\n",
        "selected from the distribution and fed to the next decoder\n",
        "state. *e decoder keeps only the top k1 of k words from the\n",
        "different inputs and discards the rest.\n",
        "2.6. Distributed Representation (Word Embedding). A word\n",
        "embedding is a word distributional vector representation\n",
        "that represents the syntax and semantic features of words\n",
        "[40]. Words must be converted to vectors to handle various\n",
        "NLP challenges such that the semantic similarity between\n",
        "words can be calculated using cosine similarity, Euclidean\n",
        "distance, etc. [41–43]. In NLP tasks, the word embeddings of\n",
        "the words are fed as inputs to neural network models. In the\n",
        "recurrent neural network encoder-decoder architecture,\n",
        "which is employed to generate the summaries, the input of\n",
        "the model is the word embedding of the text, and the output\n",
        "is the word embedding of the summary.\n",
        "In NLP, there are several word embedding models, such as\n",
        "Word2Vec, GloVe, FastText, and Bidirectional Encoder Representations from Transformers (BERT), which are the most\n",
        "recently employed word embedding models [41, 44–47]. *e\n",
        "Word2Vec model consists of two approaches, skip-gram and\n",
        "continuous bag-of-words (CBOW), which both depend on the\n",
        "context window [41]. On the other hand, GloVe represents the\n",
        "+ + + +\n",
        "X\n",
        "σ σ\n",
        "Ht\n",
        "Ct-1 + Ct\n",
        "Ht\n",
        "-1\n",
        "X\n",
        "t\n",
        "0 1 2 3\n",
        "σ Tanh\n",
        "Tanh\n",
        "X\n",
        "H\n",
        "t\n",
        "X\n",
        "(c)\n",
        "+ + + +\n",
        "X\n",
        "σ σ\n",
        "Ht\n",
        "Ct–1 + Ct\n",
        "H\n",
        "t–1\n",
        "X\n",
        "t\n",
        "0 1 2 3\n",
        "σ Tanh\n",
        "Tanh\n",
        "X\n",
        "Ht\n",
        "X\n",
        "(d)\n",
        "Figure 3: LSTM unit gates [5]: (a) input gate; (b) forget gate; (c) memory gate; (d) output gate.\n",
        "Mathematical Problems in Engineering 5global vector, which is based on statistics of the global corpus\n",
        "instead of the context window [44]. FastText extends the skipgram of the Word2Vec model by using the subword internal\n",
        "information to address the out-of-vocabulary (OOV) terms\n",
        "[46]. In FastText, the subword components are composed to\n",
        "build the vector representation of the words, which facilitates\n",
        "representation of the word morphology and lexical similarity.\n",
        "*e BERT word embedding model is based on a multilayer\n",
        "bidirectional transformer encoder [47, 48]. Instead of using\n",
        "sequential recurrence, the transformer neural network utilises\n",
        "parallel attention layers. BERTcreates a single large transformer\n",
        "by combining the representations of the words and sentences.\n",
        "Furthermore, BERT is pretrained with an unsupervised objective over a large amount of text.\n",
        "2.7. Transformers. *e contextual representations of language\n",
        "are learned from large corpora. One of the new language\n",
        "representations, which extend word embedding models, is\n",
        "referred to as BERTmentioned in the previous section [48]. In\n",
        "BERT, two tokens are inserted to the text. *e first token (CLS)\n",
        "is employed to aggregate the whole text sequence information.\n",
        "*e second token is (SEP); this token is inserted at the end of\n",
        "each sentence to represent it. *e resultant text consists of\n",
        "tokens, where each token is assigned three types of embeddings: token, segmentation, and position embeddings. Token\n",
        "embedding is applied to indicate the meaning of a token.\n",
        "Segmentation embedding identifies the sentences, and position\n",
        "embedding determines the position of the token. *e sum of\n",
        "the three embeddings is fed to the bidirectional transformer as\n",
        "a single vector. Pretrained word embedding vectors are more\n",
        "precise and rich with semantic features. BERT has the advantage of fine-tuning (based on the objectives of certain tasks)\n",
        "and feature-based methods. Moreover, transformers compute\n",
        "the presentation of the input and output by using self-attention, where the self-attention enables the learning of the relevance between the “word-pair” [47].\n",
        "3. Single-Sentence Summary\n",
        "Recently, the RNN has been employed for abstractive text\n",
        "summarisation and has provided significant results.\n",
        "*erefore, we focus on abstractive text summarisation based\n",
        "on deep learning techniques, especially the RNN [49]. We\n",
        "discussed the approaches that have applied deep learning for\n",
        "abstractive text summarisation since 2015. RNN with an\n",
        "attention mechanism was mostly utilised for abstractive text\n",
        "summarisation. We classified the research according to\n",
        "summary type (i.e., single-sentence or multisentence summary), as shown in Figure 4. We also compared the approaches in terms of encoder-decoder architecture, word\n",
        "embedding, dataset and dataset preprocessing, and evaluations and results. *is section covers single-sentence\n",
        "summary methods, while Section 4 covers multisentence\n",
        "summary methods. Single-sentence summary methods include a neural attention model for abstractive sentence\n",
        "summarisation [18], abstractive sentence summarisation\n",
        "with attentive RNN (RAS) [39], quasi-RNN [50], a method\n",
        "for generating news headlines with RNNs [29], abstractive\n",
        "text summarisation using an attentive sequence-to-sequence\n",
        "RNN [38], neural text summarisation [51], selective\n",
        "encoding for abstractive sentence summarisation (SEASS)\n",
        "[52], faithful to the original: fact aware neural abstractive\n",
        "summarization (FTSumg) [53], and the improving transformer with sequential context [54].\n",
        "3.1. Abstractive Summarization Architecture\n",
        "3.1.1. Feedforward Architecture. Neural networks were first\n",
        "employed for abstractive text summarisation by Rush et al.\n",
        "in 2015, where a local attention-based model was utilised to\n",
        "generate summary words by conditioning it to input sentences [18]. *ree types of encoders were applied: the bag-ofwords encoder, the convolution encoder, and the attentionbased encoder. *e bag-of-words model of the embedded\n",
        "input was used to distinguish between stop words and\n",
        "content words; however, this model had a limited ability to\n",
        "represent continuous phrases. *us, a model that utilised the\n",
        "deep convolutional encoder was employed to allow the\n",
        "words to interact locally without the need for context. *e\n",
        "convolutional encoder model can alternate between temporal convolution and max-pooling layers using the standard time-delay neural network (TDNN) architecture;\n",
        "however, it is limited to a single output representation. *e\n",
        "limitation of the convolutional encoder model was overcome\n",
        "by the attention-based encoder. *e attention-based encoder\n",
        "was utilised to exploit the learned soft alignment to weight\n",
        "the input based on the context to construct a representation\n",
        "of the output. Furthermore, the beam-search decoder was\n",
        "applied to limit the number of hypotheses in the summary.\n",
        "3.1.2. RNN Encoder-Decoder Architecture\n",
        "(1) LSTM-RNN. An abstractive sentence summarisation model\n",
        "that employed a conditional recurrent neural network (RNN)\n",
        "to generate the summary from the input is referred to as a\n",
        "recurrent attentive summariser (RAS) [39]. A RAS is an extension of the work in [18]. In [18], the model employed a\n",
        "feedforward neural network, while the RAS employed an RNNLSTM. *e encoder and decoder in both models were trained\n",
        "using sentence-summary pair datasets, but the decoder of the\n",
        "RAS improved the performance since it considered the position\n",
        "information of the input words. Furthermore, previous words\n",
        "and input sentences were employed to produce the next word\n",
        "in the summary during the training phase.\n",
        "Lopyrev [29] proposed a simplified attention mechanism\n",
        "that was utilised in an encoder-decoder RNN to generate\n",
        "headlines for news articles. *e news article was fed into the\n",
        "encoder one word at a time and then passed through the\n",
        "embedding layer to generate the word representation. *e\n",
        "experiments were conducted using simple and complex\n",
        "attention mechanisms. In the simple attention mechanism,\n",
        "the last layer after processing the input in the encoding was\n",
        "divided into two parts: one part for calculating the attention\n",
        "weight vector, and one part for calculating the context\n",
        "vector, as shown in Figure 5(a). However, in the complex\n",
        "attention mechanism, the last layer was employed to calculate the attention weight vector and context vector without\n",
        "6 Mathematical Problems in Engineeringfragmentation, as shown in Figure 5(b). In both figures, the\n",
        "solid lines indicate the part of the hidden state of the last\n",
        "layer that is employed to compute the context vector, while\n",
        "the dashed lines indicate the part of the hidden state of the\n",
        "last layer that is applied to compute the attention weight\n",
        "vector. *e same difference was seen on the decoder side: in\n",
        "the simple attention mechanism, the last layer was divided\n",
        "into two parts (one part was passed to the softmax layer, and\n",
        "the other part was applied to calculate the attention weight),\n",
        "while in the complex attention mechanism, no such division\n",
        "was made. A beam search at the decoder side was performed\n",
        "during testing to extend the sequence of the probability.\n",
        "Abstractive summarization apporaches based on\n",
        "recurrent neural network + attention mechanism\n",
        "Single-sentence summary\n",
        "A neural attention model for abstractive\n",
        "sentence summarization (ABS)\n",
        "Abstractive sentence summarization with\n",
        "attentive recurrent neural networks (RAS)\n",
        "Quasi-recurrent neural network\n",
        "(QRNN) + CNN\n",
        "Generating news headlines\n",
        "with recurrent neural networks\n",
        "Abstractive text summarization using attentive\n",
        "sequence-to-sequence RNNs\n",
        "Selective encoding for abstractive sentence\n",
        "summarization (SEASS)\n",
        "Faithful to the original: fact aware neural\n",
        "abstractive summarization (FTSumg)\n",
        "Improving transformer with sequential context\n",
        "representations for abstractive text\n",
        "summarization (RCT)\n",
        "Multisentence summary\n",
        "Get to the point: summarization with\n",
        "pointer-generator networks\n",
        "Reinforcement learning (RL)\n",
        "Generative adversarial network for abstractive\n",
        "text summarization\n",
        "Exploring semantic phrases (ATSDL) + CNN\n",
        "Bidirectional attentional encoder-decoder\n",
        "model and bidirectional beam search\n",
        "Key information guide network\n",
        "Improving abstraction in\n",
        "text summarization\n",
        "Dual encoding for abstractive text\n",
        "summarization (DEATS)\n",
        "Bidirectional decoder\n",
        "(BiSum)\n",
        "Text summarization with pretrained encoders\n",
        "A text abstraction summary model based on\n",
        "BERT word embedding and reinforcement\n",
        "learning\n",
        "Transformer-based model for single-document\n",
        "neural summarization\n",
        "Text summarization method based on double\n",
        "attention pointer network (DAPT)\n",
        "Figure 4: Taxonomy of several approaches that use a recurrent neural network and attention mechanism in abstractive text summarisation\n",
        "based on the summary type.\n",
        "Mathematical Problems in Engineering 7*e encoder-decoder RNN and sequence-to-sequence\n",
        "models were utilised in [55], which mapped the inputs to the\n",
        "target sequences; the same approach was also employed in\n",
        "[38, 51]. *ree different methods for global attention were\n",
        "proposed for calculating the scoring functions, including dot\n",
        "product scoring, the bilinear form, and the scalar value\n",
        "calculated from the projection of the hidden states of the\n",
        "RNN encoder [38]. *e model applied LSTM cells instead of\n",
        "GRU cells (both LSTM and GRU are commonly employed\n",
        "for abstractive summarisation tasks since LSTM has a\n",
        "memory unit that provides control but the computation time\n",
        "of GRU is lower). *ree models were employed: the first\n",
        "model applied unidirectional LSTM in both the encoder and\n",
        "the decoder; the second model was implemented using\n",
        "bidirectional LSTM in the encoder and unidirectional LSTM\n",
        "in the decoder; and the third model utilised a bidirectional\n",
        "LSTM encoder and an LSTM decoder with global attention.\n",
        "*e first hidden state of the decoder is the concatenation of\n",
        "all backward and forward hidden states of the encoder. *e\n",
        "use of attention in an encoder-decoder neural network\n",
        "generates a context vector at each timestep. For the local\n",
        "attention mechanism, the context vector is conditioned on a\n",
        "subset of the encoder’s hidden states, while for the global\n",
        "attention mechanism, the vector is conditioned on all the\n",
        "encoder’s hidden states. After generating the first decoder\n",
        "output, the next decoder input is the word embedding of the\n",
        "output of the previous decoder step. *e affine transformation is used to convert the output of the decoder LSTM to\n",
        "a dense vector prediction due to the long training time\n",
        "needed before the number of hidden states is the same as the\n",
        "number of words in the vocabulary.\n",
        "Khandelwal [51] employed a sequence-to-sequence\n",
        "model that consists of an LSTM encoder and LSTM decoder\n",
        "for abstractive summarisation of small datasets. *e decoder\n",
        "generated the output summary after reading the hidden\n",
        "representations generated by the encoder and passing them\n",
        "to the softmax layer. *e sequence-to-sequence model does\n",
        "not memorize information, so generalization of the model is\n",
        "not possible. *us, the proposed model utilised imitation\n",
        "learning to determine whether to choose the golden token\n",
        "(i.e., reference summary token) or the previously generated\n",
        "output at each step.\n",
        "(2) GRU-RNN. A combination of the elements of the RNN and\n",
        "convolutional neural network (CNN) was employed in an\n",
        "encoder-decoder model that is referred to as a quasi-recurrent\n",
        "neural network (QRNN) [50]. In the QRNN, the GRU was\n",
        "utilised in addition to the attention mechanism. *e QRNN\n",
        "was applied to address the limitation of parallelisation, which\n",
        "aimed to obtain the dependencies of the words in previous\n",
        "steps via convolution and “fo-pooling,” which were performed\n",
        "in parallel, as shown in Figure 6. *e convolution in the QRNN\n",
        "can be either mass convolution (considering previous timesteps only) or centre convolution (considering future timesteps). *e encoder-decoder model employed two neural\n",
        "networks: the first network applied the centre convolution of\n",
        "QRNN and consisted of multiple hidden layers that were fed by\n",
        "the vector representation of the words, and the second network\n",
        "comprised neural attention and considered as input the encoder hidden layers to generate one word of a headline. *e\n",
        "decoder accepted the previously generated headline word and\n",
        "produced the next word of the headline; this process continued\n",
        "until the headline was completed.\n",
        "SEASS is an extension of the sequence-to-sequence recurrent neural network that was proposed in [52]. *e selective encoding for the abstractive sentence summarisation\n",
        "(SEASS) approach includes a selective encoding model that\n",
        "consists of an encoder for sentences, a selective gate network,\n",
        "and a decoder with an attention mechanism, as shown in\n",
        "Figure 7. *e encoder uses a bidirectional GRU, while the\n",
        "decoder uses a unidirectional GRU with an attention\n",
        "mechanism. *e encoder reads the input words and their\n",
        "representations. *e meaning of the sentences is applied by\n",
        "the selective gate to choose the word representations for\n",
        "generating the word representations of the sentence. To\n",
        "produce an excellent summary and accelerate the decoding\n",
        "process, a beam search was selected as the decoder.\n",
        "On the other hand, dual attention was applied in [53].\n",
        "*e proposed dual attention approach consists of three\n",
        "modules: two bidirectional GRU encoders and one dual\n",
        "attention decoder. *e decoder has a gate network for\n",
        "context selection, as shown in Figure 8, and employs copying\n",
        "and coverage mechanisms. *e outputs of the encoders are\n",
        "two context vectors: one context vector for sentences and\n",
        "one context vector for the relation, where the relation may be\n",
        "Context\n",
        "Attention\n",
        "weight\n",
        "Word1 Word2 Word3 <EOS>\n",
        "Headline\n",
        "(a)\n",
        "Context\n",
        "Word1 Word2 Word3 <EOS>\n",
        "Headline\n",
        "Attention\n",
        "weight\n",
        "(b)\n",
        "Figure 5: (a) Simple attention and (b) complex attention [29].\n",
        "8 Mathematical Problems in Engineering---- ---- ---- ---- ---- ---- →\n",
        "---- ---- ---- ---- ---- ---- →\n",
        "Convolutional\n",
        "Max-pool\n",
        "Convolutional\n",
        "Max-pool\n",
        "Convolutional\n",
        "fo-pool\n",
        "Convolutional\n",
        "fo-pool\n",
        "LSTM CNN QRNN\n",
        "Linear\n",
        "LSTM/\n",
        "linear\n",
        "Linear\n",
        "LSTM/\n",
        "linear\n",
        "Figure 6: Comparison of the CNN, LSTM, and QRNN models [50].\n",
        "Encoder\n",
        "Word1 Word2 Word4 Word3 Word5\n",
        "MLP\n",
        "h\n",
        "i S\n",
        "Selective gate\n",
        "network\n",
        "Attention Ct\n",
        "Somax\n",
        "Yt\n",
        "Maxout\n",
        "GRU\n",
        "S\n",
        "t\n",
        "C\n",
        "t–1 St–1\n",
        "Yt\n",
        "–1\n",
        "Decoder\n",
        "Figure 7: Selective encoding for abstractive sentence summarisation (SEASS) [52].\n",
        "Sentence\n",
        "encoder\n",
        "Word1 Word2 Word4 Word3 Word5\n",
        "Attention Somax\n",
        "Yt\n",
        "GRU\n",
        "S\n",
        "t\n",
        "C\n",
        "t–1 St–1\n",
        "C\n",
        "xt\n",
        "C\n",
        "t\n",
        "r\n",
        "Context selection\n",
        "MLP\n",
        "Rel1 Rel2 Rel4 Rel3 Rel5\n",
        "Attention\n",
        "Relation\n",
        "encoder\n",
        "C\n",
        "xt\n",
        "C\n",
        "t\n",
        "r\n",
        "C\n",
        "t\n",
        "Dual attention\n",
        "decoder\n",
        "Yt\n",
        "–1\n",
        "Figure 8: Faithful to the original [53].\n",
        "Mathematical Problems in Engineering 9a triple or tuple relation. A triple relation consists of the\n",
        "subject, predicate, and object, while the tuple relation\n",
        "consists of either (subject and predicate) or (predicate and\n",
        "subject). Sometimes the triple relation cannot be extracted;\n",
        "in this case, two tuple relations are utilised. *e decoder gate\n",
        "merges both context vectors based on their relative\n",
        "association.\n",
        "(3) Others. *e long-sequence poor semantic representation of abstractive text summarisation approaches, which\n",
        "are based on an RNN encoder-decoder framework, was\n",
        "addressed using the RC-Transformer (RCT) [54]. An RCT\n",
        "is an RNN-based abstractive text summarisation model\n",
        "that is composed of two encoders (RC encoder and\n",
        "transformer encoder) and one decoder. *e transformer\n",
        "shows an advantage in parallel computing in addition to\n",
        "retrieving the global context semantic relationships. On the\n",
        "other hand, sequential context representation was achieved\n",
        "by a second encoder of the RCT-Transformer. Word ordering is very crucial for abstractive text summarisation,\n",
        "which cannot be obtained by positioning encoding.\n",
        "*erefore, an RCT utilised two encoders to address the\n",
        "problem of a shortage of sequential information at the word\n",
        "level. A beam search was utilised at the decoder. Furthermore, Cai et al. compared the speed of the RCT model\n",
        "and that of the RNN-based model and concluded that the\n",
        "RCT is 1.4x and 1.2x faster.\n",
        "3.2. Word Embedding. In the QRNN model, GloVe word\n",
        "embedding, which was pretrained using the Wikipedia and\n",
        "Gigaword datasets, was performed to represent the text and\n",
        "summary [50]. In the first model, the proposed model by\n",
        "Jobson et al., the word embedding, randomly initialised\n",
        "and updated during training, while GloVe word embedding was employed to represent the words in the second\n",
        "and third models [38]. In a study by Cai et al., Transformer\n",
        "was utilised [54].\n",
        "3.3. Dataset and Dataset Preprocessing. In the model that\n",
        "was proposed by Rush et al., datasets were preprocessed via\n",
        "PTB tokenization by using “#” to replace all digits, conversion of all letters to lowercase letters, and the use of\n",
        "“UNK” to replace words that occurred fewer than 5 times\n",
        "[18]. *e model was trained with any input-output pairs due\n",
        "to the shortage of constraints for generating the output. *e\n",
        "training process was carried out on the Gigaword datasets,\n",
        "while the summarisation evaluation was conducted on\n",
        "DUC2003 and DUC2004 [18]. Furthermore, the proposed\n",
        "model by Chopra et al. was trained using the Gigaword\n",
        "corpus with sentence separation and tokenisation [39]. To\n",
        "form sentence-summary pairs, each headline of the article\n",
        "was paired with the first sentence of the article. *e same\n",
        "preprocessing steps of the data in [18] were performed in\n",
        "[39]. Moreover, the Chopra et al. model was evaluated using\n",
        "the DUC2004 dataset, which consists of 500 pairs.\n",
        "Gigaword datasets were also employed by the QRNN\n",
        "model [50]. Furthermore, articles that started with sentences\n",
        "that contained more than 50 words or headlines with more\n",
        "than 25 words were removed. Moreover, the words in the\n",
        "articles and their headlines were converted to lowercase\n",
        "words, and the data points were split into short, medium,\n",
        "and long sentences, based on the lengths of the sentences, to\n",
        "avoid extra padding.\n",
        "Lopyrev and Jobson et al. trained the model using\n",
        "Gigaword after processing the data. In the Lopyrev model,\n",
        "the most crucial preprocessing steps for both the text and the\n",
        "headline were tokenisation and character conversion to\n",
        "lowercase [29]. In addition, only the characters of the first\n",
        "paragraph were retained, and the length of the headline was\n",
        "fixed between 25 and 50 words. Moreover, the no-headline\n",
        "articles were disregarded, and the 〈unk〉 symbol was used to\n",
        "replace rare words.\n",
        "Khandelwal employed the Association for Computational Linguistics (ACL) Anthology Reference Corpus,\n",
        "which consists of 16,845 examples for training and 500\n",
        "examples for testing, and they were considered small\n",
        "datasets, in experiments [51]. *e abstract included the first\n",
        "three sentences, and the unigram that overlaps between the\n",
        "title and the abstract was also calculated. *ere were 25\n",
        "tokens in the summary, and there were a maximum of 250\n",
        "tokens in the input text.\n",
        "*e English Gigaword dataset, DUC2004 corpus, and\n",
        "MSR-ATC were selected to train and test the SEASS model\n",
        "[52]. Moreover, the experiments of the Cao et al. model were\n",
        "conducted using the Gigaword dataset [53]. *e same\n",
        "preprocessing steps of the data in [18] were performed in\n",
        "[52, 53]. Moreover, RCT also employed the Gigaword and\n",
        "DUC2004 datasets in experiments [54].\n",
        "3.4. Evaluation and Results. Recall-Oriented Understudy\n",
        "for Gisting Evaluation 1 (ROUGE1), ROUGE2, and\n",
        "ROUGE-L were utilised to evaluate the Rush et al. model,\n",
        "and values of 28.18, 8.49, and 23.81, respectively, were\n",
        "obtained [18]. *e experimental results of the Chopra et al.\n",
        "model showed that although DUC2004 was too complex\n",
        "for the experiments, on the Gigaword corpus, the proposed\n",
        "model outperformed state-of-the-art methods in terms of\n",
        "ROUGE1, ROUGE2, and ROUGE-L [39]. *e values of\n",
        "ROUGE1, ROUGE2, and ROUGE-L were 28.97, 8.26, and\n",
        "24.06, respectively. On the other hand, BLEU was\n",
        "employed to evaluate the Lopyrev model [29], while\n",
        "Khandelwal utilised perplexity [51]. *e SEASS model was\n",
        "evaluated using ROUGE1, ROUGE2, and ROUGE-L, and\n",
        "the results of the three measures were 36.15, 17.54, and\n",
        "33.63, respectively [52]. Moreover, ROUGE1, ROUGE2,\n",
        "and ROUGE-L were selected for evaluating the Cao et al.\n",
        "model [53]. *e values of ROUGE1, ROUGE2, and\n",
        "ROUGE-L were 37.27, 17.65, and 34.24, respectively, and\n",
        "the results showed that fake summaries were reduced by\n",
        "80%. In addition, the RCT was evaluated using ROUGE1,\n",
        "ROUGE2, and ROUGE-L with values 37.27, 18.19, and\n",
        "34.62 compared with the Gigaword dataset. *e results\n",
        "showed that the RCT model outperformed other models\n",
        "by generating a high-quality summary that contains silent\n",
        "information [54].\n",
        "10 Mathematical Problems in Engineering4. Multisentence Summary\n",
        "In this section, multisentence summary and deep learningbased abstractive text summarisation are discussed. Multisentence summary methods include the get to the point\n",
        "method (summarisation with pointer-generator networks)\n",
        "[56], a deep reinforced model for abstractive summarization\n",
        "(RL) [57], generative adversarial network for abstractive text\n",
        "summarization [58], semantic phrase exploration (ATSDL)\n",
        "[30], bidirectional attention encoder-decoder and bidirectional beam search [35], key information guide network [59],\n",
        "text summarisation abstraction improvement [60], dual\n",
        "encoding for abstractive text summarisation (DEATS) [61],\n",
        "and abstractive document summarisation via bidirectional\n",
        "decoder (BiSum) [62], the text abstraction summary model\n",
        "based on BERT word embedding and RL [63], transformerbased model for single documents neural summarisation [64],\n",
        "text summarisation with pretrained encoders [65], and text\n",
        "summarisation method based on the double attention pointer\n",
        "network [49]. *e pointer-generator [55] includes singlesentence and multisentence summaries. Additional details are\n",
        "presented in the following sections.\n",
        "4.1. Abstractive Summarization Architecture\n",
        "4.1.1. LSTM RN. A novel abstractive summarisation method\n",
        "was proposed in [56]; it generated a multisentence summary\n",
        "and addressed sentence repetition and inaccurate information. See et al. proposed a model that consists of a singlelayer bidirectional LSTM encoder, a single-layer unidirectional LSTM decoder, and the sequence-to-sequence attention model proposed by [55]. *e See et al. model\n",
        "generates a long text summary instead of headlines, which\n",
        "consists of one or two sentences. Moreover, the attention\n",
        "mechanism was employed, and the attention distribution\n",
        "facilitated the production of the next word in the summary\n",
        "by telling the decoder where to search in the source words, as\n",
        "shown in Figure 9. *is mechanism constructed the\n",
        "weighted sum of the hidden state of the encoder that facilitated the generation of the context vector, where the\n",
        "context vector is the fixed size representation of the input.\n",
        "*e probability (Pvocab) produced by the decoder was\n",
        "employed to generate the final prediction using the context\n",
        "vector and the decoder’s last step. Furthermore, the value of\n",
        "Pvocab was equal to zero for OOV words. RL was employed\n",
        "for abstractive text summarisation in [57]. *e proposed\n",
        "method in [57], which combined RL with supervised word\n",
        "prediction, was composed of a bidirectional LSTM-RNN\n",
        "encoder and a single LSTM decoder.\n",
        "Two models—generative and discriminative models—were trained simultaneously to generate abstractive\n",
        "summary text using the adversarial process [58]. *e\n",
        "maximum likelihood estimation (MLE) objective function\n",
        "employed in previous sequence-sequence models suffers\n",
        "from two problems: the difference between the training loss\n",
        "and the evaluation metric, and the unavailability of a golden\n",
        "token at testing time, which causes errors to accumulate\n",
        "during testing. To address the previous problems, the\n",
        "proposed approach exploited the adversarial framework. In\n",
        "the first step of the adversarial framework, reinforcement\n",
        "learning was employed to optimize the generator, which\n",
        "generates the summary from the original text. In the second\n",
        "step, the discriminator, which acts as a binary classifier,\n",
        "classified the summary as either a ground-truth summary or\n",
        "a machine-generated summary. *e bidirectional LSTM\n",
        "encoder and attention mechanism were employed, as shown\n",
        "in [56].\n",
        "Abstract text summarisation using the LSTM-CNN\n",
        "model based on exploring semantic phrases (ATSDL) was\n",
        "proposed in [30]. ATSDL is composed of two phases: the\n",
        "first phase extracts the phrases from the sentences, while the\n",
        "second phase learns the collocation of the extracted phrases\n",
        "using the LSTM model. To generate sentences that are\n",
        "general and natural, the input and output of the ATSDL\n",
        "model were phrases instead of words, and the phrases were\n",
        "divided into three main types, i.e., subject, relation, and\n",
        "object phrases, where the relation phrase represents the\n",
        "relation between the input phrase and the output phrase. *e\n",
        "phrase was represented using a CNN layer. *ere are two\n",
        "main reasons for choosing the CNN: first, the CNN was\n",
        "efficient for sentence-level applications, and second, training\n",
        "was efficient since long-term dependency was unnecessary.\n",
        "Furthermore, to obtain several vectors for a phrase, multiple\n",
        "kernels with different widths that represent the dimensionality of the features were utilised. Within each kernel, the\n",
        "maximum feature was selected for each row in the kernel via\n",
        "maximum pooling. *e resulting values were added to\n",
        "obtain the final value for each word in a phrase. Bidirectional\n",
        "LSTM was employed instead of a GRU on the encoder side\n",
        "since parameters are easy to tune with LSTM. Moreover, the\n",
        "decoder was divided into two modes: a generate mode and a\n",
        "copy mode. *e generate mode generated the next phrase in\n",
        "the summary based on previously generated phrases and the\n",
        "hidden layers of the input on the encoder side, while the\n",
        "copy mode copied the phrase after the current input phrase\n",
        "if the current generated phrase was not suitable for the\n",
        "previously generated phrases in the summary. Figure 10\n",
        "provides additional details.\n",
        "Bidirectional encoder and decoder LSTM-RNNs were\n",
        "employed to generate abstractive multisentence summaries\n",
        "[35]. *e proposed approach considered past and future\n",
        "context on the decoder side when making a prediction as it\n",
        "employed a bidirectional RNN. Using a bidirectional RNN\n",
        "on the decoder side addressed the problem of summary\n",
        "imbalance. An unbalanced summary could occur due to\n",
        "noise in a previous prediction, which will reduce the quality\n",
        "of all subsequent summaries. *e bidirectional decoder\n",
        "consists of two LSTMs: the forward decoder and the\n",
        "backward decoder. *e forward decoder decodes the information from left to right, while the backward decoder\n",
        "decodes the information from right to left. *e last hidden\n",
        "state of the forward decoder is fed as the initial input to the\n",
        "backward decoder, and vice versa. Moreover, the researcher\n",
        "proposed a bidirectional beam-search method that generates\n",
        "summaries from the proposed bidirectional model. Bidirectional beam search combined information from the past\n",
        "and future to produce a better summary. *erefore, the\n",
        "Mathematical Problems in Engineering 11output summary was balanced by considering both past and\n",
        "future information and by using a bidirectional attention\n",
        "mechanism. In addition, the input sequence was read in\n",
        "reverse order based on the conclusion that LSTM learns\n",
        "better when reading the source in reverse order while remembering the order of the target [66, 67]. A softmax layer\n",
        "was employed on the decoder side to obtain the probability\n",
        "of each target word in the summary over the vocabulary\n",
        "distribution by taking the output of the decoder as input for\n",
        "the softmax layer. *e decoder output depends on the internal representation of the encoder, i.e., the context vector,\n",
        "the current hidden state of the decoder, and the summary\n",
        "words previously generated by the decoder hidden states.\n",
        "*e objective of training is to maximise the probability of the\n",
        "alignment between the sentence and the summary from both\n",
        "directions. During training, the input of the forward decoder\n",
        "is the previous reference summary token. However, during\n",
        "testing, the input of the forward decoder is the token\n",
        "generated in the previous step. *e same situation is true for\n",
        "the backward decoder, where the input during training is the\n",
        "future token from the summary. Nevertheless, the bidirectional decoder has difficulty during testing since the\n",
        "complete summary must be known in advance; thus, the full\n",
        "backward decoder was generated and fed to the forward\n",
        "decoder using a unidirectional backward beam search.\n",
        "A combination of abstractive and extractive methods\n",
        "was employed in the guiding generation model proposed by\n",
        "[59]. *e extractive method generates keywords that are\n",
        "encoded by a key information guide network (KIGN) to\n",
        "represent key information. Furthermore, to predict the final\n",
        "summary of the long-term value, the proposed method\n",
        "applied a prediction guide mechanism [68]. A prediction\n",
        "Word1 Word2 Word3 Word4 Word5 Word6 Word7\n",
        "Context vector\n",
        "……..\n",
        "Attention\n",
        "distribution\n",
        "Encoder hidden\n",
        "states\n",
        "<Start>Word_Sum1\n",
        "……..\n",
        "Decoder hidden\n",
        "states\n",
        "Vocabulary\n",
        "distribution\n",
        "Figure 9: Baseline sequence-to-sequence model with attention mechanism [56].\n",
        "…….. ……..\n",
        "Phrase encoder\n",
        "Phrases\n",
        "Hidden\n",
        "states\n",
        "Encoder Decoder\n",
        "Phrase vector\n",
        "Words in a phrase\n",
        "Convolution\n",
        "Max-pooling\n",
        "Figure 10: Semantic-unit-based LSTM model [30].\n",
        "12 Mathematical Problems in Engineeringguide mechanism is a feedforward single-layer neural network that predicts the key information of the final summary\n",
        "during testing. *e encoder-decoder architecture baseline of\n",
        "the proposed model is similar to that proposed by Nallapati\n",
        "et al. [55], where both the bidirectional LSTM encoder and\n",
        "the unidirectional LSTM decoder were employed. Both\n",
        "models applied the attention mechanism and softmax layer.\n",
        "Moreover, the process of generating the summary was\n",
        "improved by proposing KIGN, which considers as input the\n",
        "keywords extracted using the TextRank algorithm. In KIGN,\n",
        "key information is represented by concatenating the last\n",
        "forward hidden state and first backward hidden state. KIGN\n",
        "employs the attention mechanism and pointer mechanism.\n",
        "In general, the attention mechanism hardly identifies the\n",
        "keywords; thus, to identify keywords, the output of KIGN\n",
        "will be fed to the attention mechanism. As a result, the\n",
        "attention mechanism will be highly affected by the keywords.\n",
        "However, to enable the pointer network to identify the\n",
        "keywords, which are the output of KIGN, the encoder\n",
        "context vector and hidden state of the decoder will be fed to\n",
        "the pointer network, and the output will be employed to\n",
        "calculate the soft switch. *e soft switch determines whether\n",
        "to copy the target from the original text or generate it from\n",
        "the vocabulary of the target, as shown in Figure 11.\n",
        "*e level of abstraction in the generated summary of the\n",
        "abstractive summarisation models was enhanced via the two\n",
        "techniques proposed in [60]: decoder decomposition and the\n",
        "use of a novel metric for optimising the overlap between the\n",
        "n-gram summary and the ground-truth summary. *e decoder was decomposed into a contextual network and\n",
        "pretrained language model, as shown in Figure 12. *e\n",
        "contextual network applies the source document to extract\n",
        "the relevant parts, and the pretrained language model is\n",
        "generated via prior knowledge. *is decomposition method\n",
        "facilitates the addition of an external pretrained language\n",
        "model that is related to several domains. Furthermore, a\n",
        "novel metric was employed to generate an abstractive\n",
        "summary by including words that are not in the source\n",
        "document. Bidirectional LSTM was utilised in the encoder,\n",
        "and the decoder applied 3-layer unidirectional weightdropped LSTM. In addition, the decoder utilised a temporal\n",
        "attention mechanism, which applied the intra-attention\n",
        "mechanism to consider previous hidden states. Furthermore, a pointer network was introduced to alternate between copying the output from the source document and\n",
        "selecting it from the vocabulary. As a result, the objective\n",
        "function combined between force learning and maximum\n",
        "likelihood.\n",
        "A bidirectional decoder with a sequence-to-sequence\n",
        "architecture, which is referred to as BiSum, was employed to\n",
        "minimise error accumulation during testing [62]. Errors\n",
        "accumulate during testing as the input of the decoder is the\n",
        "previously generated summary word, and if one of the\n",
        "generated word summaries is incorrect, then the error will\n",
        "propagate through all subsequent summary words. In the\n",
        "bidirectional decoder, there are two decoders: a forward\n",
        "decoder and a backward decoder. *e forward decoder\n",
        "generates the summary from left to right, while the backward\n",
        "decoder generates the summary from right to left. *e\n",
        "forward decoder considers a reference from the backward\n",
        "decoder. However, there is only a single-layer encoder. *e\n",
        "encoder and decoder employ an LSTM unit, but while the\n",
        "encoder utilises bidirectional LSTM, the decoders use unidirectional LSTM, as shown in Figure 13. To understand the\n",
        "summary generated by the backward decoder, the attention\n",
        "mechanism is applied in both the backward decoder and the\n",
        "encoder. Moreover, to address the problem of out-of-vocabulary words, an attention mechanism is employed in\n",
        "both decoders.\n",
        "A double attention pointer network, which is referred to\n",
        "as (DAPT), was applied to generate an abstractive text\n",
        "summarisation model [49]. *e encoder utilised bidirectional LSTM, while the decoder utilised unidirectional\n",
        "LSTM. *e encoder key features were extracted using a selfattention mechanism. At the decoder, the beam search was\n",
        "employed. Moreover, more coherent and accurate summaries were generated. *e repetition problem was\n",
        "addressed using an improved coverage mechanism with a\n",
        "truncation parameter. *e model was optimised by generating a training model that is based on RL and scheduled\n",
        "sampling.\n",
        "4.1.2. GRU-RNN. Dual encoding using a sequence-to-sequence RNN was proposed as the DEATS method [61]. *e\n",
        "dual encoder consists of two levels of encoders, i.e., primary\n",
        "and secondary encoders, in addition to one decoder, and all\n",
        "of them employ a GRU. *e primary encoder considers\n",
        "coarse encoding, while the secondary encoder considers fine\n",
        "encoding. *e primary encoder and decoder are the same as\n",
        "the standard encoder-decoder model with an attention\n",
        "mechanism, and the secondary encoder generates a new\n",
        "context vector that is based on previous output and input.\n",
        "Moreover, an additional context vector provides meaningful\n",
        "information for the output. *us, the repetition problem of\n",
        "the generated summary that was encountered in previous\n",
        "approaches is addressed. *e semantic vector is generated on\n",
        "both levels of encoding: in the primary encoder, the semantic\n",
        "vector is generated for each input, while in the secondary\n",
        "encoder, the semantic vector is recalculated after the importance of each input word is calculated. *e fixed-length\n",
        "output is partially generated at each stage in the decoder\n",
        "since it decodes in stages.\n",
        "Figure 14 elaborates the DEATS process. *e primary\n",
        "encoder produces a hidden state hpj for each input j and\n",
        "content representation cp. Next, the decoder decodes a fixedlength output, which is referred to as the decoder content\n",
        "representation cd. *e weight αj can be calculated using the\n",
        "hidden states hp\n",
        "j and the content representations cp and cd.\n",
        "In this stage, the secondary encoder generates new hidden\n",
        "states or semantic context vectors hs\n",
        "m, which are fed to the\n",
        "decoder. Moreover, DEATS uses several advanced techniques, including a pointer-generator, copy mechanism, and\n",
        "coverage mechanism.\n",
        "Wang et al. proposed a hybrid extractive-abstractive text\n",
        "summarisation model, which is based on combining the\n",
        "reinforcement learning with BERT word embedding [63]. In\n",
        "this hybrid model, a BERTfeature-based strategy was used to\n",
        "Mathematical Problems in Engineering 13generate contextualised token embedding. *is model\n",
        "consists of two submodels: abstractive agents and extractive\n",
        "agents, which are bridged using RL. Important sentences are\n",
        "extracted using the extraction model and rewritten using the\n",
        "abstraction model. A pointer-generator network was utilised\n",
        "to copy some parts of the original text, where the sentencelevel and word-level attentions are combined. In addition, a\n",
        "beam search was performed at the decoder. In abstractive\n",
        "and extractive models, the encoder consists of a bidirectional\n",
        "GRU, while the decoder consists of a unidirectional GRU.\n",
        "*e training process consists of pretraining and full training\n",
        "phases.\n",
        "Egonmwan et al. proposed to use sequence-to-sequence\n",
        "and transformer models to generate abstractive summaries\n",
        "[64]. *e proposed summarisation model consists of two\n",
        "modules: an extractive model and an abstractive model. *e\n",
        "encoder transformer has the same architecture shown in\n",
        "[48]; however, instead of receiving the document representation as input, it receives sentence-level representation.\n",
        "*e architecture of the abstractive model consists of a singleWord1Word2Word3Word4Word5Word6Word7\n",
        "……..\n",
        "Encoder hidden\n",
        "states\n",
        "Word_\n",
        "<Start> Sum1 Word_ Sum2\n",
        "……..\n",
        "+\n",
        "Word_\n",
        "<Start> Sum1 Word_ Sum2\n",
        "+\n",
        "……..\n",
        "Decoder\n",
        "Language model\n",
        "Contextual model\n",
        "h\n",
        "t\n",
        "dec\n",
        "c\n",
        "t\n",
        "temp c\n",
        "t\n",
        "int Fusion\n",
        "layer\n",
        "Figure 12: Decoder decomposed into a contextual model and a language model [60].\n",
        "Word1Word2 Word3 Word4 Word5 Word6 Word7\n",
        "Attention\n",
        "……..\n",
        "Encoder hidden\n",
        "states\n",
        "<Start> Word_Sum1\n",
        "……..\n",
        "Decoder hidden\n",
        "states\n",
        "Key\n",
        "Word1\n",
        "K\n",
        "Key\n",
        "Word2\n",
        "Key\n",
        "Word3\n",
        "Key\n",
        "Word4\n",
        "……..\n",
        "Key information\n",
        "guide network\n",
        "Pointer Somax\n",
        "S\n",
        "t\n",
        "C\n",
        "t\n",
        "Figure 11: Key information guide network [59].\n",
        "14 Mathematical Problems in Engineeringlayer unidirectional GRU at the encoder and single-layer\n",
        "unidirectional GRU at the decoder. *e input of the encoder\n",
        "is the output of the transformer. A beam search was performed during inference at the decoder, while greedydecoding was employed during training and validation.\n",
        "4.1.3. Others. BERT is employed to represent the sentences\n",
        "of the document to express its semantic [65]. Liu et al.\n",
        "proposed abstractive and extractive summarisation models\n",
        "that are based on encoder-decoder architecture. *e encoder\n",
        "used a BERT pretrained document-level encoder, while the\n",
        "decoder utilised a transformer that is randomly initialised\n",
        "and trained from scratch. In the abstractive model, the\n",
        "optimisers of the encoder and decoder are separated.\n",
        "Moreover, two stages of fine-tuning are utilised at the encoder: one stage in extractive summarisation and one stage\n",
        "in abstractive summarisation. At the decoder side, a beam\n",
        "search was performed; however, the coverage and copy\n",
        "mechanisms were not employed since these two mechanisms\n",
        "need additional tuning of the hyperparameters. *e repetition problem was addressed by producing different summaries by using trigram-blocking. *e OOV words rarely\n",
        "appear in the generated summary.\n",
        "4.2. Word Embedding. *e word embedding of the input for\n",
        "the See et al. model was learned from scratch instead of using\n",
        "a pretrained word embedding model [56]. On the other\n",
        "hand, both the input and output tokens applied the same\n",
        "embedding matrix Wemb, which was generated using the\n",
        "GloVe word embedding model in the Paulus et al. model\n",
        "[57]. Another word embedding matrix referred to as Wout\n",
        "was applied in the token generation layer. Additionally, a\n",
        "sharing weighting matrix was employed by both the shared\n",
        "embedding matrix Wemb and the Wout matrix. *e sharing\n",
        "weighting matrixes improved the process of generating\n",
        "tokens since they considered the embedding syntax and\n",
        "semantic information.\n",
        "Word_\n",
        "Sum1\n",
        "<Start> Word_\n",
        "Sum2\n",
        "Word1Word2Word3Word4Word5Word6Word7\n",
        "Primary encoder\n",
        "hidden states\n",
        "Word1Word2Word3Word4 Word5 Word6 Word7\n",
        "Secondary encoder\n",
        "hidden states\n",
        "Decoder\n",
        "hidden states\n",
        "Cp Cd\n",
        "α1 α2 α3 α4 α5 α6 α7\n",
        "h\n",
        "s1\n",
        "h\n",
        "s2\n",
        "h\n",
        "s3\n",
        "h\n",
        "s4\n",
        "h\n",
        "s5\n",
        "h\n",
        "s6\n",
        "h\n",
        "s7\n",
        "h\n",
        "p1\n",
        "h\n",
        "p2\n",
        "h\n",
        "p3\n",
        "h\n",
        "p4\n",
        "h\n",
        "p5\n",
        "h\n",
        "p6\n",
        "h\n",
        "p7\n",
        "Figure 14: Dual encoding model [61].\n",
        "Word1 Word2 Word3 Word4 Word5 Word6 Word7\n",
        "……..\n",
        "Encoder\n",
        "Word_\n",
        "<Start> Sum1 Word_ Sum2\n",
        "Forward decoder\n",
        "……..\n",
        "Context vector\n",
        "Word_\n",
        "Sumn-1\n",
        "Word_\n",
        "Sumn\n",
        "Backward\n",
        "decoder\n",
        "……..\n",
        "Context vector\n",
        "Context vector\n",
        "Figure 13: Abstractive document summarisation via bidirectional decoder (BiSum) [62].\n",
        "Mathematical Problems in Engineering 15*e discriminator input sequence of the Liu et al. model\n",
        "was encoded using a maximum pooling CNN, where the\n",
        "result was passed to the softmax layer [58]. On the other\n",
        "hand, the word embedding that was applied in the Al-Sabahi\n",
        "et al. model was learned from scratch using the CNN/Daily\n",
        "Mail datasets with 128 dimensions [35]. Egonmwan et al.\n",
        "[64] used pretrained GloVe word embedding. BERT word\n",
        "embedding was utilised in the models proposed by Wang\n",
        "et al. [63] and Liu et al. [65].\n",
        "4.3. Dataset and Dataset Preprocessing. Experiments were\n",
        "conducted with the See et al. [56], Al-Sabahi et al. [35], and\n",
        "Li et al. [59] models using CNN/Daily Mail datasets, which\n",
        "consist of 781 tokens paired with 56 tokens on average;\n",
        "287,226 pairs, 13,368 pairs, and 11,490 pairs were utilised for\n",
        "training, validation, and testing, respectively [56]. In the\n",
        "model proposed by Paulus et al., the document was preprocessed using the same method applied in [55]. *e\n",
        "proposed model was evaluated using two datasets: the CNN/\n",
        "Daily News dataset and the New York Times dataset. *e\n",
        "CNN/Daily Mail dataset was utilised by Liu et al. for training\n",
        "their model [58].\n",
        "*e ATSDL model consisted of three stages: text preprocessing, phrase extractions, and summary generation [30].\n",
        "During text preprocessing, the CoreNLP tool was employed\n",
        "to segment the words, reduce the morphology, and resolve the\n",
        "coreference. *e second stage of the ATSDL model was\n",
        "phrase extraction, which included the acquisition, refinement\n",
        "and combination of phrases. In addition, multiorder semantic\n",
        "parsing (MOSP), which was proposed to create multilayer\n",
        "binary semantics, was applied for phrase extraction. *e first\n",
        "step of MOSP was to perform Stanford NLP parsing, which is\n",
        "a specialised tool that retrieved the lexical and syntactic\n",
        "features from the preprocessed sentences. Next, dependency\n",
        "parsing was performed to create a binary tree by determining\n",
        "the root of the tree, which represents the relational phrase. If\n",
        "the child node has children, then the child is considered a new\n",
        "root with children; this process continues recursively until\n",
        "there are no children for the root. In this case, the tree\n",
        "structure is completed. Accordingly, the compound phrases\n",
        "can be explored via dependency parsing. However, one of the\n",
        "important stages of phrase extraction is refinement, during\n",
        "which redundant and incorrect phrases are refined before\n",
        "training by applying simple rules. First, the phrase triples at\n",
        "the topmost level are exploited since they carry the most\n",
        "semantic information. Second, triple phrases with subject and\n",
        "object phrases and no nouns are deleted since the noun\n",
        "contains a considerable amount of conceptual information.\n",
        "Triple phrases without a verb in a relational phrase are deleted. Moreover, phrase extraction includes phrase combination, during which phrases with the same meaning are\n",
        "combined to minimise redundancy and the time required to\n",
        "train the LSTM-RNN. To achieve the goal of the previous task\n",
        "and determine whether two phrases can be combined, a set of\n",
        "artificial rules are applied. *e experiments were conducted\n",
        "using the CNN and Daily Mail datasets, which consisted of\n",
        "92,000 text sources and 219,000 text sources, respectively.\n",
        "*e Kry´scin´ski et al. [60] model was trained using a CNN/\n",
        "Daily Mail dataset, which was preprocessed using the method\n",
        "from 56[55, 56]. *e experiments of DEATS were conducted\n",
        "using the CNN/Daily Mail dataset and DUC2004 corpus [61].\n",
        "*e experiments of the BiSum model were performed using\n",
        "the CNN/Daily Mail dataset [62]. In the Wang et al. proposed\n",
        "model, CNN/Daily Mail and DUC2002 were employed in\n",
        "experiments [63] while the Egonmwan et al. model employed\n",
        "the CNN/Daily and Newsroom datasets in experiments [64].\n",
        "Experiments were conducted with the Liu et al. [65] model\n",
        "using three benchmark datasets, including CNN/Daily Mail,\n",
        "New York Times Annotated Corpus (NYT), and XSum.\n",
        "Experiments were also conducted with the DAPT model\n",
        "using the CNN/Daily Mail and LCSTS datasets [49].\n",
        "4.4. Evaluation and Results. *e evaluation metrics\n",
        "ROUGE1, ROUGE2, and ROUGE-L, with values of 39.53,\n",
        "17.28, and 36.38, respectively, were applied to measure the\n",
        "performance of the See et al. model [56], which outperformed previous approaches by at least two points in\n",
        "terms of the ROUGE metrics. Reinforcement learning with\n",
        "the intra-attention model achieved the following results:\n",
        "ROUGE1, 41.16; ROUGE2, 15.75; and ROUGE-L, 39.08\n",
        "[57]. *e results for the maximum likelihood model were\n",
        "39.87, 15.82, and 36.9 for ROUGE1, ROUGE2, and ROUGEL, respectively. Overall, the proposed approach yielded highquality generated summaries [57].\n",
        "ROUGE1, ROUGE2, and ROUGE-L were utilised to\n",
        "evaluate the Liu et al. model, which obtained values of 39.92,\n",
        "17.65, and 36.71, respectively [58]. In addition, a manual\n",
        "qualitative evaluation was performed to evaluate the quality\n",
        "and readability of the summary. Two participants evaluated\n",
        "the summaries of 50 test examples that were selected randomly from the datasets. Each summary was given a score\n",
        "from 1 to 5, where 1 indicates a low level of readability and 5\n",
        "indicates a high level of readability.\n",
        "ROUGE1 and ROUGE2 were used to evaluate the\n",
        "ATSDL model [30]. *e value of ROUGE1 was 34.9, and the\n",
        "value of ROUGE2 was 17.8. Furthermore, ROUGE1,\n",
        "ROUGE2, and ROUGE-L were applied as evaluation metrics\n",
        "of the Al-Sabahi et al. and Li et al. models, and the values of\n",
        "42.6, 18.8, and 38.5, respectively, were obtained for the AlSabahi et al. model [35], while the values of 38.95, 17.12, and\n",
        "35.68, respectively, were obtained for the Li et al. model [58].\n",
        "*e evaluation of the Kry´scinski et al. model was con- ´\n",
        "ducted using quantitative and qualitative evaluations [60].\n",
        "*e quantitative evaluations included ROUGE1, ROUGE2,\n",
        "and ROUGE-L, and the values of 40.19, 17.38, and 37.52,\n",
        "respectively, were obtained. Additionally, a novel score related to the n-gram was employed to measure the level of\n",
        "abstraction in the summary. *e qualitative evaluation involved the manual evaluation of the proposed model. Five\n",
        "participants evaluated 100 full-text summaries in terms of\n",
        "relevance and readability by giving each document a value\n",
        "from 1 to 10. Furthermore, for comparison purposes, fulltext summaries from two previous studies [56, 58] were\n",
        "selected. *e evaluators graded the output summaries\n",
        "without knowing which model generated them.\n",
        "16 Mathematical Problems in EngineeringMoreover, ROUGE1, ROUGE2, and ROUGE-L were\n",
        "applied for evaluating DEATS, and the values of 40.85, 18.08,\n",
        "and 37.13, respectively, were obtained for the CNN/Daily\n",
        "Mail dataset [61]. *e experimental results of the BiSum\n",
        "model showed that the values of ROUGE1, ROUGE2, and\n",
        "ROUGE-L were 37.01, 15.95, and 33.66, respectively [62].\n",
        "Several variations in the Wang et al. model were\n",
        "implemented. *e best results were achieved by the BEAR\n",
        "(large + WordPiece) model, where the WordPiece tokeniser\n",
        "was utilised. *e values of ROUGE1, ROUGE2, and\n",
        "ROUGE-L were 41.95, 20.26, and 39.49, respectively [63]. In\n",
        "Egonmwan et al. model, the values of ROUGE1 and\n",
        "ROUGE2 were 41.89 and 18.90, respectively, while the value\n",
        "of ROUGE3 was 38.92. Several variations in the Liu et al. [65]\n",
        "model were evaluated using ROUGE1, ROUGE2, and\n",
        "ROUGE-L, where the best model, which is referred to as\n",
        "BERTSUMEXT (large), achieved the values of 43.85, 20.34,\n",
        "and 39.90 for ROUGE1, ROUGE2, and ROUGE-L, respectively, over the CNN/Daily Mail datasets. Moreover, the\n",
        "model was evaluated by a human via a question and answering paradigm, where 20 documents were selected for\n",
        "evaluation. *ree values were chosen for evaluating the\n",
        "answer: a score of 1 indicates the correct answer; a score of\n",
        "0.5 indicates a partially correct answer; and a score of 0\n",
        "indicates a wrong answer. ROUGE1, ROUGE2, and\n",
        "ROUGE-L for the DAPT model over the CNN/Daily Mail\n",
        "datasets were 40.72, 18.28, and 37.35, respectively.\n",
        "Finally, the pointer-generator approach was applied on\n",
        "the single-sentence and multisentence summaries. Attention encoder-decoder RNNs were employed to model the\n",
        "abstractive text summaries [55]. Both the encoder and\n",
        "decoder have the same number of hidden states. Additionally, the proposed model consists of a softmax layer for\n",
        "generating the words based on the vocabulary of the target.\n",
        "*e encoder and decoder differ in terms of their components. *e encoder consists of two bidirectional GRURNNs—a GRU-RNN for the word level and a GRU-RNN\n",
        "for the sentence level—while the decoder uses a unidirectional GRU-RNN, as shown in Figure 15. Furthermore,\n",
        "the decoder uses batching, where the vocabulary at the\n",
        "decoder for each minibatch is restricted to the words in the\n",
        "batch of the source document. Instead of considering every\n",
        "vocabulary, only certain vocabularies were added based on\n",
        "the frequency of the vocabulary in the target dictionary to\n",
        "decrease the size of the decoder softmax layer. Several\n",
        "linguistic features were considered in addition to the word\n",
        "embedding of the input words to identify the key entities of\n",
        "the document. Linguistic and statistical features included\n",
        "TF-IDF statistics and the part-of-speech and named-entity\n",
        "tags of the words. Specifically, the part-of-speech tags were\n",
        "stored in matrixes for each tag type that was similar to word\n",
        "embedding, while the TF-IDF feature was discretised in\n",
        "bins with a fixed number, where one-hot representation\n",
        "was employed to represent the value of the bins. *e onehot matrix consisted of the number of bin entries, where\n",
        "only one entry was set to one to indicate the value of the TFIDF of a certain word. *is process permitted the TF-IDF\n",
        "to be addressed in the same way as any other tag by\n",
        "concatenating all the embeddings into one long vector, as\n",
        "shown in Figure 16. *e experiments were conducted using\n",
        "the annotated Gigaword corpus with 3.8 M training examples, the DUC corpus, and the CNN/Daily Mail corpus.\n",
        "*e preprocessing methods included tokenisation and\n",
        "part-of-speech and name-entity generation. Additionally,\n",
        "the Word2Vec model with 200 dimensions was applied for\n",
        "word embedding and trained using the Gigaword corpus.\n",
        "Additionally, the hidden states had 400 dimensions in both\n",
        "the encoder and the decoder. Furthermore, datasets with\n",
        "multisentence summaries were utilised in the experiments.\n",
        "*e values of ROUGE1, ROUGE2, and ROUGE-L were\n",
        "higher than those of previous work on abstractive summarisation, with values of 35.46, 13.3, and 32.65,\n",
        "respectively.\n",
        "Finally, for both single-sentence summary and multisentence summary models, the components of the encoder\n",
        "and decoder of each approach are displayed in Table 1.\n",
        "Furthermore, dataset preprocessing and word embedding of\n",
        "several approaches are appeared in Table 2 while training,\n",
        "optimization, mechanism, and search at the decoder are\n",
        "presented in Table 3.\n",
        "5. Datasets for Text Summarization\n",
        "Various datasets were selected for abstractive text summarisation, including DUC2003, DUC2004 [69], Gigaword\n",
        "[70], and CNN/Daily Mail [71]. *e DUC datasets are\n",
        "produced for the Document Understanding Conference;\n",
        "although their quality is high, they are small datasets that are\n",
        "typically employed to evaluate summarisation models. *e\n",
        "DUC2003 and DUC2004 datasets consist of 500 articles. *e\n",
        "Gigaword dataset from the Stanford University Linguistics\n",
        "Department was the most common dataset for model\n",
        "training in 2015 and 2016. Gigaword consists of approximately 10 million documents from seven news sources,\n",
        "including the New York Times, Associated Press, and\n",
        "Washington Post. Gigaword is one of the largest and most\n",
        "diverse summarisation datasets even though it contains\n",
        "headlines instead of summaries; thus, it is considered to\n",
        "contain single-sentence summaries.\n",
        "Recent studies utilised the CNN/Daily Mail datasets for\n",
        "training and evaluation. *e CNN/Daily Mail datasets\n",
        "consist of bullet points that describe the articles, where\n",
        "multisentence summaries are created by concatenating the\n",
        "bullet points of the article [5]. CNN/Daily Mail datasets that\n",
        "are applied in abstractive summarisation were presented by\n",
        "Nallapati et al. [55]. *ese datasets were created by modifying the CNN/Daily Mail datasets that were generated by\n",
        "Hermann et al. [71]. *e Hermann et al. datasets were\n",
        "utilised for extractive summarisation. *e abstractive\n",
        "summarisation CNN/Daily Mail datasets have 286,817 pairs\n",
        "for training and 13,368 pairs for validation, while 11,487\n",
        "pairs were applied in testing. In training, the source documents have 766 words (on average 29.74 sentences), while\n",
        "the summaries have 53 words (on average 3.72 sentences)\n",
        "[55].\n",
        "In April 2018, NEWSROOM, a summarisation dataset\n",
        "that consists of 1.3 million articles collected from social\n",
        "media metadata from 1998 to 2017, was produced [72]. *e\n",
        "Mathematical Problems in Engineering 17W1\n",
        "POS\n",
        "NER\n",
        "TF\n",
        "IDF\n",
        "W1\n",
        "POS\n",
        "NER\n",
        "TF\n",
        "IDF\n",
        "W1\n",
        "POS\n",
        "NER\n",
        "TF\n",
        "IDF\n",
        "W1\n",
        "POS\n",
        "NER\n",
        "TF\n",
        "IDF\n",
        "W1\n",
        "POS\n",
        "NER\n",
        "TF\n",
        "IDF\n",
        "Word_\n",
        "Sum1\n",
        "<Start> Word_\n",
        "Sum2\n",
        "Encoder hidden states\n",
        "Decoder\n",
        "hidden states\n",
        "W1\n",
        "POS\n",
        "NER\n",
        "TF\n",
        "IDF\n",
        "Figure 16: Word embedding concatenated with discretized TF-IDF, POS, and NER one-embedding vectors [55].\n",
        "Word_\n",
        "Sum1\n",
        "<Start> Word_\n",
        "Sum2\n",
        "Word1 Word2 Word3 Word4 Word5 Word6\n",
        "Word-level\n",
        "hidden states\n",
        "Decoder\n",
        "hidden states\n",
        "Encoder\n",
        "hidden states\n",
        "Sentence-level\n",
        "hidden states\n",
        "Figure 15: Word-level and sentence-level bidirectional GRU-RNN [55].\n",
        "Table 1: Encoder and decoder components.\n",
        "Reference Year Encoder Decoder\n",
        "[18] 2015 Bag-of-words, convolutional, and attention-based\n",
        "[29] 2015 RNN with LSTM units and attention RNN with LSTM units and attention\n",
        "[39] 2016 RNN-LSTM decoder RNN Word-based\n",
        "[50] 2016 GRU + QRNN + attention GRU + RNN QRNN\n",
        "[38] 2016 Unidirectional RNN attentive encoder-decoder LSTM Unidirectional RNN attentive encoder-decoder LSTM\n",
        "Bidirectional LSTM Unidirectional LSTM\n",
        "Bidirectional LSTM Decoder that had global attention\n",
        "[51] 2016 LSTM-RNN LSTM-RNN\n",
        "[55] 2016 Two bidirectional GRU-RNN GRU-RNN unidirection\n",
        "[52] 2017 Bidirectional GRU Unidirectional GRU\n",
        "[53] 2017 Bidirectional GRU Unidirectional GRU\n",
        "[56] 2017 Single-layer bidirectional LSTM + attention Single-layer unidirectional LSTM\n",
        "[57] 2017 Bidirectional LSTM-RNN + intra-attention single LSTM decoder + intra-attention\n",
        "[58] 2018 Bidirectional LSTM Unidirectional LSTM\n",
        "[30] 2018 Bidirectional LSTM Unidirectional LSTM\n",
        "[35] 2018 Bidirectional LSTM Bidirectional LSTM\n",
        "[59] 2018 Bidirectional LSTM Unidirectional LSTM\n",
        "[60] 2018 Bidirectional LSTM 3-layer unidirectional LSTM\n",
        "[61] 2018 Bidirectional GRU Unidirectional GRU\n",
        "[62] 2018 Bidirectional LSTM Two-decoder unidirectional LSTM\n",
        "[63] 2019 Bidirectional GRU Unidirectional GRU\n",
        "[64] 2019 Unidirectional GRU Unidirectional GRU\n",
        "[49] 2020 Bidirectional LSTM Unidirectional LSTM\n",
        "18 Mathematical Problems in EngineeringNEWSROOM dataset consists of 992,985 pairs for training\n",
        "and 108,612 and 108,655 pairs for validation and testing,\n",
        "respectively [22]. *e quality of the summaries is high, and\n",
        "the style of the summarisation is diverse. Figure 17 displays\n",
        "the number of surveyed papers that applied each of the\n",
        "datasets. Nine research papers utilised Gigaword, fourteen\n",
        "papers employed the CNN/Daily Mail datasets (largest\n",
        "number of papers on the list), and one study applied the ACL\n",
        "Anthology Reference, DUC2002, DUC2004, New York\n",
        "Times Annotated Corpus (NYT), and XSum datasets.\n",
        "Table 2: Dataset preprocessing and word embedding.\n",
        "Reference Authors Dataset preprocessing Input (word embedding)\n",
        "[18] Rush et al.\n",
        "PTB tokenization by using “#” to replace all digits,\n",
        "converting all letters to lower case, and “UNK” to\n",
        "replace words that occurred fewer than 5 times\n",
        "Bag-of-words of the input sentence embedding\n",
        "[39] Chopra et al.\n",
        "PTB tokenization by using “#” to replace all digits,\n",
        "converting all letters to lower case, and “UNK” to\n",
        "replace words that occurred fewer than 5 times\n",
        "Encodes the position information of the input words\n",
        "[55] Nallapati\n",
        "et al.\n",
        "Part-of-speech and name-entity tags generating and\n",
        "tokenization\n",
        "(i) Encodes the position information of the input\n",
        "words\n",
        "(ii) *e input text was represented using the\n",
        "Word2Vec model with 200 dimensions that was\n",
        "trained using Gigaword corpus\n",
        "(iii) Continuous features such as TF-IDF were\n",
        "represented using bins and one-hot representation for\n",
        "bins\n",
        "(iv) Lookup embedding for part-of-speech tagging and\n",
        "name-entity tagging\n",
        "[52] Zhou et al.\n",
        "PTB tokenization by using “#” to replace all digits,\n",
        "converting all letters to lower case, and “UNK” to\n",
        "replace words that occurred fewer than 5 times\n",
        "Word embedding with size equal to 300\n",
        "[53] Cao et al.\n",
        "Normalization and tokenization, using the “#” to\n",
        "replace digits, convert the words to lower case, and\n",
        "“UNK” to replace the least frequent words.\n",
        "GloVe word embedding with dimension size equal to\n",
        "200\n",
        "[54] Cai et al. Byte pair encoding (BPE) was used in segmentation Transformer\n",
        "[50] Adelson et al. Converting the article and their headlines to lower case\n",
        "letters GloVe word embedding\n",
        "[29] Lopyrev\n",
        "Tokenization, converting the article and their\n",
        "headlines to lower case letters, using the symbol 〈unk〉\n",
        "to replace rare words\n",
        "*e input was represented using the distributed\n",
        "representation\n",
        "[38] Jobson et al.\n",
        "*e word embedding randomly initialised and\n",
        "updated during training while GloVe word embedding\n",
        "was used to represent the words in the second and\n",
        "third models\n",
        "[56] See et al.\n",
        "*e word embedding of the input for was learned from\n",
        "scratch instead of using a pretrained word embedding\n",
        "model\n",
        "[57] Paulus et al. *e same as in [55] GloVe\n",
        "[58] Liu et al. CNN maximum pooling was used to encode the\n",
        "discriminator input sequence\n",
        "[30] Song et al.\n",
        "*e words were segmented using CoreNLP tool,\n",
        "resolving the coreference and reducing the\n",
        "morphology\n",
        "Convolutional neural network was used to represent\n",
        "the phrases\n",
        "[35] Al-Sabahi\n",
        "et al.\n",
        "*e word embedding is learned from scratch during\n",
        "training with a dimension of 128\n",
        "[59] Li et al. *e same as in [55] Learned from scratch during training\n",
        "[60] Kry´scinski ´\n",
        "et al. *e same as in [55] Embedding layer with a dimension of 400\n",
        "[61] Yao et al. *e word embedding is learned from scratch during\n",
        "training with a dimension of 128\n",
        "[62] Wan et al. No word segmentation Embedding layer learned during training\n",
        "[65] Liu et al. BERT\n",
        "[63] Wang et al. Using WordPiece tokenizer BERT\n",
        "[64] Egonmwan\n",
        "et al.\n",
        "GloVe word embedding with dimension size equal to\n",
        "300\n",
        "Mathematical Problems in Engineering 19Table 4 lists the datasets that are used to train and validate\n",
        "the summarisation methods in the research papers listed in\n",
        "this work.\n",
        "6. Evaluation Measures\n",
        "*e package ROUGE is employed to evaluate the text\n",
        "summarisation techniques by comparing the generated\n",
        "summary with a manually generated summary [73]. *e\n",
        "package consists of several measures to evaluate the performance of text summarisation techniques, such as\n",
        "ROUGE-N (ROUGE1 and ROUGE2) and ROUGE-L, which\n",
        "were employed in several studies [38]. ROUGE-N is n-gram\n",
        "recall such that ROUGE1 and ROUGE2 are related to\n",
        "unigrams and bigrams, respectively, while ROUGE-L is\n",
        "related to the longest common substring. Since the manual\n",
        "Table 3: Training, optimization, mechanism, and search at the decoder.\n",
        "Reference Authors Training and optimization Mechanism Search at decoder (siz)\n",
        "[18] Rush et al. Stochastic gradient descent to\n",
        "minimise negative log-likelihood Beam search\n",
        "[39] Chopra et al.\n",
        "Minimizing negative log-likelihood\n",
        "using end-to-end using stochastic\n",
        "gradient descent\n",
        "Encodes the position information of\n",
        "the input words Beam search\n",
        "[55] Nallapati\n",
        "et al.\n",
        "Optimize the conditional likelihood\n",
        "using Adadelta Pointer mechanism Beam search (5)\n",
        "[52] Zhou et al.\n",
        "Stochastic gradient descent, Adam\n",
        "optimizer, optimizing the negative\n",
        "log-likelihood\n",
        "Attention mechanism Beam search (12)\n",
        "[53] Cao et al. Adam optimizer, optimizing the\n",
        "negative log-likelihood\n",
        "Copy mechanism, coverage\n",
        "mechanism, dual-attention decoder Beam search (6)\n",
        "[54] Cai et al. Cross entropy is used as the loss\n",
        "function Attention mechanism Beam search (5)\n",
        "[50] Adelson et al. Adam Attention mechanism\n",
        "[29] Lopyrev RMSProp adaptive gradient method Simple and complex attention mechanism Beam search\n",
        "[38] Jobson et al. Adadelta, minimising the negative\n",
        "log probability of prediction word\n",
        "Bilinear attention mechanism,\n",
        "pointer mechanism\n",
        "[56] See et al. Adadelta Coverage mechanism, attention\n",
        "mechanism, pointer mechanism Beam search (4)\n",
        "[57] Paulus et al. Adam, RL\n",
        "Intradecoder attention mechanism,\n",
        "pointer mechanism, copy\n",
        "mechanism, RL\n",
        "Beam search (5)\n",
        "[58] Liu et al. Adadelta stochastic gradient\n",
        "descent\n",
        "Attention mechanism, pointer\n",
        "mechanism, copy mechanism, RL\n",
        "[30] Song et al. Attention mechanism, copy mechanism\n",
        "[35] Al-Sabahi\n",
        "et al. Adagrad mechanism, copy mechanism Pointer mechanism, coverage Bidirectional beam search\n",
        "[59] Li et al. Adadelta\n",
        "Attention mechanism, pointer\n",
        "mechanism, copy mechanism,\n",
        "prediction guide mechanism\n",
        "Beam search\n",
        "[60] Kry´scinski ´\n",
        "et al.\n",
        "Asynchronous gradient descent\n",
        "optimizer\n",
        "Temporal attention and intraattention pointer mechanism, RL Beam search\n",
        "[61] Yao et al. RL, Adagrad\n",
        "Attention mechanism, pointer\n",
        "mechanism, copy mechanism,\n",
        "coverage mechanism, RL\n",
        "Beam search (4)\n",
        "[62] Wan et al. Adagrad Attention mechanism, pointer mechanism Beam-search backward (2) and forward (4)\n",
        "[65] Liu et al. Adam Self-attention mechanism Beam search (5)\n",
        "[63] Wang et al. Gradient of reinforcement learning,\n",
        "Adam, cross-entropy loss function\n",
        "Attention mechanism, pointer\n",
        "mechanism, copy mechanism, new\n",
        "coverage mechanism\n",
        "Beam search\n",
        "[64] Egonmwan\n",
        "et al. Adam Self-attention mechanism\n",
        "Greedy-decoding during training\n",
        "and validation. Beam search at\n",
        "decoding during testing\n",
        "[49] Peng et al. Adam, gradient descent, crossentropy loss\n",
        "Coverage mechanism, RL, double\n",
        "attention pointer network (DAPT) Beam search (5)\n",
        "20 Mathematical Problems in Engineeringevaluation of automatic text summarisation is a time-consuming process and requires extensive effort, ROUGE is\n",
        "employed as a standard for evaluating text summarisation.\n",
        "ROUGE-N is calculated using the following equation:\n",
        "ROUGE − N �\n",
        "􏽐 S ∈ {REFERENCE SUMMARIES} 􏽐 gramn ∈ Countmatch 􏼁 gramn\n",
        "􏽐 S ∈ {REFERENCE SUMMARIES} 􏽐 gramn ∈ Count gram  􏼁 n , (1)\n",
        "where S is the reference summary, n is the n-gram length,\n",
        "and Countmatch (gramn) is the maximum number of\n",
        "matching n-gram words between the reference summary\n",
        "and the generated summary. Count (gramn) is the total\n",
        "number of n-gram words in the reference summary [73].\n",
        "ROUGE-L is the longest common subsequence (LCS),\n",
        "which represents the maximum length of the common\n",
        "matching words between the reference summary and the\n",
        "generated summary. LCS calculation does not necessarily\n",
        "require the match words to be consecutive; however, the\n",
        "order of occurrence is important. In addition, no predefined\n",
        "number of match words is required. LCS considers only the\n",
        "main in-sequence, which is one of its disadvantages since the\n",
        "final score will not include other matches. For example,\n",
        "assume that the reference summary R and the automatic\n",
        "summary A are as follows:\n",
        "Gigaword CNN/Daily\n",
        "Mail\n",
        "ACL DUC2004 DUC2002 Newsroom NYT XSum\n",
        "Datasets\n",
        "Number of\n",
        "researches\n",
        "86420\n",
        "10\n",
        "12\n",
        "14\n",
        "16\n",
        "Datasets\n",
        "Figure 17: *e number of research papers that used the Gigaword, CNN/Daily Mail, ACL DUC2002, DUC2004, NYT, Newsroom, and\n",
        "XSum datasets [61].\n",
        "Table 4: Abstractive summarisation datasets.\n",
        "Reference Training Summarization Evaluation\n",
        "[18] Gigaword DUC2003 and DUC2004\n",
        "[39] Gigaword DUC2004\n",
        "[50] Gigaword Gigaword\n",
        "[29] Gigaword Articles from BBC, *e Wall Street Journal, Guardian, Huffington Post, and Forbes\n",
        "[38] Gigaword —\n",
        "[54] Gigaword and DUC2004 Gigaword and DUC2004\n",
        "[51] ACL anthology reference ACL anthology reference\n",
        "[52] Gigaword and DUC2004 Gigaword and DUC2004\n",
        "[53] Gigaword and DUC2004 Gigaword and DUC2004\n",
        "[56] CNN/Daily Mail CNN/Daily Mail\n",
        "[57] CNN/Daily and New York Times CNN/Daily and New York Times\n",
        "[58] CNN/Daily Mail CNN/Daily Mail\n",
        "[30] CNN/Daily Mail CNN/Daily Mail\n",
        "[35] CNN/Daily Mail CNN/Daily Mail\n",
        "[59] CNN/Daily Mail CNN/Daily Mail\n",
        "[60] CNN/Daily Mail CNN/Daily Mail\n",
        "[61] CNN/Daily Mail CNN/Daily Mail\n",
        "[55] Gigaword DUC CNN/Daily Mail Gigaword DUC CNN/Daily Mail\n",
        "[62] CNN/Daily Mail CNN/Daily Mail\n",
        "[65] CNN/Daily Mail, NYT, and XSum CNN/Daily Mail, NYT, and XSum\n",
        "[63] CNN/Daily Mail and DUC2002 CNN/Daily Mail and DUC2002\n",
        "[64] CNN/Daily Mail and Newsroom CNN/Daily Mail and Newsroom\n",
        "[49] CNN/Daily Mail CNN/Daily Mail\n",
        "Mathematical Problems in Engineering 21R: Ahmed ate the apple.\n",
        "A: the apple Ahmed ate.\n",
        "In this case, ROUGE-L will consider either “Ahmed ate”\n",
        "or “the apple” but not both, similar to LCS.\n",
        "Tables 5 and 6 present the values of ROUGE1, ROUGE2,\n",
        "and ROUGE-L for the text summarisation methods in the\n",
        "various studies reviewed in this research. In addition, Perplexity was employed in [18, 39, 51], and BLEU was utilised\n",
        "in [29]. *e models were evaluated using various datasets.\n",
        "*e other models applied ROUGE1, ROUGE2, and\n",
        "ROUGE-L for evaluation. It can be seen that the highest\n",
        "values of ROUGE1, ROUGE2, and ROUGE-L for text\n",
        "summarisation with the pretrained encoder model were\n",
        "43.85, 20.34, and 39.9, respectively [65]. Even though\n",
        "ROUGE was employed to evaluate abstractive summarisation, it is better to obtain new methods to evaluate the\n",
        "quality of summarisation. *e new evaluation metrics must\n",
        "consider novel words and semantics since the generated\n",
        "summary contains words that do not exist in the original\n",
        "text. However, ROUGE was very suitable for extractive text\n",
        "summarisation.\n",
        "Based on our taxonomy, we divided the results of\n",
        "ROUGE1, ROUGE2, and ROUGE-L into two groups. *e\n",
        "first group considered single-sentence summary approaches,\n",
        "while the second group considered multisentence summary\n",
        "approaches. Figure 18 compares several deep learning\n",
        "techniques in terms of ROUGE1, ROUGE2, and ROUGE-L\n",
        "for the Gigaword datasets, consisting of single-sentence\n",
        "summary documents. *e highest values for ROUGE1,\n",
        "ROUGE2, and ROUGE-L were achieved by the RCT model\n",
        "[54]. *e values for ROUGE1, ROUGE2, and ROUGE-L\n",
        "were 37.27, 18.19, and 34.62, respectively.\n",
        "Furthermore, Figure 19 compares the ROUGE1,\n",
        "ROUGE2, and ROUGE-L values for abstractive text summarisation methods for the CNN/Daily Mail datasets, which\n",
        "consist of multisentence summary documents. *e highest\n",
        "values of ROUGE1, ROUGE2, and ROUGE-L were achieved\n",
        "for text summarisation with a pretrained encoder model.\n",
        "*e values for ROUGE1, ROUGE2, and ROUGE-L were\n",
        "43.85, 20.34, and 39.9, respectively [65]. It can be clearly seen\n",
        "that the best model in the single-sentence summary and\n",
        "multisentence summary is the models that employed BERT\n",
        "word embedding and were based on transformers. *e\n",
        "ROUGE values for the CNN/Daily Mail datasets are larger\n",
        "than those for the Gigaword dataset, as Gigaword is utilised\n",
        "for single-sentence summaries as it contains headlines that\n",
        "are treated as summaries, while the CNN/Daily Mail datasets\n",
        "are multisentence summaries. *us, the summaries in the\n",
        "CNN/Daily Mail datasets are longer than the summaries in\n",
        "Gigaword.\n",
        "Liu et al. selected two human elevators to evaluate the\n",
        "readability of the generated summary of 50 test examples of 5\n",
        "models [58]. *e value of 5 indicates that the generated\n",
        "summary is highly readable, while the value of 1 indicates\n",
        "that the generated summary has a low level of readability. It\n",
        "can be clearly seen from the results that the Liu et al. model\n",
        "was better than the other four models in terms of ROUGE1,\n",
        "ROUGE2, and human evaluation, even though the model is\n",
        "not optimal with respect to the ROUGE-L value. In addition\n",
        "to quantitative measures, qualitative evaluation measures are\n",
        "important. Kry´scinski et al. also performed qualitative ´\n",
        "evaluation to evaluate the quality of the generated summary\n",
        "[60]. Five human evaluators evaluated the relevance and\n",
        "readability of 100 randomly selected test examples, where\n",
        "two values are utilised: 1 and 10. *e value of 1 indicates that\n",
        "the generated summary is less readable and less relevance\n",
        "while the value of 10 indicates that the generated summary is\n",
        "readable and very relevance. *e results showed that, in\n",
        "terms of readability, the model proposed by Kry´scinski et al. ´\n",
        "is slightly inferior to See et al. [56] and Liu et al. [58] models\n",
        "with mean values of 6.35, 6.76, and 6.79 for Kry´scinski et al., ´\n",
        "to See et al. and Liu et al., respectively. On the other hand,\n",
        "with respect to the relevance, the means values of the three\n",
        "models are relevance with values of 6.63, 6.73, and 6.74 for\n",
        "Kry´scinski et al., to See et al. and Liu et al., respectively. ´\n",
        "However, the Kry´scinski et al. model was the best in terms of ´\n",
        "ROUGE1, ROUGE2, and ROUGE-L.\n",
        "Liu et al. evaluated the quality of the generated summary\n",
        "in terms of succinctness, informativeness, and fluency in\n",
        "addition to measuring the level of retaining key information,\n",
        "which was achieved by human evaluation [65]. In addition,\n",
        "qualitative evaluation evaluated the output in terms of\n",
        "grammatical mistakes. *ree values were selected for evaluating 20 test examples: 1 indicates a correct answer, 0.5\n",
        "indicates a partially correct answer, and 0 indicates an incorrect answer. We can conclude that quantitative evaluations, which include ROUGE1, ROUGE2, and ROUGE-L,\n",
        "are not enough for evaluating the generated summary of\n",
        "abstractive text summarisation, especially when measuring\n",
        "readability, relevance, and fluency. *erefore, qualitative\n",
        "measures, which can be achieved by manual evaluation, are\n",
        "very important. However, qualitative measures without\n",
        "quantitative measures are not enough due to the small\n",
        "number of testing examples and evaluators.\n",
        "7. Challenges and Solutions\n",
        "Text summarisation approaches have faced various challenges; although some have been solved, others still need to\n",
        "be addressed. In this section, these challenges and their\n",
        "possible solutions are discussed.\n",
        "7.1. Unavailability of the Golden Token during Testing.\n",
        "Due to the availability of golden tokens (i.e., reference\n",
        "summary tokens) during training, previous tokens in the\n",
        "headline can be input into the decoder at the next step.\n",
        "However, during testing, the golden tokens are not available;\n",
        "thus, the input for the next step in the decoder will be limited\n",
        "to the previously generated output word. To solve this issue,\n",
        "which becomes more challenging when addressing small\n",
        "datasets, different solutions have been proposed. For example, in reference [51], the data-as-demonstrator (DaD)\n",
        "model [74] is utilised. In DaD, at each step, based on a coin\n",
        "flip, either a gold token is utilised during training or the\n",
        "previous step is employed during both testing and training.\n",
        "In this manner, at least the training step receives the same\n",
        "22 Mathematical Problems in Engineeringinput as testing. In all cases, the first input of the decoder is\n",
        "the 〈EOS〉 token, and the same calculations are applied to\n",
        "compute the loss. In [29], teacher forcing is employed to\n",
        "address this challenge: during training, instead of feeding the\n",
        "expected word from the headline, 10% of the time, the\n",
        "generated word of the previous step is fed back [75, 76].\n",
        "Moreover, the mass convolution of the QRNN is applied in\n",
        "[50] since the dependency of words generated in the future is\n",
        "difficult to determine.\n",
        "7.2. Out-of-Vocabulary (OOV) Words. One of the challenges\n",
        "that may occur during testing is that the central words of the\n",
        "test document may be rare or unseen during training; these\n",
        "words are referred to as OOV words. In 61[55, 61], a\n",
        "switching decoder/pointer was employed to address OOV\n",
        "words by using pointers to point to their original positions in\n",
        "the source document. *e switch on the decoder side is used\n",
        "to alternate between generating a word and using a pointer,\n",
        "as shown in Figure 20 [55]. When the switch is turned off,\n",
        "the decoder will use the pointer to point to the word in the\n",
        "source to copy it to the memory. When the switch is turned\n",
        "on, the decoder will generate a word from the target vocabularies. Conversely, researchers in [56] addressed OOV\n",
        "words via probability generation Pgen, where the value is\n",
        "calculated from the context vector and decoder state, as\n",
        "shown in Figure 21. To generate the output word, Pgen\n",
        "switches between copying the output words from the input\n",
        "sequence and generating them from the vocabulary. Furthermore, the pointer-generator technique is applied to\n",
        "point to input words to copy them. *e combination between the words in the input and the vocabulary is referred\n",
        "to the extended vocabulary. In addition, in [57], to generate\n",
        "the tokens on the decoder side, the decoder utilised the\n",
        "switch function at each timestep to switch between generating the token using the softmax layer and using the pointer\n",
        "mechanism to point to the input sequence position for\n",
        "Table 5: Evaluation measures of several deep learning abstractive text summarisation methods over the Gigaword dataset.\n",
        "Reference Year Authors Model ROUGE1 ROUGE2 ROUGE-L\n",
        "[18] 2015 Rush et al. ABS+ 28.18 8.49 23.81\n",
        "[39] 2016 Chopra et al. RAS-Elman (k � 10) 28.97 8.26 24.06\n",
        "[55] 2016 Nallapati et al. Words-lvt5k-1sent 28.61 9.42 25.24\n",
        "[52] 2017 Zhou et al. SEASS 36.15 17.54 33.63\n",
        "[53] 2018 Cao et al. FTSumg 37.27 17.65 34.24\n",
        "[54] 2019 Cai et al. RCT 37.27 18.19 34.62\n",
        "Table 6: Evaluation measures of several abstractive text summarisation methods over the CNN/Daily Mail datasets.\n",
        "Reference Year Authors Model ROUGE1 ROUGE2 ROUGE-L\n",
        "[55] 2016 Nallapati et al. Words-lvt2k-temp-att 35.46 13.30 32.65\n",
        "[56] 2017 See et al. Pointer-generator + coverage 39.53 17.28 36.38\n",
        "[57] 2017 Paulus et al. Reinforcement learning, with intra-attention 41.16 15.75 39.08\n",
        "[57] 2017 Paulus et al. Maximum-likelihood + RL, with intra-attention 39.87 15.82 36.90\n",
        "[58] 2018 Liu et al. Adversarial network 39.92 17.65 36.71\n",
        "[30] 2018 Song et al. ATSDL 34.9 17.8 —\n",
        "[35] 2018 Al-Sabahi et al. Bidirectional attentional encoder-decoder 42.6 18.8 38.5\n",
        "[59] 2018 Li et al. Key information guide network 38.95 17.12 35.68\n",
        "[60] 2018 Kry´scinski et al. ML + RL ROUGE + Novel, with LM ´ 40.19 17.38 37.52\n",
        "[61] 2018 Yao et al. DEATS 40.85 18.08 37.13\n",
        "[62] 2018 Wan et al. BiSum 37.01 15.95 33.66\n",
        "[63] 2019 Wang et al. BEAR (large + WordPiece) 41.95 20.26 39.49\n",
        "[64] 2019 Egonmwan et al. TRANS-ext + filter + abs 41.89 18.9 38.92\n",
        "[65] 2020 Liu et al. BERTSUMEXT (large) 43.85 20.34 39.90\n",
        "[49] 2020 Peng et al. DAPT + imp-coverage (RL + MLE (ss)) 40.72 18.28 37.35\n",
        "ROUGE1 ROUGE2 ROUGE-L\n",
        "5 0\n",
        "10\n",
        "15\n",
        "20\n",
        "25\n",
        "30\n",
        "35\n",
        "40\n",
        "ABS+\n",
        "RAS-Elman (k = 10)\n",
        "SEASS\n",
        "Words-lvt5k-1sent (Gigaword)\n",
        "FTSumg\n",
        "RCT\n",
        "Figure 18: ROUGE1, ROUGE2, and ROUGE-L scores of several\n",
        "deep learning abstractive text summarisation methods for the\n",
        "Gigaword dataset.\n",
        "Mathematical Problems in Engineering 23unseen tokens to copy them. Moreover, in [30], rare words\n",
        "were addressed by using the location of the phrase, and the\n",
        "resulting summary was more natural. Moreover, in 35\n",
        "[35, 58, 60], the OOV problem was addressed by using\n",
        "the pointer-generator technique employed in [56], which\n",
        "alternates between generating a new word and coping the\n",
        "word from the original input text.\n",
        "7.3. Summary Sentence Repetition and Inaccurate Information\n",
        "Summary. *e repetition of phrases and generation of incoherent phrases in the generated output summary are two\n",
        "challenges that must be considered. Both challenges are due\n",
        "to the summarisation of long documents and the production\n",
        "of long summaries using the attention-based encoder-decoder RNN [57]. In [35, 56], repetition was addressed by\n",
        "using the coverage model to create the coverage vector by\n",
        "aggregating the attention over all previous timesteps. In [57],\n",
        "repetition was addressed by using the key attention mechanism, where for each input token, the encoder intratemporal attention records the weights of the previous\n",
        "attention. Furthermore, the intratemporal attention uses the\n",
        "hidden states of the decoder at a certain timestep, the\n",
        "previously generated words, and the specific part of the\n",
        "encoded input sequence, as shown in Figure 22, to prevent\n",
        "repetition and attend to the same sequence of the input at a\n",
        "different step of the decoder. However, the intra-attention\n",
        "encoder mechanism cannot address all the repetition\n",
        "challenges, especially when a long sequence is generated.\n",
        "*us, the intradecoder attention mechanism was proposed\n",
        "to allow the decoder to consider more previously generated\n",
        "words. Moreover, the proposed intradecoder attention\n",
        "mechanism is applicable to any type of the RNN decoder.\n",
        "Repetition was also addressed by using an objective function\n",
        "that combines the cross-entropy loss maximum likelihood\n",
        "and gradient reinforcement learning to minimise the exposure bias. In addition, the probability of trigram p (yt) was\n",
        "proposed to address repetition in the generated summary,\n",
        "where yt is the trigram sequence. In this case, the value of\n",
        "p (yt) is 0 during a beam search in the decoder when the\n",
        "same trigram sequence was already generated in the output\n",
        "summary. Furthermore, in [60], the heuristic proposed by\n",
        "[57] was employed to reduce repetition in the summary.\n",
        "Moreover, in [61], the proposed approach addressed repetition by exploiting the encoding features generated using a\n",
        "secondary encoder to remember the previously generated\n",
        "decoder output, and the coverage mechanism is utilised.\n",
        "7.4. Fake Facts. Abstractive summarisation may generate\n",
        "summaries with fake facts, and 30% of summaries generated\n",
        "from abstractive text summarisation suffer from this\n",
        "problem [53]. With fake facts, there may be a mismatch\n",
        "between the subject and the object of the predicates. *us, to\n",
        "address this problem, dependency parsing and open information extraction (e.g., open information extraction\n",
        "(OpenIE)) are performed to extract facts.\n",
        "*erefore, the sequence-to-sequence framework with\n",
        "dual attention was proposed, where the generated summary\n",
        "was conditioned by the input text and description of the\n",
        "extracted facts. OpenIE facilitates entity extraction from a\n",
        "relation, and Stanford CoreNLP was employed to provide\n",
        "the proposed approach with OpenIE and the dependency\n",
        "parser. Moreover, the decoder utilised copying and coverage\n",
        "mechanisms.\n",
        "7.5. Other Challenges. *e main issue of the abstractive text\n",
        "summarisation dataset is the quality of the reference summary (Golden summary). In the CNN/Daily Mail dataset,\n",
        "the reference summary is the highlight of the news. Every\n",
        "highlight represents a sentence in the summary; therefore,\n",
        "5 0\n",
        "10\n",
        "15\n",
        "20\n",
        "25\n",
        "30\n",
        "35\n",
        "40\n",
        "45\n",
        "50\n",
        "ROUGE1 ROUGE2 ROUGE-L\n",
        "Pointer-generator + coverage\n",
        "Reinforcement learning, with intra-attention\n",
        "Maximum-likelihood + RL, with intra-attention\n",
        "Adversarial network\n",
        "ATSDL\n",
        "Bidirectional attentional encoder-decoder\n",
        "Key information guide network\n",
        "ML + RL ROUGE + novel, with LM\n",
        "DEATS\n",
        "Words-lvt2k-temp-att (CNN/Daily Mail)\n",
        "BiSum\n",
        "BERTSUMEXT (large)\n",
        "BEAR (large + wordPiece)\n",
        "TRANS-ext + filter + abs\n",
        "DAPT + imp-coverage (RL + MLE (ss))\n",
        "Figure 19: ROUGE1, ROUGE2, and ROUGE-L scores of abstractive text summarisation methods for the CNN/Daily Mail\n",
        "datasets.\n",
        "24 Mathematical Problems in Engineeringthe number of sentences in the summary is equal to the\n",
        "number of highlights. Sometimes, the highlights do not\n",
        "address all crucial points in the summary. *erefore, a highquality dataset needs high effort to become available.\n",
        "Moreover, in some languages, such as Arabic, the multisentence dataset for abstractive summarisation is not\n",
        "available. Single-sentence abstractive Arabic text summarisation is available but is not free.\n",
        "Another issue of abstractive summarisation is the use of\n",
        "ROUGE for evaluation. ROUGE provides reasonable results\n",
        "in the case of extractive summarisation. However, in abstractive summarisation, ROUGE is not enough as ROUGE\n",
        "depends on exact matching between words. For example, the\n",
        "words book and books are considered different using any\n",
        "one of the ROUGE metrics. *erefore, a new evaluation\n",
        "measure must be proposed to consider the context of the\n",
        "words (words that have the same meaning must be considered the same even if they have a different surface form).\n",
        "In this case, we propose to use METEOR which was used\n",
        "recently in evaluating machine translation and automatic\n",
        "summarisation models [77]. Moreover, METEOR considers\n",
        "stemming, morphological variants, and synonyms. In addition, in flexible order language, it is better to use ROUGE\n",
        "without caring about the order of the words.\n",
        "*e quality of the generated summary can be improved\n",
        "using linguistic features. For example, we proposed the use\n",
        "Encoder hidden states Decoder hidden states\n",
        "Word1 Word2 Word3 Word4 Word5 Word6 Word7\n",
        "G P P G G\n",
        "Figure 20: *e generator/pointer switching model [55].\n",
        "Word1 Word2 Word3 Word4 Word5 Word6 Word7\n",
        "Context vector\n",
        "……..\n",
        "Attention\n",
        "distribution\n",
        "Encoder hidden\n",
        "states\n",
        "<Start> Word_Sum1\n",
        "……..\n",
        "Decoder hidden\n",
        "states\n",
        "Vocabulary\n",
        "distribution\n",
        "X (1 – P\n",
        "gen) X Pgen\n",
        "P\n",
        "gen\n",
        "Figure 21: Pointer-generator model [56].\n",
        "Mathematical Problems in Engineering 25of dependency parsing at the encoder in a separate layer at\n",
        "the top of the first hidden state layer. We proposed the use of\n",
        "the word embedding, which was built by considering the\n",
        "dependency parsing or part-of-speech tagging. At the decoder side, the beam-search quality can be improved by\n",
        "considering the part-of-speech tagging of the words and its\n",
        "surrounding words.\n",
        "Based on the new trends and evaluation results, we think\n",
        "that the most promising feature among all the features is the\n",
        "use of the BERT pretrained model. *e quality of the models\n",
        "that are based on the transformer is high and will yield\n",
        "promising results.\n",
        "8. Conclusion and Discussion\n",
        "In recent years, due to the vast quantity of data available on\n",
        "the Internet, the importance of the text summarisation\n",
        "process has increased. Text summarisation can be divided\n",
        "into extractive and abstractive methods. An extractive text\n",
        "summarisation method generates a summary that consists of\n",
        "words and phrases from the original text based on linguistics\n",
        "and statistical features, while an abstractive text summarisation method rephrases the original text to generate a\n",
        "summary that consists of novel phrases. *is paper reviewed\n",
        "recent approaches that applied deep learning for abstractive\n",
        "text summarisation, datasets, and measures for evaluation of\n",
        "these approaches. Moreover, the challenges encountered\n",
        "when employing various approaches and their solutions\n",
        "were discussed and analysed. *e overview of the reviewed\n",
        "approaches yielded several conclusions. *e RNN and attention mechanism were the most commonly employed\n",
        "deep learning techniques. Some approaches applied LSTM\n",
        "to solve the gradient vanishing problem that was encountered when using an RNN, while other approaches applied a\n",
        "GRU. Additionally, the sequence-to-sequence model was\n",
        "utilised for abstractive summarisation. Several datasets were\n",
        "employed, including Gigaword, CNN/Daily Mail, and the\n",
        "New York Times. Gigaword was selected for single-sentence\n",
        "summarisation, and CNN/Daily Mail was employed for\n",
        "multisentence summarisation. Furthermore, ROUGE1,\n",
        "ROUGE2, and ROUGE-L were utilised to evaluate the\n",
        "quality of the summaries. *e experiments showed that the\n",
        "highest values of ROUGE1, ROUGE2, and ROUGE-L were\n",
        "obtained in text summarisation with a pretrained encoder\n",
        "mode, with values of 43.85, 20.34, and 39.9, respectively. *e\n",
        "best results were achieved by the models that apply\n",
        "Transformer. *e most common challenges faced during the\n",
        "summarisation process were the unavailability of a golden\n",
        "token at testing time, the presence of OOV words, summary\n",
        "sentence repetition, sentence inaccuracy, and the presence of\n",
        "fake facts. In addition, there are several issues that must be\n",
        "considered in abstractive summarisation, including the\n",
        "dataset, evaluation measures, and quality of the generated\n",
        "summary.\n",
        "Data Availability\n",
        "No data were used to support this study.\n",
        "Conflicts of Interest\n",
        "*e authors declare no conflicts of interest.\n",
        "References\n",
        "[1] M. Allahyari, S. Pouriyeh, M. Assefi et al., “Text summarization techniques: a brief survey,” International Journal of\n",
        "Advanced Computer Science and Applications, vol. 8, no. 10,\n",
        "2017.\n",
        "[2] A. B. Al-Saleh and M. E. B. Menai, “Automatic Arabic text\n",
        "summarization: a survey,” Artificial Intelligence Review,\n",
        "vol. 45, no. 2, pp. 203–234, 2016.\n",
        "[3] A. Turpin, Y. Tsegay, D. Hawking, and H. E. Williams, “Fast\n",
        "generation of result snippets in web search,” in Proceedings of\n",
        "the 30th Annual international ACM SIGIR Conference on\n",
        "Word1 Word2 Word3 Word4 Word5 Word6 Word7\n",
        "……..\n",
        "Encoder\n",
        "+\n",
        "Word_\n",
        "<Start> Sum1 Word_ Sum2\n",
        "+\n",
        "C C H\n",
        "Decoder\n",
        "……..\n",
        "Figure 22: A new word is added to the output sequence by combining the current hidden state “H” of the decoder and the two context\n",
        "vectors, marked as “C” [57].\n",
        "26 Mathematical Problems in EngineeringResearch and Development in information Retrieval-SIGIR’07,\n",
        "p. 127, Amsterdam, *e Netherlands, 2007.\n",
        "[4] E. D. Trippe, “A vision for health informatics: introducing the\n",
        "SKED framework an extensible architecture for scientific\n",
        "knowledge extraction from data,” 2017, http://arxiv.org/abs/\n",
        "1706.07992.\n",
        "[5] S. Syed, Abstractive Summarization of Social Media Posts: A\n",
        "case Study using Deep Learning, Master’s thesis, Bauhaus\n",
        "University, Weimar, Germany, 2017.\n",
        "[6] D. Suleiman and A. A. Awajan, “Deep learning based extractive text summarization: approaches, datasets and evaluation measures,” in Proceedings of the 2019 Sixth\n",
        "International Conference on Social Networks Analysis, Management and Security (SNAMS), pp. 204–210, Granada, Spain,\n",
        "2019.\n",
        "[7] Q. A. Al-Radaideh and D. Q. Bataineh, “A hybrid approach\n",
        "for Arabic text summarization using domain knowledge and\n",
        "genetic algorithms,” Cognitive Computation, vol. 10, no. 4,\n",
        "pp. 651–669, 2018.\n",
        "[8] C. Sunitha, A. Jaya, and A. Ganesh, “A study on abstractive\n",
        "summarization techniques in Indian languages,” Procedia\n",
        "Computer Science, vol. 87, pp. 25–31, 2016.\n",
        "[9] D. R. Radev, E. Hovy, and K. McKeown, “Introduction to the\n",
        "special issue on summarization,” Computational Linguistics,\n",
        "vol. 28, no. 4, pp. 399–408, 2002.\n",
        "[10] A. Khan and N. Salim, “A review on abstractive summarization methods,” Journal of @eoretical and Applied Information Technology, vol. 59, no. 1, pp. 64–72, 2014.\n",
        "[11] N. Moratanch and S. Chitrakala, “A survey on abstractive text\n",
        "summarization,” in Proceedings of the 2016 International\n",
        "Conference on Circuit, Power and Computing Technologies\n",
        "(ICCPCT), pp. 1–7, Nagercoil, India, 2016.\n",
        "[12] S. Shimpikar and S. Govilkar, “A survey of text summarization\n",
        "techniques for Indian regional languages,” International\n",
        "Journal of Computer Applications, vol. 165, no. 11, pp. 29–33,\n",
        "2017.\n",
        "[13] N. R. Kasture, N. Yargal, N. N. Singh, N. Kulkarni, and\n",
        "V. Mathur, “A survey on methods of abstractive text summarization,” International Journal for Research in Emerging\n",
        "Science andTechnology, vol. 1, no. 6, p. 5, 2014.\n",
        "[14] P. Kartheek Rachabathuni, “A survey on abstractive summarization techniques,” in Proceedings of the 2017 International Conference on Inventive Computing and Informatics\n",
        "(ICICI), pp. 762–765, Coimbatore, 2017.\n",
        "[15] S. Yeasmin, P. B. Tumpa, A. M. Nitu, E. Ali, and M. I. Afjal,\n",
        "“Study of abstractive text summarization techniques,”\n",
        "American Journal of Engineering Research, vol. 8, 2017.\n",
        "[16] A. Khan, N. Salim, H. Farman et al., “Abstractive text\n",
        "summarization based on improved semantic graph approach,” International Journal of Parallel Programming,\n",
        "vol. 46, no. 5, pp. 992–1016, 2018.\n",
        "[17] Y. Jaafar and K. Bouzoubaa, “Towards a new hybrid approach\n",
        "for abstractive summarization,” Procedia Computer Science,\n",
        "vol. 142, pp. 286–293, 2018.\n",
        "[18] A. M. Rush, S. Chopra, and J. Weston, “A neural attention\n",
        "model for abstractive sentence summarization,” in Proceedings of the 2015 Conference on Empirical Methods in Natural\n",
        "Language Processing, Lisbon, Portugal, 2015.\n",
        "[19] N. Raphal, H. Duwarah, and P. Daniel, “Survey on abstractive\n",
        "text summarization,” in Proceedings of the 2018 International\n",
        "Conference on Communication and Signal Processing (ICCSP),\n",
        "pp. 513–517, Chennai, 2018.\n",
        "[20] Y. Dong, “A survey on neural network-based summarization\n",
        "methods,” 2018, http://arxiv.org/abs/1804.04589.\n",
        "[21] A. Mahajani, V. Pandya, I. Maria, and D. Sharma, “A comprehensive survey on extractive and abstractive techniques for\n",
        "text summarization,” in Ambient Communications and\n",
        "Computer Systems, Y.-C. Hu, S. Tiwari, K. K. Mishra, and\n",
        "M. C. Trivedi, Eds., vol. 904, pp. 339–351, Springer, Singapore,\n",
        "2019.\n",
        "[22] T. Shi, Y. Keneshloo, N. Ramakrishnan, and C. K. Reddy,\n",
        "Neural Abstractive Text Summarization with Sequence-ToSequence Models: A Survey, http://arxiv.org/abs/1812.02303,\n",
        "2020.\n",
        "[23] A. Joshi, E. Fidalgo, E. Alegre, and U. de Le´on, “Deep learning\n",
        "based text summarization: approaches, databases and evaluation measures,” in Proceedings of the International Conference of Applications of Intelligent Systems, Spain, 2018.\n",
        "[24] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature,\n",
        "vol. 521, no. 7553, pp. 436–444, 2015.\n",
        "[25] D. Suleiman, A. Awajan, and W. Al Etaiwi, “*e use of hidden\n",
        "Markov model in natural Arabic language processing: a\n",
        "survey,” Procedia Computer Science, vol. 113, pp. 240–247,\n",
        "2017.\n",
        "[26] H. Wang and D. Zeng, “Fusing logical relationship information of text in neural network for text classification,”\n",
        "Mathematical Problems in Engineering, vol. 2020, pp. 1–16,\n",
        "2020.\n",
        "[27] J. Yi, Y. Zhang, X. Zhao, and J. Wan, “A novel text clustering\n",
        "approach using deep-learning vocabulary network,” Mathematical Problems in Engineering, vol. 2017, pp. 1–13, 2017.\n",
        "[28] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent\n",
        "trends in deep learning based natural language processing\n",
        "[review article],” IEEE Computational Intelligence Magazine,\n",
        "vol. 13, no. 3, pp. 55–75, 2018.\n",
        "[29] K. Lopyrev, Generating news headlines with recurrent neural\n",
        "networks, p. 9, 2015, https://arxiv.org/abs/1512.01712.\n",
        "[30] S. Song, H. Huang, and T. Ruan, “Abstractive text summarization using LSTM-CNN Based Deep Learning,” Multimedia Tools and Applications, 2018.\n",
        "[31] C. L. Giles, G. M. Kuhn, and R. J. Williams, “Dynamic recurrent neural networks: theory and applications,” IEEE\n",
        "Transactions on Neural Networks, vol. 5, no. 2, pp. 153–156,\n",
        "1994.\n",
        "[32] A. J. Robinson, “An application of recurrent nets to phone\n",
        "probability estimation,” IEEE Transactions on Neural Networks, vol. 5, no. 2, pp. 298–305, 1994.\n",
        "[33] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine\n",
        "translation by jointly learning to align and translate,” in\n",
        "Proceedings of the International Conference on Learning\n",
        "Representations, Canada, 2014, http://arxiv.org/abs/1409.\n",
        "0473.\n",
        "[34] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural\n",
        "networks,” IEEE Transactions on Signal Processing, vol. 45,\n",
        "no. 11, pp. 2673–2681, Nov. 1997.\n",
        "[35] K. Al-Sabahi, Z. Zuping, and Y. Kang, Bidirectional Attentional Encoder-Decoder Model and Bidirectional Beam Search\n",
        "for Abstractive Summarization, Cornell University, Ithaca,\n",
        "NY, USA, 2018, http://arxiv.org/abs/1809.06662.\n",
        "[36] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n",
        "[37] K. Cho, “Learning phrase representations using RNN\n",
        "encoder–decoder for statistical machine translation,” in\n",
        "Proceedings of the 2014 Conference on Empirical Methods in\n",
        "Natural Language Processing (EMNLP), pp. 1724–1734, Doha,\n",
        "Qatar, 2014.\n",
        "[38] E. Jobson and A. Guti´errez, Abstractive Text Summarization\n",
        "Using Attentive Sequence-To-Sequence RNNs, p. 8, 2016.\n",
        "Mathematical Problems in Engineering 27[39] S. Chopra, M. Auli, and A. M. Rush, “Abstractive sentence\n",
        "summarization with attentive recurrent neural networks,” in\n",
        "Proceedings of the NAACL-HLT16, pp. 93–98, San Diego, CA,\n",
        "USA, 2016.\n",
        "[40] C. Sun, L. Lv, G. Tian, Q. Wang, X. Zhang, and L. Guo,\n",
        "“Leverage label and word embedding for semantic sparse web\n",
        "service discovery,” Mathematical Problems in Engineering,\n",
        "vol. 2020, Article ID 5670215, 8 pages, 2020.\n",
        "[41] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient\n",
        "estimation of word representations in vector space,” 2013,\n",
        "http://arxiv.org/abs/1301.3781.\n",
        "[42] D. Suleiman, A. Awajan, and N. Al-Madi, “Deep learning\n",
        "based technique for Plagiarism detection in Arabic texts,” in\n",
        "Proceedings of the 2017 International Conference on New\n",
        "Trends in Computing Sciences (ICTCS), pp. 216–222, Amman,\n",
        "Jordan, 2017.\n",
        "[43] D. Suleiman and A. Awajan, “Comparative study of word\n",
        "embeddings models and their usage in Arabic language applications,” in Proceedings of the 2018 International Arab\n",
        "Conference on Information Technology (ACIT), pp. 1–7,\n",
        "Werdanye, Lebanon, 2018.\n",
        "[44] J. Pennington, R. Socher, and C. Manning, “Glove: global\n",
        "vectors for word representation,” in Proceedings of the 2014\n",
        "Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532–1543, Doha, Qatar, 2014.\n",
        "[45] D. Suleiman and A. A. Awajan, “Using part of speech tagging\n",
        "for improving Word2vec model,” in Proceedings of the 2019\n",
        "2nd International Conference on new Trends in Computing\n",
        "Sciences (ICTCS), pp. 1–7, Amman, Jordan, 2019.\n",
        "[46] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jegou, and ´\n",
        "T. Mikolov, “FastText.zip: compressing text classification\n",
        "models,” 2016, http://arxiv.org/abs/161203651.\n",
        "[47] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you\n",
        "need,” Advances in Neural Information Processing Systems,\n",
        "pp. 5998–6008, 2017.\n",
        "[48] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Pretraining of deep bidirectional transformers for language\n",
        "understanding,” in Proceedings of the 2019 Conference of the\n",
        "North American Chapter of the Association for Computational\n",
        "Linguistics: Human Language Technologies, pp. 4171–4186,\n",
        "Minneapolis, MN, USA, 2019.\n",
        "[49] Z. Li, Z. Peng, S. Tang, C. Zhang, and H. Ma, “Text summarization method based on double attention pointer network,” IEEE Access, vol. 8, pp. 11279–11288, 2020.\n",
        "[50] J. Bradbury, S. Merity, C. Xiong, and R. Socher, Quasi-recurrent neural networks, https://arxiv.org/abs/1611.01576,\n",
        "2015.\n",
        "[51] U. Khandelwal, P. Qi, and D. Jurafsky, Neural Text Summarization, Stanford University, Stanford, CA, USA, 2016.\n",
        "[52] Q. Zhou, N. Yang, F. Wei, and M. Zhou, “Selective encoding\n",
        "for abstractive sentence summarization,” in Proceedings of the\n",
        "55th Annual Meeting of the Association for Computational\n",
        "Linguistics, pp. 1095–1104, Vancouver, Canada, July 2017.\n",
        "[53] Z. Cao, F. Wei, W. Li, and S. Li, “Faithful to the original: fact\n",
        "aware neural abstractive summarization,” in Proceedings of the\n",
        "AAAI Conference on Artificial Intelligence (AAAI), New\n",
        "Orleans, LA, USA, February 2018.\n",
        "[54] T. Cai, M. Shen, H. Peng, L. Jiang, and Q. Dai, “Improving\n",
        "transformer with sequential context representations for abstractive text summarization,” in Natural Language Processing\n",
        "and Chinese Computing, J. Tang, M.-Y. Kan, D. Zhao, S. Li,\n",
        "and H. Zan, Eds., pp. 512–524, Springer International Publishing, Cham, Switzerland, 2019.\n",
        "[55] R. Nallapati, B. Zhou, C. N. dos Santos, C. Gulcehre, and\n",
        "B. Xiang, “Abstractive text summarization using sequence-tosequence RNNs and beyond,” in Proceedings of the CoNLL-16,\n",
        "Berlin, Germany, August 2016.\n",
        "[56] A. See, P. J. Liu, and C. D. Manning, “Get to the point:\n",
        "summarization with pointer-generator networks,” in Proceedings of the 55th ACL, pp. 1073–1083, Vancouver, Canada,\n",
        "2017.\n",
        "[57] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model\n",
        "for abstractive summarization,” 2017, http://arxiv.org/abs/\n",
        "1705.04304.\n",
        "[58] K. S. Bose, R. H. Sarma, M. Yang, Q. Qu, J. Zhu, and H. Li,\n",
        "“Delineation of the intimate details of the backbone conformation of pyridine nucleotide coenzymes in aqueous solution,” Biochemical and Biophysical Research\n",
        "Communications, vol. 66, no. 4, 1975.\n",
        "[59] C. Li, W. Xu, S. Li, and S. Gao, “Guiding generation for\n",
        "abstractive text summarization based on key information\n",
        "guide network,” in Proceedings of the 2018 Conference of the\n",
        "North American Chapter of the Association for Computational\n",
        "Linguistics: Human Language Technologies, pp. 55–60, New\n",
        "Orleans, LA, USA, 2018.\n",
        "[60] W. Kry´scinski, R. Paulus, C. Xiong, and R. Socher, “Im- ´\n",
        "proving abstraction in text summarization,” in Proceedings of\n",
        "the Conference on Empirical Methods in Natural Language\n",
        "Processing (EMNLP), Brussels, Belgium, November 2018.\n",
        "[61] K. Yao, L. Zhang, D. Du, T. Luo, L. Tao, and Y. Wu, “Dual\n",
        "encoding for abstractive text summarization,” IEEE Transactions on Cybernetics, pp. 1–12, 2018.\n",
        "[62] X. Wan, C. Li, R. Wang, D. Xiao, and C. Shi, “Abstractive\n",
        "document summarization via bidirectional decoder,” in Advanced Data Mining and Applications, G. Gan, B. Li, X. Li, and\n",
        "S. Wang, Eds., pp. 364–377, Springer International Publishing, Cham, Switzerland, 2018.\n",
        "[63] Q. Wang, P. Liu, Z. Zhu, H. Yin, Q. Zhang, and L. Zhang, “A\n",
        "text abstraction summary model based on BERT word embedding and reinforcement learning,” Applied Sciences, vol. 9,\n",
        "no. 21, p. 4701, 2019.\n",
        "[64] E. Egonmwan and Y. Chali, “Transformer-based model for\n",
        "single documents neural summarization,” in Proceedings of\n",
        "the 3rd Workshop on Neural Generation and Translation,\n",
        "pp. 70–79, Hong Kong, 2019.\n",
        "[65] Y. Liu and M. Lapata, “Text summarization with pretrained\n",
        "encoders,” 2019, http://arxiv.org/abs/1908.08345.\n",
        "[66] P. Doetsch, A. Zeyer, and H. Ney, “Bidirectional decoder\n",
        "networks for attention-based end-to-end offline handwriting\n",
        "recognition,” in Proceedings of the 2016 15th International\n",
        "Conference on Frontiers in Handwriting Recognition (ICFHR),\n",
        "pp. 361–366, Shenzhen, China, 2016.\n",
        "[67] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to Sequence\n",
        "Learning with Neural Networks,” in Proceedings of the Advances in Neural Information Processing Systems (NIPS),\n",
        "Montreal, Quebec, Canada, December 2014.\n",
        "[68] D. He, H. Lu, Y. Xia, T. Qin, L. Wang, and T.-Y. Liu,\n",
        "“Decoding with value networks for neural machine translation,” in Proceedings of the Advances in Neural Information\n",
        "Processing Systems, Long Beach, CA, USA, December 2017.\n",
        "[69] D. Harman and P. Over, “*e effects of human variation in\n",
        "DUC summarization evaluation, text summarization\n",
        "branches out,” Proceedings of the ACL-04 Workshop, vol. 8,\n",
        "2004.\n",
        "[70] C. Napoles, M. Gormley, and B. V. Durme, “Annotated\n",
        "Gigaword,” in Proceedings of the AKBC-WEKEX, Montr´eal,\n",
        "Canada, 2012.\n",
        "28 Mathematical Problems in Engineering[71] K. M. Hermann, T. Kocisky, E. Grefenstette et al., “Machines\n",
        "to read and comprehend,” in Proceedings of the Advances in\n",
        "Neural Information Processing Systems (NIPS), Montreal,\n",
        "Quebec, Canada, December 2015.\n",
        "[72] M. Grusky, M. Naaman, and Y. Artzi, “Newsroom: a dataset of\n",
        "1.3 million summaries with diverse extractive strategies,” in\n",
        "Proceedings of the 2018 Conference of the North American\n",
        "Chapter of the Association for Computational Linguistics:\n",
        "Human Language Technologies, Association for Computational Linguistics, New Orleans, LA, USA, pp. 708–719, June\n",
        "2018.\n",
        "[73] C.-Y. Lin, “ROUGE: a package for automatic evaluation of\n",
        "summaries,” in Proceedings of the 2004 ACL Workshop,\n",
        "Barcelona, Spain, July 2004.\n",
        "[74] A. Venkatraman, M. Hebert, and J. A. Bagnell, “Improving\n",
        "multi-step prediction of learned time series models,” in\n",
        "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 3024–3030, Austin, TX, USA, 2015.\n",
        "[75] I. Goodfellow, A. Courville, and Y. Bengio, Deep Learning,\n",
        "MIT Press, Cambridge, MA, USA, 2015.\n",
        "[76] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled\n",
        "sampling for sequence prediction with recurrent neural\n",
        "networks,” in Proceedings of the Annual Conference on Neural\n",
        "Information Processing Systems, pp. 1171–1179, Montreal,\n",
        "Quebec, Canada, December 2015.\n",
        "[77] A. Lavie and M. J. Denkowski, “*e Meteor metric for automatic evaluation of machine translation,” Machine Translation, vol. 23, no. 2-3, pp. 105–115, 2009.\n",
        "Mathematical Problems in Engineering 29\n",
        "\n",
        "Abstract—The security situation of the Internet of Things (IoT) is\n",
        "serious. IoT encounters security problems more than traditional\n",
        "computer networks does. The attributes of dispersity and mass of\n",
        "IoT require that approaches to IoT security should be dynamic.\n",
        "Inspired by immunology, a novel approach to IoT security is\n",
        "proposed in this paper. Traditional network security models are\n",
        "used for reference and special requests of IoT security are taken\n",
        "into account. A dynamic defense frame for IoT security is formed\n",
        "in the proposed approach. The links in the frame are correlated\n",
        "with relative data of IoT security. Performance in biological\n",
        "immunology is applied into some links to make the proposed\n",
        "approach be adaptive to IoT environment. The immunity-based\n",
        "antigen, self and detector in the real IoT environment are\n",
        "simulated. They are adopted to imitate the mechanisms which are\n",
        "used to recognize pathogens in biological immune systems.\n",
        "Simulation experiment results show that the proposed approach\n",
        "may provide a novel effective method to ensure IoT security.\n",
        "Keywords- IoT Security; Immunology; Detection; Detector;\n",
        "Security Defense\n",
        "I. INTRODUCTION\n",
        "The application of the Internet of Things (IoT) [1]\n",
        "increased sharply in recent years [2-5]. However, IoT has its\n",
        "special security requests other than the traditional problems\n",
        "harming computer, Internet and mobile communication\n",
        "network. The terminals in the sense layer have wide varieties\n",
        "and are massive and widely distributed. It makes that the\n",
        "security problems are amplified and affect IoT more severely\n",
        "[6]. Meanwhile, it causes that IoT is faced with more serious\n",
        "potential security threats than in the other networks. Traditional\n",
        "security architectures can not fully satisfy the security requests.\n",
        "A specifical approach to IoT security is in urgent need to cope\n",
        "with the complicated and changeful security situation of IoT.\n",
        "In the interest of resolving the problems of information\n",
        "security, researchers introduced Artificial Immune System\n",
        "(AIS) [7-10] into the research field of information security.\n",
        "AIS imitates the excellent mechanisms of Biological Immune\n",
        "System. It has attracted much attention widely and been a\n",
        "research hotspot in the fields of bionics and computation\n",
        "intelligence. Since 2002, International Conference on Artificial\n",
        "Immune Systems (ICARIS) has been held 12 times [11].\n",
        "Researchers applied AIS into research fields of information\n",
        "security and network security and got many outstanding\n",
        "achievements [12-16].\n",
        "In the following of this paper, the research background of\n",
        "IoT security is surveyed in the second section. The state of\n",
        "security architecture research for traditional networks and IoT\n",
        "is discussed. In the third section, this paper will propose a\n",
        "novel approach to cope with the complicated and changeful\n",
        "IoT environment based on immunology. The proposed\n",
        "approach is expected to provide scientific reasons for the active\n",
        "defense strategy and effective defense measures.\n",
        "II. RESEARCH BACKGROUND\n",
        "The current technologies for IoT security primarily come\n",
        "from the concepts of tradition network security. Most of them\n",
        "focus on identity authentication, access control, privacy\n",
        "protection, encryption, security protocol, and etc [17-21]. The\n",
        "security ways are in the stage of passive defense. To construct\n",
        "effective defense measures for IoT security, researchers\n",
        "proposed some methods and models of active defense.\n",
        "Mirowski et al [22] developed a system of intrusion detection\n",
        "according to the past access frequency of thing labels. Yang et\n",
        "al [23] presented a distributed intrusion detection method for\n",
        "nodes of wireless sense networks. Wu et al [24] proposed a\n",
        "security transport model for IoT confidentiality. The abovementioned system, method and model solved one aspect of the\n",
        "IoT security problems. However, they don’t form an effective\n",
        "IoT security architecture and can not provide an appropriate\n",
        "approach to resolve problems of IoT security.\n",
        "Standard and commercial organizations put forward a series\n",
        "of architectures and models for traditional network and\n",
        "information security. These security architectures include\n",
        "ISO/OSI security system, TCP/IP security system, time-based\n",
        "PDR (Protection, Detection, Response) model, P2DR (Policy,\n",
        "Protection, Response, Recovery) model, P2DR2 (Policy,\n",
        "Protection, Detection, Response, Recovery) model, etc. They\n",
        "realize the guarantee strategy of network security through the\n",
        "aspects of management and technology. However, the\n",
        "traditional security architectures neglect entire construction of\n",
        "networks and information security system. They emphasize\n",
        "simple protection measures and do not make full use of the\n",
        "other links of network security system [25]. They can not be\n",
        "copied blindly to construct the IoT security system because of\n",
        "the special attributes of IoT. It is necessary to study a novel\n",
        "dynamical theoretical approach for IoT security to specially\n",
        "adapt the IoT network environment.\n",
        "Dynamical approach to IoT security can use the traditional\n",
        "network security models for reference. But it has to take the\n",
        "special requests of IoT security into account. It must adopt\n",
        "effective ways to deal with the attributes of dispersity and mass\n",
        "in IoT. In consideration of the excellent AIS attributes such as\n",
        "self-learning, self-adaptation, robustness, distribution, etc [10],\n",
        "it is valuable to use bionics principles of AIS to construct a\n",
        "novel dynamical approach to IoT security.\n",
        "III. APPROACH TO IOT SECURITY\n",
        "The proposed approach adopts immune principles and\n",
        "mechanisms to simulate real defense environment of IoT\n",
        "security as an immune system. It transforms its strategies of\n",
        "security defense along with the change of the security\n",
        "environment of IoT. It makes the proposed approach be\n",
        "adaptive to real IoT. The proposed approach is described in the\n",
        "following.\n",
        "A. Frame of IoT Security\n",
        "To resolve the problems of static defense strategies, the\n",
        "proposed approach adopts dynamic and circular defense\n",
        "processes against security threats. Its frame is shown in Fig. 1.\n",
        "It consists of five links. The first link Security Threat Detection\n",
        "collects and analyzes original IoT network packets. The other\n",
        "links perform based on the analysis results provided by the\n",
        "previous link. All links serve IoT security.\n",
        "Figure 1. Frame of proposed approach.\n",
        "B. Simmulation of Immune Principles and Mechanisms\n",
        "The proposed approach captures original data from IoT\n",
        "traffic and analyzes the data to judge whether it contains\n",
        "security threats. It simulates the principles and mechanisms of\n",
        "AIS to detect security threats in the original data. The\n",
        "principles and mechanisms have the attributes of self-learning,\n",
        "self-adaptation, etc. They make that the proposed approach can\n",
        "adapt the dynamic IoT security environments and discover\n",
        "mutated security threats. The proposed approach simulates the\n",
        "following principles and mechanisms.\n",
        "1) Antigen simulation\n",
        "In immune systems, antigen is the original data to be\n",
        "recognized. In the proposed approach, the original data of IoT\n",
        "traffic is simulated into antigen.\n",
        "The original data comes from real-time IoT traffic. The\n",
        "proposed approach captures IoT packets and gets their head data which is the signature. Let the data set of antigen be A\n",
        "which is shown in (1). The elements in A constitute antigen set\n",
        "to be recognized by the proposed approach.\n",
        "A           a a l a toString Original Data Set ,    (1)\n",
        "Where, l is the length of an antigen, l N , N is the nature\n",
        "number set, toString( ) is a function to convert the original data\n",
        "set of IoT into binary strings.\n",
        "\n",
        "A contains normal antigens and abnormal antigens which\n",
        "threaten IoT system. Normal ones belong to self set which is\n",
        "defined as S. Abnormal ones belongs to non-self set which is\n",
        "defined as N. The antigens in N are abnormal and hide in all\n",
        "antigens. The proposed approach uses immune principles and\n",
        "mechanisms to recognize the abnormal ones (non-self antigens).\n",
        "Eventually, it maintains the IoT to be safe.\n",
        "2) Detector simulation\n",
        "To imitate the recognition mechanism in immune systems,\n",
        "detection elements of security threats are simulated into\n",
        "detectors. Let the data set of detector be D which is shown in\n",
        "(2).\n",
        "D ant ag cnt tp t fam     , , , , ,   (2)\n",
        "where, ant is the antibody string, ag is the living time, cnt is the\n",
        "amount of recognized antigens, tp is the class, t is the thickness.\n",
        "3) Match mechanism\n",
        "To simulate the match mechanism, a match method is\n",
        "needed to judge whether a antigen or detector matches another.\n",
        "Presently, feasible matching methods include Hamming,\n",
        "Euclidean, r-Contiguous, etc. The proposed approach adopts an\n",
        "improved match method of r-Contiguous. It is shown in (3)\n",
        "   , , ,   \n",
        ",\n",
        "r\n",
        "r improved\n",
        "true f d a\n",
        "f d a\n",
        "false Otherwise\n",
        "\n",
        "  \n",
        " \n",
        "signature data of known IoT security threats. They contain the\n",
        "precise signature information to recognize security threats.\n",
        "5) Self tolerance mechanism\n",
        "In immune systems, the mechanism of self tolerance is used\n",
        "to avoid that immune cells recognize self antigens. In the\n",
        "proposed approach, new detectors may recognize self elements.\n",
        "They can not be used to detect security threats directly. They\n",
        "must accept the training process of the self-tolerance.\n",
        "Let the function of the self-tolerance be which is\n",
        "shown in (4).\n",
        "ftolerance ()\n",
        "  \n",
        "  \n",
        "( 1 ) 1 , .\n",
        "1 ,\n",
        "tolerance I I\n",
        "r improved\n",
        "f D t r r D t r ag\n",
        "s S t f r s false\n",
        "   \n",
        "  ,\n",
        "      \n",
        "(4)\n",
        "Where, t is the current moment,   is the period threshold of\n",
        "self-tolerance, DI is immature detector set, S is self set.\n",
        "ftolerance ( ) returns the detector set which passed selftolerance. The detectors in it do not match self antigens in a\n",
        "period of time. They evolve to mature ones after they succeed\n",
        "to pass the self-tolerance. In the process of the self-tolerance,\n",
        "new immature detectors are trained by all self elements. If an\n",
        "immature one is matched by a self cell in a special period time,\n",
        "it fails to accept self-tolerance.\n",
        "6) Evolution mechanism of self\n",
        "Changeful IoT security environment cause that self set\n",
        "changes along with the dynamic security threats. The initial\n",
        "self elements are gathered in a safe IoT environment. When the\n",
        "proposed approach recognize new abnormal antigens, potential\n",
        "normal antigens are generated. Therefore, self set should be\n",
        "expanded. The expansion process is shown in (5). It make that\n",
        "self set evolutes.\n",
        "S S toSelf A     init Normal  (5)\n",
        "Where, is the data set of initial self elements, is\n",
        "the function to convert normal antigens into self elements,\n",
        "is the normal antigen set recognized by detectors.\n",
        "Sinit toSelf ( )\n",
        "ANormal\n",
        "C. Dynamic Links of IoT Security\n",
        "1) Detection of security threats\n",
        "Detectors and the simulative immune mechanisms are used\n",
        "to recognize abnormal antigens from real-time antigens in the\n",
        "proposed approach. The detection process is shown in (6).\n",
        "AHarm               a a A d D f d a true , , I r improved   \n",
        "Where, AHarm is a data set recognized by memory detectors.\t(6)\n",
        "A memory detector is activated when it detect a harmful\n",
        "antigen. It begins to copy itself and accumulate its thickness.\n",
        "2) Danger computation\n",
        "In this link, the quantitative danger caused by security\n",
        "threats is computed. It needs the elements of harmfulness of\n",
        "security threats, asset cost and memory detectors' thickness\n",
        "which is generated in the previous link. The danger\n",
        "computation process is shown in (7).\n",
        "Fig. 2 illuminates the association relationship of IoT\n",
        "security threats, IoT asset, danger of threats, IoT security\n",
        "response grade and response policy. The broken lines indicate\n",
        "how the proposed approach provides scientific reasons for\n",
        "policies and measures of IoT security defense. The proposed\n",
        "approach produces security response grades and chooses\n",
        "security response polices. Furthermore, it serves security\n",
        "defense.\n",
        "IV. SIMULATION EXPERIMENTS\n",
        "In the experiments, simulation software and devices were\n",
        "used to construct the simulation experiment environment which\n",
        "is shown in Fig. 3. The simulated network consisted of sense\n",
        "network, IoT gateway and a computer server which runs the\n",
        "proposed approach. In the sense network, cloning attacks,\n",
        "mutated cloning attacks, replay attacks and mutated replay\n",
        "attacks were simulated. The simulated server used AIS\n",
        "principles and mechanisms to detect security threats, compute\n",
        "danger, save security defense strategy library, respond to\n",
        "security threats and defend the security. The sense network kept safe for three hours to collect initial\n",
        "self data set. The simulated server detected simulated attacks\n",
        "and adopted defense measures. Simulation experiment results\n",
        "are shown in Table I. Table I shows that the proposed approach can detect\n",
        "security threats and change detectors to adapt the dynamic IoT\n",
        "environment. Meanwhile, it indicates that appropriate defense\n",
        "strategies and measures can be adopted to deal with the relative\n",
        "security threats. IoT has some special security requests. Traditional security\n",
        "models for computer networks can not be applied in IoT security directly. The proposed approach uses traditional\n",
        "security models for reference and adopts circular links to\n",
        "construct a dynamic defense system for IoT security. The first\n",
        "link performs based on recognition of security threats. The\n",
        "other links use the real data provided by its previous link. The\n",
        "whole process for IoT security is dynamic. Furthermore,\n",
        "principles in immunology are simulated and applied into the\n",
        "key links. It makes that the proposed model can be adaptive to\n",
        "the IoT environment. The proposed approach changes the\n",
        "defense ways for IoT security and provides a novel effective\n",
        "method to ensure IoT security.\n",
        " \n",
        "\n",
        " \n",
        "(3)\n",
        "Where, d D  , a A  ,   is a threshold, it meets\n",
        "1 /               l r , r is the amount of binary chars of contiguous\n",
        "match, fr () is a single group match function of r-Contiguous.\n",
        "If d matches a, fr () returns 1. Otherwise, it returns 0.\n",
        "4) Evolution mechanism of detector\n",
        "To simulate the evolution process of immune cells,\n",
        "detectors are classified into immature detector DI, mature\n",
        "detector DM and memory detector DR. The domain tp indicates\n",
        "which class the detector belongs to. It is one of the class data\n",
        "set T which meets T i m r       , , . Detectors adopt artificial\n",
        "immune mechanisms to evolve dynamically to adapt IoT\n",
        "security environment. Immature detectors are generated with\n",
        "the mutation and recombination of gene sections or randomly.\n",
        "They pass the detection of self-tolerance to evolve into mature\n",
        "detectors. They can improve the diversity of detectors. Mature\n",
        "detectors come from immature detectors. They get the training\n",
        "of antigens. If they do not match any harmful antigens in a\n",
        "period of time, they get the opportunity to evolve into memory\n",
        "detectors. This embodies the process of self-learning and selfadaptation. Memory detectors come from mature detectors and\n",
        "signature data of known IoT security threats. They contain the\n",
        "precise signature information to recognize security threats.\n",
        "5) Self tolerance mechanism\n",
        "In immune systems, the mechanism of self tolerance is used\n",
        "to avoid that immune cells recognize self antigens. In the\n",
        "proposed approach, new detectors may recognize self elements.\n",
        "They can not be used to detect security threats directly. They\n",
        "must accept the training process of the self-tolerance.\n",
        "Let the function of the self-tolerance be which is\n",
        "shown in (4).\n",
        "ftolerance ()\n",
        " \u0003 \u0004  \u0003\n",
        " \u0003  \u0003 \u0005\n",
        "( 1 ) 1 , .\n",
        "1 ,\n",
        "tolerance I I\n",
        "r improved\n",
        "f D t r r D t r ag\n",
        "s S t f r s false\n",
        "\t \u0006 \u0007 \t \u000b \u0015,\n",
        "\u0016 \u0007 \t \u0017 \u0006\n",
        "(4)\n",
        "Where, t is the current moment, \u0015 is the period threshold of\n",
        "self-tolerance, DI is immature detector set, S is self set.\n",
        "ftolerance ( ) returns the detector set which passed selftolerance. The detectors in it do not match self antigens in a\n",
        "period of time. They evolve to mature ones after they succeed\n",
        "to pass the self-tolerance. In the process of the self-tolerance,\n",
        "new immature detectors are trained by all self elements. If an\n",
        "immature one is matched by a self cell in a special period time,\n",
        "it fails to accept self-tolerance.\n",
        "6) Evolution mechanism of self\n",
        "Changeful IoT security environment cause that self set\n",
        "changes along with the dynamic security threats. The initial\n",
        "self elements are gathered in a safe IoT environment. When the\n",
        "proposed approach recognize new abnormal antigens, potential\n",
        "normal antigens are generated. Therefore, self set should be\n",
        "expanded. The expansion process is shown in (5). It make that\n",
        "self set evolutes.\n",
        "S S toSelf A \u0006 \u0018 init  Normal \u0003 (5)\n",
        "Where, is the data set of initial self elements, is\n",
        "the function to convert normal antigens into self elements,\n",
        "is the normal antigen set recognized by detectors.\n",
        "Sinit toSelf ( )\n",
        "ANormal\n",
        "C. Dynamic Links of IoT Security\n",
        "1) Detection of security threats\n",
        "Detectors and the simulative immune mechanisms are used\n",
        "to recognize abnormal antigens from real-time antigens in the\n",
        "proposed approach. The detection process is shown in (6).\n",
        "AHarm \u0006 \u0016 \u0007 \u0019 \u0007 \u0017 \u0006 \u0004a a A d D f d a true , , I r improved  \u0003 \u0005 (6)\n",
        "Where, AHarm is a data set recognized by memory detectors.\n",
        "A memory detector is activated when it detect a harmful\n",
        "antigen. It begins to copy itself and accumulate its thickness.\n",
        "2) Danger computation\n",
        "In this link, the quantitative danger caused by security\n",
        "threats is computed. It needs the elements of harmfulness of\n",
        "security threats, asset cost and memory detectors' thickness\n",
        "which is generated in the previous link. The danger\n",
        "computation process is shown in (7). fdanger r f r t r h c \u0003 \u0006  . , . , \u0003 (7)\n",
        "Where, r is a memory detector, r.h is the harmfulness of a\n",
        "security threat, c is the cost of IoT asset.\n",
        "3) Security Response and Defense\n",
        "The process of response and defense for IoT security is\n",
        "shown in Fig. 2. Fig. 2 illuminates the association relationship of IoT\n",
        "security threats, IoT asset, danger of threats, IoT security\n",
        "response grade and response policy. The broken lines indicate\n",
        "how the proposed approach provides scientific reasons for\n",
        "policies and measures of IoT security defense. The proposed\n",
        "approach produces security response grades and chooses\n",
        "security response polices. Furthermore, it serves security\n",
        "defense.\n",
        "IV. SIMULATION EXPERIMENTS\n",
        "In the experiments, simulation software and devices were\n",
        "used to construct the simulation experiment environment which\n",
        "is shown in Fig. 3. The simulated network consisted of sense\n",
        "network, IoT gateway and a computer server which runs the\n",
        "proposed approach. In the sense network, cloning attacks,\n",
        "mutated cloning attacks, replay attacks and mutated replay\n",
        "attacks were simulated. The simulated server used AIS\n",
        "principles and mechanisms to detect security threats, compute\n",
        "danger, save security defense strategy library, respond to\n",
        "security threats and defend the security.\n",
        "Figure 3. Simulation Experiment Environment.\n",
        "The sense network kept safe for three hours to collect initial\n",
        "self data set. The simulated server detected simulated attacks\n",
        "and adopted defense measures. Simulation experiment results\n",
        "are shown in Table I.\n",
        "TABLE I. SIMULATION EXPERIMENT RESULTS\n",
        "ID Number Defense Measures Implementation\n",
        "Times\n",
        "1 Logging 500\n",
        "2 Alarm 309\n",
        "3 Forensic 231\n",
        "4 Modification 132\n",
        "5 Part Deletion 95\n",
        "6 Abandonment 53\n",
        "Table I shows that the proposed approach can detect\n",
        "security threats and change detectors to adapt the dynamic IoT\n",
        "environment. Meanwhile, it indicates that appropriate defense\n",
        "strategies and measures can be adopted to deal with the relative\n",
        "security threats.\n",
        "V. CONCLUSION\n",
        "IoT has some special security requests. Traditional security\n",
        "models for computer networks can not be applied in IoT security directly. The proposed approach uses traditional\n",
        "security models for reference and adopts circular links to\n",
        "construct a dynamic defense system for IoT security. The first\n",
        "link performs based on recognition of security threats. The\n",
        "other links use the real data provided by its previous link. The\n",
        "whole process for IoT security is dynamic. Furthermore,\n",
        "principles in immunology are simulated and applied into the\n",
        "key links. It makes that the proposed model can be adaptive to\n",
        "the IoT environment. The proposed approach changes the\n",
        "defense ways for IoT security and provides a novel effective\n",
        "method to ensure IoT security.\n",
        "ACKNOWLEDGMENT\n",
        "This work is supported by the National Natural Science\n",
        "Foundation of China (No. 61103249), the Open Fund of\n",
        "Artificial Intelligence Key Laboratory of Sichuan Province (No.\n",
        "2011RYJ01), the Scientific Research Fund of Sichuan\n",
        "Provincial Education Department (No. 13ZA0107, 13ZB0106\n",
        "and 13TD0014) and the Construction Project of Science\n",
        "Research Innovation Team of Leshan Normal University.\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 31.3 s (started: 2022-01-18 18:56:53 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdY_kTj5GejQ",
        "outputId": "248a39b3-7961-4c7a-f8dd-bd7ad27697ae"
      },
      "source": [
        "text=\"\"\"\n",
        "\n",
        "Contextual String Embeddings for Sequence Labeling\n",
        "Roland Vollgraf\n",
        "Zalando Research Mu ̈hlenstraße 25 10243 Berlin\n",
        "Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CONLL03 shared task.\n",
        "We release all code and pre-trained language models in a simple-to-use framework to the re- search community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair\n",
        "1 Introduction\n",
        "A large family of NLP tasks such as named entity recognition (NER) and part-of-speech (PoS) tagging may be formulated as sequence labeling problems; text is treated as a sequence of words to be labeled with linguistic tags. Current state-of-the-art approaches for sequence labeling typically use the LSTM variant of bidirectional recurrent neural networks (BiLSTMs), and a subsequent conditional random field (CRF) decoding layer (Huang et al., 2015; Ma and Hovy, 2016).\n",
        "A crucial component in such approaches are word embeddings, typically trained over very large collec- tions of unlabeled data to assist learning and generalization. Current state-of-the-art methods concatenate up to three distinct embedding types:\n",
        "1. Classical word embeddings (Pennington et al., 2014; Mikolov et al., 2013), pre-trained over very large corpora and shown to capture latent syntactic and semantic similarities.\n",
        "2. Character-level features (Ma and Hovy, 2016; Lample et al., 2016), which are not pre-trained, but trained on task data to capture task-specific subword features.\n",
        "3. Contextualized word embeddings (Peters et al., 2017; Peters et al., 2018) that capture word seman- tics in context to address the polysemous and context-dependent nature of words.\n",
        "This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/\n",
        "1638\n",
        "Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649 Santa Fe, New Mexico, USA, August 20-26, 2018.\n",
        "Alan Akbik\n",
        "Zalando Research Mu ̈hlenstraße 25 10243 Berlin\n",
        "Duncan Blythe\n",
        "Zalando Research Mu ̈hlenstraße 25\n",
        "10243 Berlin {firstname.lastname}@zalando.de\n",
        "Abstract\n",
        " \n",
        "  B-PER\n",
        "E-PER\n",
        "  Sequence Labeling Model\n",
        "      rGeorge rWashington rwas rborn\n",
        "George Washington was born\n",
        "Figure 1: High level overview of proposed approach. A sentence is input as a character sequence into a pre-trained bidirec- tional character language model. From this LM, we retrieve for each word a contextual embedding that we pass into a vanilla BiLSTM-CRF sequence labeler, achieving robust state-of-the-art results on downstream tasks (NER in Figure).\n",
        "Contextual string embeddings. In this paper, we propose a novel type of contextualized character- level word embedding which we hypothesize to combine the best attributes of the above-mentioned embeddings; namely, the ability to (1) pre-train on large unlabeled corpora, (2) capture word meaning in context and therefore produce different embeddings for polysemous words depending on their usage, and (3) model words and context fundamentally as sequences of characters, to both better handle rare and misspelled words as well as model subword structures such as prefixes and endings.\n",
        "We present a method to generate such a contextualized embedding for any string of characters in a sentential context, and thus refer to the proposed representations as contextual string embeddings. Neural character-level language modeling. We base our proposed embeddings on recent advances in neural language modeling (LM) that have allowed language to be modeled as distributions over se- quences of characters instead of words (Sutskever et al., 2011; Graves, 2013; Kim et al., 2015). Recent work has shown that by learning to predict the next character on the basis of previous characters, such models learn internal representations that capture syntactic and semantic properties: even though trained without an explicit notion of word and sentence boundaries, they have been shown to generate grammat- ically correct text, including words, subclauses, quotes and sentences (Sutskever et al., 2014; Graves, 2013; Karpathy et al., 2015). More recently, Radford et al. (2017) showed that individual neurons in a large LSTM-LM can be attributed to specific semantic functions, such as predicting sentiment, without explicitly trained on a sentiment label set.\n",
        "We show that an appropriate selection of hidden states from such a language model can be utilized to generate word-level embeddings that are highly effective in downstream sequence labeling tasks. State-of-the-art sequence labeling. Based on this, we propose the sequence tagging architecture illus- trated in Figure 1: each sentence is passed as a sequence of characters to a bidirectional character-level neural language model, from which we retrieve for each word the internal character states to create a contextual string embedding. This embedding is then utilized in the BiLSTM-CRF sequence tagging module to address a downstream NLP task (NER in the Figure).\n",
        "We experimentally verify our approach in the classic sequence labeling tasks of named entity recog- nition for English and German, phrase chunking and part-of-speech tagging, and find that our approach reliably achieves state-of-the-art results. In particular, for both German and English NER, our approach significantly improves the state-of-the-art. But even for highly saturated tasks such as PoS tagging and chunking we find slight improvements over the already strong state-of-the-art (see Table 1).\n",
        "We also find that our proposed embeddings on some tasks subsume previous embedding types, en- abling simplified sequence labeling architectures. In addition to this, the character-level LM is compact and relatively efficient to train in comparison to word-level models. This allows us to easily train models for new languages or domains.\n",
        "Contributions. To summarize, this paper proposes contextual string embeddings, a novel type of word embeddings based on character-level language modeling, and their use in a state-of-the-art sequence labeling architecture. Specifically, we\n",
        "• illustrate how we extract such representations from a character-level neural language model, and 1639\n",
        "OO\n",
        "    Character Language Model\n",
        "\n",
        "Task\n",
        "NER English\n",
        "NER German Chunking PoS tagging\n",
        "PROPOSED 93.09±0.12\n",
        "88.32±0.2 96.72±0.05 97.85±0.01\n",
        "Previous best\n",
        "92.22±0.1\n",
        "(Peters et al., 2018) 78.76\n",
        "(Lample et al., 2016) 96.37±0.05\n",
        "(Peters et al., 2017) 97.64\n",
        "(Choi, 2016)\n",
        "  Table 1: Summary of evaluation results for best configuration of proposed architecture, and current best published results. The proposed approach significantly outperforms previous work on the CONLL03 NER task for German and English and slightly outperforms previous works on CONLL2000 chunking and Penn treebank PoS tagging.\n",
        "integrate them into a simplified sequence labeling architecture;\n",
        "• present experiments in which we quantiatively evaluate the usefulness and inherent semantics of the proposed embeddings against previous embeddings and their stacked combinations in downstream tasks;\n",
        "• report a new state-of-the-art on the CONLL03 NER task for English (93.09 F1, ↑0.87 pp vs. pre- vious best) and German (88.33 F1, ↑9.56 pp vs. previous best), and state-of-the-art scores for chunking and PoS;\n",
        "• release all code and pre-trained language models in a simple-to-use framework to the research com- munity, to enable reproduction of these experiments and application of our proposed embeddings to other tasks.\n",
        "This paper is structured as follows: we present our approach for extracting contextual string embed- dings from character-level language models in Section 2. We evaluate our approach against prior work in Section 3. We then discuss the results and present an outlook into future work in Section 4.\n",
        "2 Contextual String Embeddings\n",
        "Our proposed approach passes sentences as sequences of characters into a character-level language model to form word-level embeddings. Refer to Figure 2 for an example illustration.\n",
        "2.1 Recurrent Network States\n",
        "Like recent work, we use the LSTM variant (Hochreiter and Schmidhuber, 1997; Graves, 2013; Zaremba et al., 2014) of recurrent neural networks (Sutskever et al., 2011) as language modeling architecture. These have been shown to far outperform earlier n-gram based models (Jozefowicz et al., 2016) due to the ability of LSTMs to flexibly encode long-term dependencies with their hidden state. We use characters as atomic units of language modeling (Graves, 2013), allowing text to be treated as a sequence of characters passed to an LSTM which at each point in the sequence is trained to predict the next character1. This means that the model possesses a hidden state for each character in the sequence.\n",
        "Formally, the goal of a character-level language model is to estimate a good distribution P (x0:T ) oversequencesofcharaters(x0,x1,...,xT)=:x0:T reflectingnaturallanguageproduction(Rosenfeld, 2000). By training a language model, we learn P (xt|x0, . . . , xt−1), an estimate of the predictive distri- bution over the next character given past characters. The joint distribution over entire sentences can then be decomposed as a product of the predictive distribution over characters conditioned on the preceeding characters:\n",
        "T\n",
        "P (x0:T ) = 􏰆 P (xt|x0:t−1) (1)\n",
        "t=0\n",
        "1Note that character-level LM is different from character-aware LM (Kim et al., 2015) which still operates on the word-\n",
        "level, but also takes into account character-level features through an additional CNN encoding step. 1640\n",
        " \n",
        "rWashington\n",
        "     Figure 2: Extraction of a contextual string embedding for a word (“Washington”) in a sentential context. From the forward language model (shown in red), we extract the output hidden state after the last character in the word. This hidden state thus contains information propagated from the beginning of the sentence up to this point. From the backward language model (shown in blue), we extract the output hidden state before the first character in the word. It thus contains information propagated from the end of the sentence to this point. Both output hidden states are concatenated to form the final embedding.\n",
        "In the LSTM architecture, the conditional probability P (xt|x0:t−1) is approximately a function of the network output ht.\n",
        "T\n",
        "P (xt|x0:t−1) ≈ 􏰆 P (xt|ht; θ) (2)\n",
        "t=0\n",
        "ht represents the entire past of the character sequence. In an LSTM in particular, it is computed recur-\n",
        "sively, with the help of an additional recurrent quantity ct, the memory cell, ht (x0:t−1) = fh (xt−1, ht−1, ct−1; θ)\n",
        "ct (x0:t−1) = fc (xt−1, ht−1, ct−1; θ) ,\n",
        "where θ denotes all the parameters of the model. h−1 and c−1 can be initialized with zero or can be treated as part of the model parameters θ. In our model, a fully conected softmax layer (without bias) is placed ontop of ht, so the likelihood of every character is given by\n",
        "P (xt|ht; V) = softmax (Vht + b) (3) = exp(Vht+b) (4)\n",
        "∥exp (Vht + b)∥1\n",
        "where V and b, weights and biases, are part of the model parameters θ (Graves, 2013; Jozefowicz et al.,\n",
        "2016).\n",
        "2.2 Extracting Word Representations\n",
        "We utilize the hidden states of a forward-backward recurrent neural network to create contextualized word embeddings. This means, alongside with the forward model (2), we also have a backward model, which works in the same way but in the reversed direction:\n",
        "T\n",
        "Pb (xt|xt+1:T) ≈ 􏰆Pb 􏰀xt|hbt,θ􏰁 (5)\n",
        "t=0\n",
        "hbt =fhb􏰀xt+1,hbt+1,cbt+1;θ􏰁 (6)\n",
        "cbt =fcb􏰀xt+1,hbt+1,cbt+1;θ􏰁 (7)\n",
        "Note that, in the following, we will use the superscript ·f to define hft := ht, cft := ct for the forward model described in the previous section.\n",
        "From this forward-backward LM, we concatenate the following hidden character states for each word: from the fLM, we extract the output hidden state after the last character in the word. Since the fLM\n",
        " 1641\n",
        "\n",
        "is trained to predict likely continuations of the sentence after this character, the hidden state encodes semantic-syntactic information of the sentence up to this point, including the word itself. Similarly, we extract the output hidden state before the word’s first character from the bLM to capture semantic- syntactic information from the end of the sentence to this character. Both output hidden states are con- catenated to form the final embedding and capture the semantic-syntactic information of the word itself as well as its surrounding context.\n",
        "Formally, let the individual word-strings begin at character inputs with indices t0, t1, . . . , tn, then we define contextual string embeddings of these words as:\n",
        "Where:\n",
        "n\n",
        "P􏰇(y0:n|r0:n) ∝ 􏰆 ψi(yi−1, yi, ri) (10)\n",
        "i=1\n",
        "ψi(y′, y, r) = exp(Wy′,yr + by′,y) (11)\n",
        "􏰄hf 􏰅 wCharLM := ti+1−1\n",
        "i h bt i − 1\n",
        "(8)\n",
        "We illustrate our approach in Figure 2 for a word in an example sentence, with the fLM in red and the bLM shown in blue.\n",
        "Our approach thus produces embeddings from hidden states that are computed not only on the char- acters of a word, but also the characters of the surrounding context, since it influences the LM’s ability to predict likely continuations of a sentence. As we later illustrate in Section 3.4, our proposed approach thus produces different embeddings for the same lexical word string in different contexts, and is able to accurately capture the semantics of contextual use together with word semantics itself.\n",
        "2.3 Sequence Labeling Architecture\n",
        "In the default configuration of our approach, the final word embeddings are passed into a BiLSTM-CRF sequence labeling module as proposed by Huang et al. (2015) to address downstream sequence labeling tasks. Then let us call the inputs to the BiLSTM gl: w0, . . . , wn. Then we have that:\n",
        "􏰂rf 􏰃\n",
        "ri:= i (9)\n",
        "Where rfi and rfi are the forward and backward output states of the BiLSTM gl. The final sequence probability is then given by a CRF over the possible sequence labels y:\n",
        "Alternatively, we also experiment with directly applying a simple feedforward linear architecture (es- sentially multinomial logistic regression (Menard, 2018)). This configuration simply linearly projects the hidden states of the neural character LM to make predictions:\n",
        "ri = Wrwi + br (12) Then the prediction of the label is given by:\n",
        "P (yi = j|ri) = softmax(ri)[j] (13)\n",
        "Stacking Embeddings. Current sequence labeling models often combine different types of embeddings by concatenating each embedding vector to form the final word vectors. We similarly experiment with different stackings of embeddings; for instance in many configurations it may be benficial to add classic word embeddings to add potentially greater latent word-level semantics to our proposed embeddings. In this case, the final words representation is given by\n",
        "􏰂wCharLM 􏰃\n",
        "wi= wiGloVe (14)\n",
        "i\n",
        "Here wGloV e is a precomputed GLOVE embedding (Pennington et al., 2014). We present different i\n",
        "configurations of stacked embeddings in the next section for the purpose of evaluation. 1642\n",
        "rbi\n",
        "\n",
        "3 Experiments\n",
        "We conduct an experimental evaluation to assess in how far our proposed contextual string embeddings are useful for sequence labeling, and how they compare to existing embedding types. In addition, we aim to gain insights into the inherent semantics of the proposed embeddings.\n",
        "Tasks. To this end, we evaluate our embeddings in four classic sequence labeling tasks in their de- fault evaluation setups, namely named entity recognition in the CONLL03 setup (Tjong Kim Sang and De Meulder, 2003), chunking in the CONLL2000 setup (Tjong Kim Sang and Buchholz, 2000) and PoS tagging in the default Penn treebank setup described by Collins (2002). We evaluate NER to determine in how far our embeddings are helpful for shallow semantic tasks, while we use chunking and PoS to determine in how far they are helpful for shallow syntactic tasks. We also include the German NER task from the CONLL03 setup to evaluate our embeddings in a language with richer morphology.\n",
        "3.1 Experimental Setup\n",
        "We utilize the BiLSTM-CRF sequence labeling architecture proposed by Huang et. al (2015) in all con- figurations of our comparative evaluation. In this architecture, we evaluate our proposed contextual string embeddings in stacked combinations with the three types of previous embeddings discussed in Section 1; namely (1) pre-trained static (“classic”) word embeddings (Pennington et al., 2014), (2) task- trained (i.e. not pre-trained) static character features (Ma and Hovy, 2016; Lample et al., 2016), and (3) pre-trained contextual word embeddings (Peters et al., 2018).\n",
        "Baselines. We also evaluate setups that involve only previous word embeddings. These are effectively reimplementations of state-of-the-art approaches within our sequence labeling architecture. We conduct these experiments to isolate the impact of our proposed embeddings vis-a-vis earlier approaches. The setups are as follows:\n",
        "HUANG : A standard BiLSTM-CRF setup with pre-trained word embeddings, and thus a reimplemen- tation of Huang et al. (2015).\n",
        "LAMPLE :AhierarchicalBiLSTM-CRFsetupwithpre-trainedwordembeddings,inwhichtask-trained character features are additionally computed by a character-level BiLSTM for each word. It is thus a reimplementation of Lample et al. (2016).\n",
        "PETERS : A BiLSTM-CRF setup that utilizes the release of Peters et al. (2018) in the ALLENNLP library to produce contextualized word embeddings, and thus is effectively a reimplementation of Peters et al. (2018) in our framework. Note that only English embeddings are provided, so we use this baseline only on the English tasks.\n",
        "Proposed approach. We evaluate our proposed contextual string embeddings in the following configu- rations:\n",
        "PROPOSED : The simplest setup of our proposed approach that relies solely on our proposed contex- tual string embeddings, passed to a standard BiLSTM-CRF architecture. This configuration thus matches the illustration in Figure 1.\n",
        "PROPOSED+WORD : An extension of PROPOSED in which we concatenate pre-trained static word em- beddings with our contextual string embeddings as per Equation (14), to determine whether they complement or subsume our proposed embeddings.\n",
        "PROPOSED+CHAR :A similar extension in which we concatenate task-trained character features in a hi- erarchical BiLSTM-CRF architecture to our contextual string embeddings, to determine whether they complement or subsume our proposed embeddings.\n",
        "PROPOSED+WORD+CHAR : We also evaluate a setting in which we add both pre-trained word and task- trained character embeddings.\n",
        "PROPOSED+ALL : Finally, we evaluate a setup that uses all four embeddings. Since Peters et al. (2018) embeddings are only distributed for English, we use this setup only on the English tasks.\n",
        "1643\n",
        "\n",
        "Approach\n",
        "proposed\n",
        "PROPOSED PROPOSED+WORD PROPOSED+CHAR PROPOSED+WORD+CHAR PROPOSED+ALL\n",
        "baselines\n",
        "HUANG LAMPLE PETERS\n",
        "best published\n",
        "NER-English F1-score\n",
        "91.97±0.04 93.07±0.10 91.92±0.03 93.09±0.12 92.72±0.09\n",
        "88.54±0.08 89.3±0.23 92.34±0.09\n",
        "92.22±0.10\n",
        "(Peters et al., 2018) 91.93±0.19\n",
        "(Peters et al., 2017) 91.71±0.10\n",
        "(Liu et al., 2017) 91.21\n",
        "(Ma and Hovy, 2016)\n",
        "NER-German F1-score\n",
        "85.78 ± 0.18 88.20 ± 0.21 85.88 ± 0.20 88.32 ± 0.20 n/a\n",
        "82.32 ± 0.35 83.78 ± 0.39 n/a\n",
        "78.76\n",
        "(Lample et al., 2016) 77.20\n",
        "(Seyler et al., 2017) 76.22\n",
        "(Gillick et al., 2015) 75.72\n",
        "(Qi et al., 2009)\n",
        "Chunking F1-score\n",
        "96.68±0.03 96.70±0.04 96.72±0.05 96.71±0.07 96.65±0.05\n",
        "95.4±0.08 95.34±0.06 96.69±0.05\n",
        "96.37±0.05\n",
        "(Peters et al., 2017) 95.96±0.08\n",
        "(Liu et al., 2017)\n",
        "95.77\n",
        "(Hashimoto et al., 2016) 95.56\n",
        "Søgaard et al. (2016)\n",
        "POS Accuracy\n",
        "97.73±0.02 97.82±0.02 97.8±0.01 97.76±0.01 97.85±0.01\n",
        "96.94±0.02 97.02±0.03 97.81± 0.02\n",
        "97.64\n",
        "(Choi, 2016)\n",
        "97.55\n",
        "(Ma and Hovy, 2016) 97.53±0.03\n",
        "(Liu et al., 2017) 97.30\n",
        "(Lample et al., 2016)\n",
        "    Table 2: Summary of evaluation results or all proposed setups and baselines. We also list the best published scores for each task for reference. We significantly outperform all previous works on NER, and slightly outperform the previous state-of-the-art in PoS tagging and chunking.\n",
        "3.2 Model Training and Parameters\n",
        "Character-level language models. We train our LMs using SGD to perform truncated backpropagation through time (BPTT) with a window length of 250, a non-annealed learning rate of 20.0, a batch size of 100, clipping gradients at 0.25 and dropout probabilities of 0.25. We prepare the data by unking-the rarest 0.0001 percent of characters. We set the number of hidden states of the (one-layered) LSTM to 2048. We halt training by tracking the performance on the validation set, stopping when negligible gains were observed, or after 1 week, whichever came first.\n",
        "We train our English models on the 1-billion word corpus (Chelba et al., 2013) and our German models on a corpus of half a billion words aggregated from various open source corpora in the OPUS project2. After training, we find the models trained for the full week achieve character level perplexity on the supplied test-set of 2.42 for English and 2.38 for German. Resources did not allow us to train for longer or on larger language model variants. We hypothesize that more resources and time would permit learning an even better model.\n",
        "Sequence tagging model. We train the sequence tagging model using vanilla SGD with no momentum, clipping gradients at 5, for 150 epochs. We employ a simple learning rate annealing method in which we halve the learning rate if training loss does not fall for 5 consecutive epochs. Following recommendations of Reimers et al. (2017)’s in-depth analysis of hyperparameters in sequence labeling, we utilize varia- tional dropout, set the number of hidden states per-layer of the LSTM to 256, set the number of LSTM layers to 1, and perform model selection over the learning rate ∈ {0.01, 0.05, 0.1} and mini-batch size ∈ {8, 16, 32}, choosing the model with the best F-measure (for NER and chunking) or accuracy (for PoS) in the best epoch as judged by performance on the validation set. Following Peters et al. (2017), we then repeat the experiment for the model chosen 5 times with different random seeds, and train using both train and development set, reporting both average performance and standard deviation over these runs on the test set as final performance.\n",
        "Classic word embeddings. Following Reimers et al. (2017), we use GLOVE embeddings for English NER and KOMNIOS embeddings (Komninos and Manandhar, 2016) for PoS tagging and chunking. For German, we use the German FASTTEXT embeddings (Grave et al., 2018). In configurations that train character features, we apply a BiLSTM with 25 hidden states to each word separately and extract the final hidden output states (see Lample et al. (2016)).\n",
        "2We aggregate over the PARACRAWL, EUROPARL, OPENSUBTITLES2018, and WIKIPEDIA releases in OPUS for versatile text and to allow reproduction of our results.\n",
        " 1644\n",
        "\n",
        "Embedding + Architecture\n",
        "PROPOSED+WORD +BiLSTM-CRF +Map-CRF +Map\n",
        "PROPOSED +BiLSTM-CRF +Map-CRF +Map\n",
        "CLASSIC WORD EMBEDDINGS +BiLSTM-CRF\n",
        "+Map-CRF\n",
        "+Map\n",
        "NER-English F1-score\n",
        "93.07 ± 0.10 90.17 ± 0.06 79.86 ± 0.12\n",
        "91.97 ± 0.04 88.62 ± 0.15 81.42 ± 0.16\n",
        "88.54 ± 0.08 66.53 ± 0.03 48.79 ± 0.27\n",
        "NER-German F1-score\n",
        "88.20 ± 0.21 85.17 ± 0.04 76.97 ± 0.16\n",
        "85.78 ± 0.18 82.27 ± 0.22 73.90 ± 0.09\n",
        "82.32 ± 0.35 72.69 ± 0.12 57.43 ± 0.12\n",
        "Chunking F1-score\n",
        "96.70 ± 0.04 96.05 ± 0.04 90.55 ± 0.05\n",
        "96.68 ± 0.03 95.96 ± 0.05 90.50 ± 0.06\n",
        "95.40 ± 0.08 91.26 ± 0.04 65.01 ± 0.50\n",
        "POS Accuracy\n",
        "97.82 ± 0.02 97.62 ± 0.01 97.35 ± 0.01\n",
        "97.73 ± 0.02 97.53 ± 0.02 97.26 ± 0.01\n",
        "96.94 ± 0.02 94.06 ± 0.02 89.58 ± 0.02\n",
        "    Table 3: Additional ablation experiment in which we evaluate our approach without BiLSTM and CRF. Here, we instead use a simple linear map over the embeddings to determine their direct information content.\n",
        "3.3 Results\n",
        "Our experimental results are summarized in Table 2 for each of the four tasks. We find that our approach either slightly or strongly surpasses all previously published results. This shows that our proposed con- textual string embeddings are indeed highly useful for sequence labeling. In more detail, we make the following observations:\n",
        "New state-of-the-art for NER. We find that our approach performs particularly well on the task of named entity recognition. For English, we surpass the previous state-of-the-art approach by a significant margin (93.09 F1, ↑0.87 pp vs. Peters et al. (2018)). For German in particular, we significantly raise the state-of-the-art (88.33 F1, ↑9.56 pp vs. Lample et al. (2016)). We hypothesize that pre-trained contextual character-level features are particularly helpful for this task, since entities are an open vocabulary of names that are often indicated by character features (such as capitalization or endings), as well as their contextual use. In addition, the CONLL03 shared task data set contains many sentences (such as article titles) that are in all-caps; since most traditional embeddings perform lower casing to manage vocabulary size, we assume that our character model can better capture the features of such sentences.\n",
        "Good performance on syntactic tasks. We also find that our approach slightly outperforms the latest state-of-the-art approaches for chunking and PoS tagging. We do not however see the same improve- ments as for NER, which we mainly attribute to the fact that syntactic tasks are already solved by current approaches to very high accuracy, making it difficult to record further strong improvements (see Man- ning et al. (2014) for a discussion of this issue).\n",
        "Traditional word embeddings helpful. Additionally, we observe that the added use of classic word embeddings in setup PROPOSED+WORD often produces the best results. Especially for NER, the use of classic word embeddings increases average F1 score by 1.1 pp to 93.07, compared with 91.97 for PRO- POSED. We hypothesize that classic word embeddings capture word-level semantics that complement the strong character-level features of our proposed embeddings. We therefore recommend the setup PROPOSED+WORD as default setup for sequence labeling tasks.\n",
        "Task-specific character features unnecessary. Another observation is that setups with task-specific character features (PROPOSED+CHAR and PROPOSED+WORD+CHAR) do not perform better than those with- out. This indicates that they are largely subsumed by our proposed embeddings, and are thus no longer required. A positive side-effect of this is that this simplifies training of the sequence labeler (see Sec- tion 3.5).\n",
        "3.4 Inherent Semantics\n",
        "Quantiative investigation (Table 3). To gain additional insight into the content of the contextual string embeddings relative to the task, we replace the standard BiLSTM-CRF in the sequence labeling architec- ture with a direct feedforward map as per Equation (12), with and without a CRF decoder. This simplified approach makes predictions directly on the basis of the proposed embeddings without additional learning of the BiLSTM recurrence. The results for this simplified architecture are displayed in Table 3 for the\n",
        "1645\n",
        "\n",
        "word\n",
        "Washington\n",
        "Washington\n",
        "Washington\n",
        "Washington\n",
        "Washington\n",
        "context\n",
        "(a) Washington to curb support for [..]\n",
        "(b) [..] Anthony Washington (U.S.) [..]\n",
        "(c) [..] flown to Washington for [..]\n",
        "(d) [..] when Washington came charging back [..]\n",
        "(e) [..] said Washington [..]\n",
        "selected nearest neighbors\n",
        "(1) Washington would also take [..] action [..] (2) Russia to clamp down on barter deals [..] (3) Brazil to use hovercrafts for [..]\n",
        "(1) [..] Carla Sacramento ( Portugal ) [..] (2) [..] Charles Austin ( U.S. ) [..]\n",
        "(3) [..] Steve Backley ( Britain ) [..]\n",
        "(1) [..] while visiting Washington to [..]\n",
        "(2) [..] journey to New York City and Washington [..] (14) [..] lives in Chicago [..]\n",
        "(1) [..] point for victory when Washington found [..] (4) [..] before England struck back with [..]\n",
        "(6) [..] before Ethiopia won the spot kick decider [..]\n",
        "(1) [..] subdue the never-say-die Washington [..] (4) [..] a private school in Washington [..]\n",
        "(9) [..] said Florida manager John Boles [..]\n",
        "      Table 4: Examples of the word “Washington” in different contexts\n",
        "cosine distance over our proposed embeddings. Since our approach produces different embeddings based on context, we retrieve different nearest neighbors for each mention of the same word.\n",
        "setups PROPOSED and PROPOSED+WORD, as well as for a setup that involves only traditional word em- beddings (GLOVE for English NER, KOMNIOS for English PoS and chunking, FASTTEXT for German NER).\n",
        "We find that the effect of removing the BiLSTM layer on downstream task accuracy is far lower for the proposed embeddings than for classic embeddings. For the setups PROPOSED and PROPOSED+WORD, we record only an average drop of 3% in F-score/accuracy between the BiLSTM-CRF and Map-CRF architectures. This stands in contrast to classic embeddings in which we find an average drop of 20% from BiLSTM-CRF to Map-CRF. This indicates that the inherent semantics of the proposed embeddings are meaningful enough as to require much less powerful learning architectures on top to perform down- stream sequence labeling tasks. In particular, for PoS tagging, the simple feedforward map is competitive to BiLSTM and much more effective to train.\n",
        "Qualitative inspection (Table 4). To illustrate the contextualized nature of our proposed embeddings, we present example embeddings of the polysemous word “Washington” in different contexts. We com- pute contextual string embeddings for all words in the English CONLL03 corpus and compute nearest neighbors in the embedding space using the cosine distance. We then look up nearest neighbors for different mentions of the word “Washington”.\n",
        "As Table 4 shows, the embeddings successfully pry apart person, place, legislative entity and team (a-d). For instance, “Washington” used as last name in context (b) is closest to other last names, many of which are also place names (“Carla Sacramento”); “Washington” used as a sport team name in context (d) is closest to other place names used in sports team contexts. We include a negative example (e) in Table 4 in which the context is not sufficient to determine the type of mention. We hypothesize that modeling semantics in context is a key feature that allows our proposed embeddings to better address downstream sequence labeling task.\n",
        "3.5 Discussion\n",
        "Our proposed approach is one of the first to leverage hidden states from a language model to im- prove sequence labeling performance. Two prior works have suggested related approaches: The first is Liu et al. (2017) that jointly train a character-level language model together with the sequence labeling BiLSTM. In effect, this means that the language model is trained only on labeled task data and therefore has orders of magnitude fewer data available than our proposed approach (which we can pre-train on basically unlimited amounts of unlabled data). We hypothesize that this is the main reason for why our approach outperforms Liu et al. (2017) across all tasks.\n",
        "A second approach is the method by Peters et. al (2017) which proposed to extract hidden states from pre-trained word-level language models as features for downstream NLP tasks. They report new state-\n",
        "1646\n",
        "in the CONLL03 data set, and nearest neighbors using\n",
        "\n",
        "of-the-art results for NER (Peters et al., 2018) and chunking (Peters et al., 2017), but require the training of massive word-level language models: their best configuration uses a language model that was trained for 5 weeks on 32 GPUs (Jozefowicz et al., 2016), and still requires lower casing of all words to deal with the large vocabulary size. Their approach is the strongest baseline against which we compared.\n",
        "We similarly propose to utilize trained LMs to generate embeddings, but consider LM at the character level. This has a number of advantages: First, character-level LM is independent of tokenization and a fixed vocabulary. Second, they produce stronger character-level features, which is particularly useful for downstream tasks such as NER. Finally, such models have a much smaller vocabulary size (distinct characters vs. distinct words) and are thus significantly easier to train and deploy in applications: the LM parameter-count scales according to nlayers n2hidden (not true for word-level LM since these contain many inputs) and may be trained in 1 week on 1 GPU (e.g. significantly less than the 5 weeks on 32 GPUs used by Peters et al. (2017)). This allowed us to train models for other languages such as German with moderate resources, and thus allows our method to more effectively scale to new languages or domains.\n",
        "4 Conclusion\n",
        "We have proposed novel contextual string embeddings and a method for producing them using neural character LM. We find that they capture syntactic-semantic word features and disambiguate words in context, resulting in state-of-the-art performance in classic NLP sequence labeling tasks. To facilitate reproduction of our experiments, and enable application of our proposed embeddings to other tasks, we release all code and pre-trained language models in a simple-to-use framework to the research commu- nity3. Future work will focus on applying these embeddings to additional sentence level tasks such as image-retrieval and neural translation.\n",
        "Acknowledgements\n",
        "We would like to thank the anonymous reviewers for their helpful comments. This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement no 732328 (“FashionBrain”).\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 341 ms (started: 2022-01-01 20:22:23 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_8wGOPDl9v3",
        "outputId": "f688b48c-1cda-4392-eafe-73b204b19b8d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.2 ms (started: 2021-12-31 22:34:09 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iGnPoaRyTC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d5e565-e533-4d97-ce3e-350e329ad841"
      },
      "source": [
        "# Define search queries and embed them to vectors\n",
        "queries = [\n",
        "           \n",
        "    #'Contextual string embeddings which are trained without any explicit notion of words and therefore, words are modelled as sequences of characters.'\n",
        "    'IoT devices are more prone to security than traditional computers.'\n",
        "    #'IoT matters a lot in real world applications these days'\n",
        "    #'Implementing IoT itself has challenges. That included figuring out how to secure connected devices and networks, and the data they handle. IoT devices are more prone to security than traditional computers.'\n",
        "\n",
        "    #'It has special features in encryption and decryption in terms of speed and it can also improve internet security by several structures during its implementation.'\n",
        "    \n",
        "    #'The hidden message in which private key is just recognized by the receiver and public key by both sender and receiver. The encryption process must be done secretly and safely.'\n",
        "    #ref_3: another query\n",
        "    # 'It should be noted that for digital signature, we must go from the sender to the receiver, so the receiver of the former step acts as the sender now, and the sender of the former step acts as the receiver.'\n",
        "\n",
        "    #ref 9:\n",
        "    #'When the message is encrypted, in other way receiver tries to open the message by its private key or to encrypt the message. For message decryption in HAN algorithm, NTRU algorithm will be used partially.'\n",
        "\n",
        "    #'However, abstractive summarisation is also better than extractive summarisation since the summary is an approximate representation of a human-generated summary, which makes it more meaningful.'\n",
        "\n",
        "    ]\n",
        "query_embeddings = model.encode(queries)\n",
        "import numpy as np\n",
        "print(\"Query shape: \", query_embeddings.shape)\n",
        "\n",
        "print(\"\\n\\n======================\\n\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query shape:  (1, 512)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "time: 146 ms (started: 2022-01-18 18:57:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHk1U0W4iSKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4579f1-2fdb-416b-e8ee-898f3dfd2d18"
      },
      "source": [
        "#For NSS=1\n",
        "_c=text.strip()\n",
        "_c= _c.replace(\"\\n\",\" \") \n",
        "#print(_c)\n",
        "\n",
        "splitted_c = _c.split(\".\")\n",
        "\n",
        "\n",
        "# splitted = _c.split(\".\")\n",
        "# splitted_c = []\n",
        "\n",
        "# for sentence in splitted:\n",
        "#   if len(sentence.split(\" \")) > 3:\n",
        "#     splitted_c.append(sentence)\n",
        "  \n",
        "for x in range(1, len(splitted_c), 1):\n",
        "    splitted_c[x] = splitted_c[x].lstrip()\n",
        "_c = \".\".join(splitted_c)\n",
        "# Convert the corpus into a list of sentences\n",
        "corpus=[i+\".\" for i in _c.split('.') if i != '' and len(i.split(' '))>=4]\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.', 'Velit scelerisque in dictum non consectetur a erat.', 'Sit amet justo donec enim diam vulputate.', 'Id aliquet lectus proin nibh nisl condimentum id venenatis a.', 'Eget gravida cum sociis natoque penatibus et magnis dis.', 'Habitant morbi tristique senectus et netus et.', 'Interdum consectetur libero id faucibus nisl tincidunt eget nullam.', 'Aliquam purus sit amet luctus.', 'Fringilla ut morbi tincidunt augue interdum velit.', 'Neque sodales ut etiam sit.', 'Quam viverra orci sagittis eu volutpat odio facilisis mauris.', 'Ornare suspendisse sed nisi lacus sed.', 'Iaculis at erat pellentesque adipiscing commodo elit at imperdiet dui.', 'Quam nulla porttitor massa id neque aliquam vestibulum morbi.', 'Dignissim diam quis enim lobortis scelerisque fermentum dui faucibus.', 'Turpis egestas integer eget aliquet.', 'In nisl nisi scelerisque eu ultrices vitae auctor eu.', 'Dolor sit amet consectetur adipiscing elit duis.', 'Tortor dignissim convallis aenean et tortor at.', 'Iaculis at erat pellentesque adipiscing commodo.', 'Viverra suspendisse potenti nullam ac tortor.', 'Elementum nibh tellus molestie nunc non blandit massa enim.', 'Ultricies integer quis auctor elit sed.', 'Varius vel pharetra vel turpis nunc eget lorem dolor.', 'Sit amet massa vitae tortor condimentum.', 'Adipiscing elit ut aliquam purus sit amet luctus venenatis lectus.', 'Nascetur ridiculus mus mauris vitae ultricies leo integer.', 'Urna nunc id cursus metus.', 'Id leo in vitae turpis massa.', 'Blandit turpis cursus in hac habitasse platea.', 'Feugiat sed lectus vestibulum mattis ullamcorper.', 'Diam sit amet nisl suscipit adipiscing bibendum est.', 'Enim nunc faucibus a pellentesque sit amet porttitor eget dolor.', 'Enim eu turpis egestas pretium aenean pharetra.', 'Amet mattis vulputate enim nulla aliquet.', 'Tristique et egestas quis ipsum suspendisse ultrices gravida dictum fusce.', 'Risus commodo viverra maecenas accumsan lacus vel.', 'Eu mi bibendum neque egestas congue quisque egestas diam in.', 'Fermentum odio eu feugiat pretium nibh ipsum consequat.', 'Egestas egestas fringilla phasellus faucibus scelerisque.', 'Sit amet dictum sit amet justo donec.', 'Cum sociis natoque penatibus et magnis dis parturient montes.', 'Habitasse platea dictumst quisque sagittis purus sit amet volutpat.', 'Magna etiam tempor orci eu lobortis elementum nibh tellus molestie.', 'Gravida arcu ac tortor dignissim convallis.', 'Consequat nisl vel pretium lectus quam id leo in.', 'Amet venenatis urna cursus eget.', 'In est ante in nibh.', 'Mauris commodo quis imperdiet massa.', 'Pellentesque diam volutpat commodo sed egestas egestas fringilla phasellus.', 'Laoreet suspendisse interdum consectetur libero id faucibus nisl tincidunt.', 'Enim sit amet venenatis urna cursus eget.', 'Tristique senectus et netus et.', 'Ipsum suspendisse ultrices gravida dictum fusce ut.', 'Velit ut tortor pretium viverra suspendisse potenti nullam ac tortor.', 'Sapien eget mi proin sed libero enim sed faucibus turpis.', 'Ullamcorper malesuada proin libero nunc consequat interdum varius.', 'Suscipit adipiscing bibendum est ultricies integer quis.', 'Libero volutpat sed cras ornare arcu.', 'In ante metus dictum at.', 'Sed augue lacus viverra vitae congue eu consequat.', 'Mi eget mauris pharetra et ultrices neque ornare aenean.', 'Pellentesque elit ullamcorper dignissim cras tincidunt lobortis feugiat vivamus at.', 'Blandit cursus risus at ultrices mi tempus.', 'Ultrices gravida dictum fusce ut placerat.', 'Sit amet justo donec enim.', 'Malesuada fames ac turpis egestas integer eget.', 'Nibh praesent tristique magna sit amet purus gravida quis.', 'Commodo viverra maecenas accumsan lacus vel facilisis volutpat est.', 'Sed viverra tellus in hac habitasse.', 'Eu augue ut lectus arcu bibendum at varius vel pharetra.', 'Leo vel fringilla est ullamcorper eget nulla facilisi etiam dignissim.', 'Gravida dictum fusce ut placerat orci nulla pellentesque dignissim.', 'Varius sit amet mattis vulputate enim nulla aliquet porttitor.', 'Egestas diam in arcu cursus euismod quis viverra nibh.', 'Facilisi cras fermentum odio eu feugiat pretium nibh ipsum consequat.', 'Enim nulla aliquet porttitor lacus luctus.', 'At varius vel pharetra vel turpis.', 'Eget mi proin sed libero enim sed faucibus turpis in.', 'Sed odio morbi quis commodo odio aenean sed.', 'Nibh mauris cursus mattis molestie a iaculis at.', 'Tellus pellentesque eu tincidunt tortor.', 'Massa vitae tortor condimentum lacinia quis.', 'Scelerisque eu ultrices vitae auctor eu augue ut.', 'Purus gravida quis blandit turpis cursus in hac habitasse platea.', 'Ullamcorper sit amet risus nullam eget felis.', 'Adipiscing vitae proin sagittis nisl rhoncus mattis rhoncus urna neque.', 'Pellentesque habitant morbi tristique senectus et netus.', 'Vehicula ipsum a arcu cursus vitae.', 'Amet luctus venenatis lectus magna fringilla urna porttitor rhoncus.', 'Tristique senectus et netus et malesuada.', 'Placerat duis ultricies lacus sed turpis tincidunt id aliquet.', 'Dolor purus non enim praesent elementum facilisis leo.', 'Blandit libero volutpat sed cras ornare arcu.', 'Neque vitae tempus quam pellentesque.', 'Elit eget gravida cum sociis.', 'Nunc lobortis mattis aliquam faucibus purus in massa tempor.', 'Amet consectetur adipiscing elit pellentesque.', 'Fermentum posuere urna nec tincidunt praesent semper feugiat nibh sed.', 'Sed tempus urna et pharetra pharetra massa massa ultricies mi.', 'Turpis egestas integer eget aliquet nibh praesent tristique magna sit.', 'Pellentesque elit ullamcorper dignissim cras tincidunt lobortis feugiat vivamus.', 'In est ante in nibh mauris.', 'Vel facilisis volutpat est velit egestas.', 'Elementum integer enim neque volutpat ac tincidunt vitae.', 'Id velit ut tortor pretium viverra.', 'Commodo viverra maecenas accumsan lacus.', 'Mi bibendum neque egestas congue.', 'Cras sed felis eget velit aliquet sagittis.', 'Et sollicitudin ac orci phasellus egestas tellus rutrum tellus pellentesque.', 'Elementum pulvinar etiam non quam lacus suspendisse faucibus interdum.', 'Morbi tincidunt augue interdum velit euismod in.', 'Vitae justo eget magna fermentum iaculis eu non.', 'Pellentesque diam volutpat commodo sed.', 'Cras ornare arcu dui vivamus arcu.', 'Sed arcu non odio euismod lacinia at quis risus sed.', 'Volutpat blandit aliquam etiam erat.', 'Id neque aliquam vestibulum morbi blandit.', 'Non sodales neque sodales ut etiam sit amet.', 'Elit eget gravida cum sociis natoque penatibus et magnis.', 'Eu feugiat pretium nibh ipsum consequat nisl vel pretium lectus.', 'Suspendisse interdum consectetur libero id faucibus nisl tincidunt eget.', 'Amet risus nullam eget felis eget nunc lobortis mattis.', 'Volutpat consequat mauris nunc congue nisi vitae suscipit tellus.', 'Elementum facilisis leo vel fringilla est ullamcorper eget nulla.', 'Sit amet nulla facilisi morbi tempus iaculis urna id.', 'Bibendum ut tristique et egestas quis.', 'Eu feugiat pretium nibh ipsum consequat nisl vel pretium lectus.', 'Volutpat commodo sed egestas egestas fringilla phasellus faucibus.', 'Mauris a diam maecenas sed enim ut sem.', 'Lacus vestibulum sed arcu non odio.', 'In fermentum et sollicitudin ac orci phasellus egestas.', 'Blandit aliquam etiam erat velit scelerisque.', 'Sit amet nisl suscipit adipiscing.', 'Feugiat sed lectus vestibulum mattis ullamcorper.', 'Nisl suscipit adipiscing bibendum est ultricies integer.', 'Nam libero justo laoreet sit amet cursus sit amet.', 'Mi eget mauris pharetra et.', 'A erat nam at lectus urna duis convallis.', 'Sit amet porttitor eget dolor morbi non.', 'Adipiscing at in tellus integer feugiat scelerisque.', 'A erat nam at lectus urna duis.', 'Sit amet porttitor eget dolor morbi.', 'Massa tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada.', 'Tellus orci ac auctor augue mauris augue neque.', 'Suspendisse potenti nullam ac tortor vitae purus.', 'Ut consequat semper viverra nam libero justo laoreet.', 'Iaculis nunc sed augue lacus viverra vitae congue eu consequat.', 'Accumsan lacus vel facilisis volutpat est velit egestas dui id.', 'Diam sit amet nisl suscipit.', 'Enim lobortis scelerisque fermentum dui faucibus in ornare quam viverra.', 'Vitae auctor eu augue ut lectus.', 'Nunc sed augue lacus viverra.', 'Nibh ipsum consequat nisl vel pretium lectus.', 'Tincidunt ornare massa eget egestas purus viverra accumsan in.', 'Habitasse platea dictumst quisque sagittis purus sit amet volutpat.', 'Turpis massa sed elementum tempus egestas sed.', 'Pellentesque nec nam aliquam sem et tortor consequat id porta.', 'Et netus et malesuada fames ac turpis egestas.', 'Velit sed ullamcorper morbi tincidunt ornare massa.', 'Tincidunt praesent semper feugiat nibh sed pulvinar.', 'Ac tortor dignissim convallis aenean.', 'Risus in hendrerit gravida rutrum quisque non tellus orci ac.', 'In pellentesque massa placerat duis ultricies lacus sed turpis tincidunt.', 'Bibendum enim facilisis gravida neque convallis a cras semper.', 'Nec feugiat in fermentum posuere urna.', 'Lacus suspendisse faucibus interdum posuere lorem ipsum dolor.', 'Id diam vel quam elementum pulvinar etiam non quam lacus.', 'Nibh tellus molestie nunc non blandit massa enim nec dui.', 'Interdum posuere lorem ipsum dolor sit.', 'Arcu bibendum at varius vel.', 'Nibh tellus molestie nunc non blandit massa.', 'Sit amet nisl suscipit adipiscing.', 'Dui accumsan sit amet nulla facilisi morbi tempus.', 'Adipiscing at in tellus integer feugiat scelerisque varius.', 'Sit amet cursus sit amet.', 'Consectetur libero id faucibus nisl tincidunt eget nullam non.', 'Diam quam nulla porttitor massa.', 'Quis viverra nibh cras pulvinar mattis nunc sed.', 'Ultricies mi quis hendrerit dolor magna eget.', 'Vitae congue eu consequat ac.', 'Dolor magna eget est lorem ipsum dolor sit amet.', 'Habitant morbi tristique senectus et netus et malesuada fames ac.', 'Metus dictum at tempor commodo ullamcorper.', 'Proin fermentum leo vel orci porta non pulvinar.', 'In arcu cursus euismod quis.', 'Eget lorem dolor sed viverra ipsum.', 'Volutpat commodo sed egestas egestas fringilla phasellus faucibus scelerisque.', 'Commodo sed egestas egestas fringilla phasellus faucibus scelerisque eleifend.', 'Sem fringilla ut morbi tincidunt augue interdum.', 'Lectus quam id leo in vitae turpis massa.', 'Malesuada pellentesque elit eget gravida cum.', 'Volutpat sed cras ornare arcu dui.', 'Quam pellentesque nec nam aliquam sem.', 'Viverra vitae congue eu consequat ac felis.', 'Eu facilisis sed odio morbi.', 'Sapien faucibus et molestie ac.', 'Mauris nunc congue nisi vitae suscipit tellus mauris.', 'Senectus et netus et malesuada fames.', 'Risus in hendrerit gravida rutrum quisque non tellus orci ac.', 'Volutpat blandit aliquam etiam erat velit scelerisque in dictum non.', 'Varius morbi enim nunc faucibus.', 'Risus pretium quam vulputate dignissim suspendisse.', 'Sed euismod nisi porta lorem mollis aliquam ut.', 'Id aliquet lectus proin nibh.', 'Sed euismod nisi porta lorem.', 'Mauris pharetra et ultrices neque ornare aenean euismod elementum nisi.', 'Fringilla ut morbi tincidunt augue interdum velit euismod in pellentesque.', 'Scelerisque varius morbi enim nunc faucibus a pellentesque.', 'Convallis tellus id interdum velit.', 'Quam pellentesque nec nam aliquam.', 'Nisi vitae suscipit tellus mauris a diam maecenas sed enim.', 'Euismod quis viverra nibh cras pulvinar mattis.', 'Eget arcu dictum varius duis at consectetur lorem.', 'Convallis aenean et tortor at risus.', 'Ut morbi tincidunt augue interdum velit euismod.', 'Duis ultricies lacus sed turpis tincidunt id aliquet.', 'Nibh ipsum consequat nisl vel pretium lectus quam id leo.', 'A cras semper auctor neque.', 'Vitae turpis massa sed elementum tempus.', 'Eget aliquet nibh praesent tristique magna sit amet.', 'Amet venenatis urna cursus eget nunc scelerisque viverra mauris in.', 'Ac tincidunt vitae semper quis lectus nulla at volutpat.', 'Nulla facilisi nullam vehicula ipsum a arcu.', 'Vel facilisis volutpat est velit egestas dui.', 'Sed blandit libero volutpat sed cras ornare arcu dui vivamus.', 'Morbi tincidunt augue interdum velit euismod in pellentesque massa.', 'Tellus at urna condimentum mattis pellentesque id nibh.', 'Lectus nulla at volutpat diam ut venenatis.', 'Purus sit amet volutpat consequat mauris nunc congue.', 'Mattis ullamcorper velit sed ullamcorper morbi tincidunt ornare massa eget.', 'Hac habitasse platea dictumst quisque sagittis.', 'Hendrerit gravida rutrum quisque non tellus orci ac auctor.', 'Tellus at urna condimentum mattis.', 'Mi ipsum faucibus vitae aliquet nec ullamcorper sit.', 'Nisi quis eleifend quam adipiscing vitae proin sagittis.', 'Dictum fusce ut placerat orci nulla pellentesque.', 'Mauris pellentesque pulvinar pellentesque habitant morbi tristique senectus et.', 'Sodales ut eu sem integer vitae justo eget magna.', 'Orci ac auctor augue mauris augue neque gravida in fermentum.', 'Nunc vel risus commodo viverra maecenas accumsan lacus.', 'Euismod nisi porta lorem mollis.', 'Ultrices dui sapien eget mi proin sed.', 'Lobortis mattis aliquam faucibus purus in massa.', 'Adipiscing bibendum est ultricies integer quis auctor elit sed vulputate.', 'Ut faucibus pulvinar elementum integer.', 'Bibendum ut tristique et egestas quis.', 'Facilisis sed odio morbi quis commodo odio aenean sed.', 'Augue ut lectus arcu bibendum at.', 'Eget dolor morbi non arcu risus.', 'Adipiscing bibendum est ultricies integer quis auctor elit sed vulputate.', 'Quisque non tellus orci ac auctor.', 'At auctor urna nunc id cursus metus aliquam.', 'Eleifend mi in nulla posuere sollicitudin.', 'Quisque sagittis purus sit amet volutpat consequat mauris nunc.', 'Ut aliquam purus sit amet luctus venenatis lectus.', 'Quis imperdiet massa tincidunt nunc.', 'Ut ornare lectus sit amet.', 'Semper quis lectus nulla at.', 'Eget magna fermentum iaculis eu non.', 'Faucibus et molestie ac feugiat sed.', 'Eget lorem dolor sed viverra ipsum nunc aliquet bibendum.', 'Tincidunt praesent semper feugiat nibh sed pulvinar proin gravida.', 'Pellentesque diam volutpat commodo sed egestas.', 'Maecenas accumsan lacus vel facilisis volutpat est velit egestas dui.', 'Ultrices in iaculis nunc sed augue lacus viverra.', 'Facilisi nullam vehicula ipsum a.', 'Diam quam nulla porttitor massa id neque.', 'Ornare massa eget egestas purus viverra accumsan in nisl.', 'In arcu cursus euismod quis viverra nibh cras.', 'Feugiat scelerisque varius morbi enim nunc faucibus a pellentesque.', 'Nunc vel risus commodo viverra maecenas accumsan lacus vel.', 'Lacus luctus accumsan tortor posuere ac.', 'Nibh venenatis cras sed felis.', 'Turpis egestas sed tempus urna.', 'Lectus arcu bibendum at varius.', 'Est lorem ipsum dolor sit amet consectetur adipiscing elit.', 'Etiam non quam lacus suspendisse faucibus interdum posuere lorem.', 'Facilisi nullam vehicula ipsum a arcu cursus vitae congue.', 'Morbi leo urna molestie at elementum eu facilisis sed.', 'Molestie a iaculis at erat pellentesque.', 'Proin sagittis nisl rhoncus mattis rhoncus urna neque viverra.', 'Justo nec ultrices dui sapien eget.', 'Eget velit aliquet sagittis id consectetur.', 'Suspendisse potenti nullam ac tortor vitae.', 'At varius vel pharetra vel.', 'Aliquam sem fringilla ut morbi tincidunt augue.', 'Ante in nibh mauris cursus mattis molestie a iaculis at.', 'Neque ornare aenean euismod elementum nisi quis eleifend.', 'Consectetur lorem donec massa sapien faucibus et molestie.', 'Netus et malesuada fames ac.', 'Pharetra sit amet aliquam id diam.', 'Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.', 'Massa tincidunt dui ut ornare.', 'Scelerisque varius morbi enim nunc faucibus a pellentesque sit amet.', 'Imperdiet massa tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada.', 'Varius quam quisque id diam vel quam.', 'Dictum non consectetur a erat nam at lectus urna.', 'In aliquam sem fringilla ut morbi tincidunt augue.', 'Semper auctor neque vitae tempus quam pellentesque nec nam.', 'Vestibulum rhoncus est pellentesque elit ullamcorper dignissim cras tincidunt.', 'Magna eget est lorem ipsum dolor sit.', 'Vestibulum lectus mauris ultrices eros in cursus turpis massa.', 'Id ornare arcu odio ut sem nulla pharetra diam sit.', 'Aliquet porttitor lacus luctus accumsan tortor posuere ac.', 'Id venenatis a condimentum vitae sapien pellentesque habitant morbi tristique.', 'Eget egestas purus viverra accumsan in nisl.', 'Amet risus nullam eget felis eget nunc lobortis.', 'Feugiat in ante metus dictum at tempor.', 'Phasellus egestas tellus rutrum tellus pellentesque eu tincidunt tortor aliquam.', 'Hac habitasse platea dictumst vestibulum rhoncus est pellentesque.', 'Venenatis urna cursus eget nunc scelerisque viverra mauris.', 'Morbi tincidunt augue interdum velit euismod in pellentesque.', 'Amet dictum sit amet justo donec.', 'Lorem donec massa sapien faucibus et molestie ac feugiat sed.', 'In est ante in nibh mauris cursus.', 'Pretium vulputate sapien nec sagittis aliquam malesuada.', 'Fames ac turpis egestas sed.', 'Commodo elit at imperdiet dui.', 'Mi quis hendrerit dolor magna.', 'Mi in nulla posuere sollicitudin aliquam.', 'Dolor sed viverra ipsum nunc.', 'Phasellus vestibulum lorem sed risus ultricies tristique.', 'Integer quis auctor elit sed vulputate mi sit.', 'Volutpat lacus laoreet non curabitur gravida arcu ac.', 'Eget mi proin sed libero enim.', 'At tempor commodo ullamcorper a.', 'Elit ut aliquam purus sit amet.', 'Nunc faucibus a pellentesque sit amet porttitor eget dolor morbi.', 'Netus et malesuada fames ac turpis egestas.', 'Morbi tristique senectus et netus.', 'Fames ac turpis egestas maecenas pharetra convallis posuere morbi.', 'Enim tortor at auctor urna nunc id cursus.', 'Tempus iaculis urna id volutpat lacus laoreet non.', 'Maecenas ultricies mi eget mauris pharetra et ultrices neque ornare.', 'Accumsan lacus vel facilisis volutpat est velit egestas dui.', 'At imperdiet dui accumsan sit amet nulla facilisi.', 'Turpis tincidunt id aliquet risus feugiat in.', 'Nunc pulvinar sapien et ligula ullamcorper malesuada proin libero.', 'Integer quis auctor elit sed vulputate.', 'Netus et malesuada fames ac turpis.', 'Diam donec adipiscing tristique risus.', 'Ornare quam viverra orci sagittis eu.', 'Feugiat in fermentum posuere urna nec tincidunt praesent.', 'Turpis cursus in hac habitasse platea.', 'Aliquam eleifend mi in nulla posuere sollicitudin.', 'Dui faucibus in ornare quam viverra orci sagittis.', 'Scelerisque in dictum non consectetur a.', 'Scelerisque mauris pellentesque pulvinar pellentesque habitant morbi.', 'Dictumst quisque sagittis purus sit amet volutpat consequat mauris nunc.', 'Id faucibus nisl tincidunt eget nullam non nisi est sit.', 'Montes nascetur ridiculus mus mauris vitae.', 'Ut tristique et egestas quis ipsum suspendisse ultrices gravida dictum.', 'Pellentesque dignissim enim sit amet.', 'Mauris in aliquam sem fringilla ut.', 'Ut tellus elementum sagittis vitae.', 'Faucibus et molestie ac feugiat sed lectus vestibulum mattis.', 'Turpis egestas integer eget aliquet nibh praesent tristique magna sit.', 'Lectus vestibulum mattis ullamcorper velit sed ullamcorper morbi tincidunt.', 'Lorem sed risus ultricies tristique nulla.', 'Molestie at elementum eu facilisis.', 'In hendrerit gravida rutrum quisque non tellus orci.', 'At auctor urna nunc id.', 'Habitasse platea dictumst vestibulum rhoncus est pellentesque elit ullamcorper dignissim.', 'Vestibulum rhoncus est pellentesque elit ullamcorper dignissim cras tincidunt lobortis.', 'Nunc id cursus metus aliquam eleifend mi.', 'Magna ac placerat vestibulum lectus mauris ultrices eros in.', 'Non odio euismod lacinia at.', 'At auctor urna nunc id cursus metus aliquam eleifend.', 'Morbi non arcu risus quis.', 'Dolor sit amet consectetur adipiscing elit.', 'Aliquam nulla facilisi cras fermentum odio eu feugiat pretium nibh.', 'At lectus urna duis convallis convallis tellus id interdum velit.', 'Parturient montes nascetur ridiculus mus mauris vitae ultricies.', 'At tempor commodo ullamcorper a lacus vestibulum sed.', 'Proin nibh nisl condimentum id venenatis a.', 'Accumsan tortor posuere ac ut consequat.', 'Orci nulla pellentesque dignissim enim sit amet venenatis.', 'Cras semper auctor neque vitae tempus quam.', 'Elementum eu facilisis sed odio morbi quis.', 'Aliquet risus feugiat in ante metus.', 'Leo a diam sollicitudin tempor id eu nisl.', 'Dui faucibus in ornare quam viverra orci sagittis eu.', 'Aliquet nec ullamcorper sit amet risus.', 'Eros in cursus turpis massa tincidunt.', 'Pulvinar elementum integer enim neque volutpat ac tincidunt vitae semper.', 'Velit aliquet sagittis id consectetur.', 'Sed enim ut sem viverra aliquet eget sit amet tellus.', 'Velit laoreet id donec ultrices.', 'Iaculis urna id volutpat lacus laoreet non curabitur gravida arcu.', 'Sit amet nisl suscipit adipiscing bibendum.', 'Aliquam faucibus purus in massa tempor nec feugiat.', 'Quam nulla porttitor massa id neque aliquam.', 'Blandit massa enim nec dui.', 'Amet volutpat consequat mauris nunc congue.', 'Vel turpis nunc eget lorem dolor sed viverra ipsum.', 'Adipiscing elit pellentesque habitant morbi.', 'Eget aliquet nibh praesent tristique magna sit.', 'In cursus turpis massa tincidunt dui ut ornare.', 'Convallis aenean et tortor at risus viverra adipiscing at in.', 'Pellentesque elit ullamcorper dignissim cras tincidunt lobortis feugiat.', 'Fermentum dui faucibus in ornare quam viverra orci sagittis.', 'Facilisi etiam dignissim diam quis enim lobortis scelerisque fermentum dui.', 'Elit ut aliquam purus sit amet luctus venenatis lectus magna.', 'Purus faucibus ornare suspendisse sed nisi lacus.', 'Viverra nam libero justo laoreet.', 'Ornare quam viverra orci sagittis eu volutpat odio facilisis mauris.', 'Diam quam nulla porttitor massa.', 'Cursus euismod quis viverra nibh cras pulvinar.', 'Purus viverra accumsan in nisl nisi.', 'Amet consectetur adipiscing elit ut aliquam.', 'Sed odio morbi quis commodo.', 'Eu mi bibendum neque egestas.', 'In egestas erat imperdiet sed euismod nisi.', 'Semper viverra nam libero justo laoreet sit amet.', 'Tellus at urna condimentum mattis.', 'In metus vulputate eu scelerisque felis imperdiet proin.', 'Massa ultricies mi quis hendrerit dolor magna eget.', 'Lectus mauris ultrices eros in.', 'Mi sit amet mauris commodo quis imperdiet massa.', 'Fusce ut placerat orci nulla pellentesque.', 'Non diam phasellus vestibulum lorem sed risus ultricies.', 'Risus feugiat in ante metus dictum at tempor commodo.', 'Suspendisse ultrices gravida dictum fusce ut.', 'Neque convallis a cras semper auctor neque vitae tempus quam.', 'Pharetra massa massa ultricies mi.', 'Arcu non odio euismod lacinia at quis risus.', 'Eget velit aliquet sagittis id consectetur purus ut.', 'Mattis molestie a iaculis at erat pellentesque adipiscing commodo elit.', 'Lorem ipsum dolor sit amet.', 'Enim facilisis gravida neque convallis a cras.', 'Amet mattis vulputate enim nulla aliquet porttitor lacus luctus accumsan.', 'Eros donec ac odio tempor.', 'Tortor aliquam nulla facilisi cras fermentum odio.', 'Erat pellentesque adipiscing commodo elit at imperdiet dui.', 'Eget duis at tellus at urna.', 'Gravida arcu ac tortor dignissim convallis aenean et.', 'Vitae justo eget magna fermentum iaculis eu non diam phasellus.', 'Mauris augue neque gravida in fermentum et sollicitudin.', 'Auctor neque vitae tempus quam pellentesque.', 'Lacus vestibulum sed arcu non odio euismod lacinia at quis.', 'Viverra vitae congue eu consequat.', 'Elementum tempus egestas sed sed.', 'Fringilla urna porttitor rhoncus dolor purus non enim.', 'Morbi tristique senectus et netus.', 'Ut sem viverra aliquet eget.', 'Ac placerat vestibulum lectus mauris ultrices eros in cursus turpis.', 'Turpis massa sed elementum tempus egestas.', 'Aliquam malesuada bibendum arcu vitae elementum curabitur vitae nunc sed.', 'Cursus euismod quis viverra nibh.', 'Ut sem nulla pharetra diam sit amet.', 'Quis auctor elit sed vulputate.', 'Gravida dictum fusce ut placerat orci nulla.', 'Sem integer vitae justo eget.', 'Consectetur adipiscing elit pellentesque habitant morbi tristique senectus et.', 'Ultricies leo integer malesuada nunc vel risus commodo viverra.', 'Lobortis scelerisque fermentum dui faucibus in ornare quam viverra.', 'Suspendisse sed nisi lacus sed viverra tellus.', 'Condimentum id venenatis a condimentum vitae sapien pellentesque.', 'Dictum varius duis at consectetur.', 'Ultrices tincidunt arcu non sodales neque sodales.', 'Sed lectus vestibulum mattis ullamcorper velit sed ullamcorper morbi.', 'Posuere ac ut consequat semper viverra nam.', 'Aliquet nec ullamcorper sit amet risus nullam eget.', 'Semper eget duis at tellus at urna condimentum.', 'In metus vulputate eu scelerisque felis imperdiet proin.', 'Fermentum dui faucibus in ornare quam viverra.', 'Habitant morbi tristique senectus et netus et.', 'Phasellus egestas tellus rutrum tellus pellentesque eu tincidunt tortor.', 'Elementum sagittis vitae et leo duis.', 'Tempor orci eu lobortis elementum.', 'Nulla pellentesque dignissim enim sit.', 'Auctor eu augue ut lectus arcu bibendum at varius.', 'Donec et odio pellentesque diam volutpat commodo sed egestas.', 'Gravida cum sociis natoque penatibus et magnis dis parturient.', 'Scelerisque in dictum non consectetur a erat nam at.', 'Mauris pellentesque pulvinar pellentesque habitant morbi tristique senectus et.', 'Aliquet nibh praesent tristique magna sit amet purus.', 'Tempus imperdiet nulla malesuada pellentesque.', 'Posuere lorem ipsum dolor sit.', 'Diam sit amet nisl suscipit adipiscing bibendum est ultricies integer.', 'Suscipit adipiscing bibendum est ultricies integer quis auctor elit sed.', 'Nunc consequat interdum varius sit amet mattis.', 'Non enim praesent elementum facilisis.', 'Luctus venenatis lectus magna fringilla urna porttitor.', 'Netus et malesuada fames ac turpis egestas.', 'Pharetra pharetra massa massa ultricies mi quis hendrerit.', 'Dui vivamus arcu felis bibendum ut tristique et egestas.', 'Nam at lectus urna duis convallis convallis tellus id.', 'Malesuada nunc vel risus commodo viverra maecenas accumsan lacus vel.', 'Odio aenean sed adipiscing diam donec adipiscing tristique risus nec.', 'Mauris vitae ultricies leo integer.', 'Tincidunt arcu non sodales neque sodales ut etiam sit amet.', 'Quam id leo in vitae turpis massa sed elementum tempus.', 'Ultrices dui sapien eget mi proin.', 'Dui sapien eget mi proin sed libero.', 'Quam lacus suspendisse faucibus interdum posuere lorem ipsum dolor.', 'Purus semper eget duis at.', 'Aliquet nec ullamcorper sit amet risus nullam eget felis eget.', 'Egestas erat imperdiet sed euismod nisi porta.', 'Tincidunt lobortis feugiat vivamus at.', 'A pellentesque sit amet porttitor eget dolor morbi non.', 'Egestas tellus rutrum tellus pellentesque eu tincidunt.', 'Vehicula ipsum a arcu cursus vitae congue mauris rhoncus aenean.', 'Blandit aliquam etiam erat velit scelerisque.', 'Ut pharetra sit amet aliquam id diam maecenas.', 'Ultrices eros in cursus turpis massa tincidunt dui ut ornare.', 'Libero justo laoreet sit amet cursus sit amet dictum sit.', 'Nisl rhoncus mattis rhoncus urna neque viverra justo nec ultrices.', 'Eu turpis egestas pretium aenean pharetra.', 'Mattis aliquam faucibus purus in.', 'Mi bibendum neque egestas congue.', 'Ipsum faucibus vitae aliquet nec.', 'Consequat nisl vel pretium lectus quam id leo.', 'Dolor sit amet consectetur adipiscing elit.', 'Feugiat vivamus at augue eget arcu dictum.', 'Mattis molestie a iaculis at.', 'In eu mi bibendum neque egestas congue quisque egestas diam.', 'Magna fermentum iaculis eu non diam phasellus vestibulum lorem sed.', 'Ultrices gravida dictum fusce ut placerat orci nulla pellentesque.', 'Viverra tellus in hac habitasse platea.', 'Maecenas sed enim ut sem viverra aliquet.', 'Pellentesque adipiscing commodo elit at imperdiet dui accumsan sit amet.', 'Sociis natoque penatibus et magnis dis parturient montes.', 'Vulputate odio ut enim blandit volutpat maecenas.', 'Elementum pulvinar etiam non quam lacus suspendisse faucibus.', 'Commodo odio aenean sed adipiscing.', 'Risus feugiat in ante metus dictum at tempor commodo ullamcorper.', 'Est ante in nibh mauris.', 'Platea dictumst quisque sagittis purus sit amet volutpat consequat.', 'Sagittis purus sit amet volutpat consequat mauris nunc congue nisi.', 'At in tellus integer feugiat scelerisque varius.', 'Tincidunt id aliquet risus feugiat.', 'Metus dictum at tempor commodo ullamcorper a lacus vestibulum sed.', 'Nunc congue nisi vitae suscipit tellus.', 'Tristique senectus et netus et malesuada fames ac.', 'Eget mauris pharetra et ultrices neque ornare.', 'Fusce id velit ut tortor.', 'Viverra vitae congue eu consequat ac felis donec et odio.', 'Netus et malesuada fames ac.', 'Convallis aenean et tortor at risus.', 'Ornare massa eget egestas purus viverra accumsan in nisl nisi.', 'Nulla posuere sollicitudin aliquam ultrices.', 'Lorem dolor sed viverra ipsum.', 'Aliquam sem fringilla ut morbi tincidunt augue.', 'Scelerisque felis imperdiet proin fermentum.', 'Proin fermentum leo vel orci porta non pulvinar.', 'Nec feugiat in fermentum posuere urna nec.', 'Habitant morbi tristique senectus et netus et malesuada fames.', 'Faucibus pulvinar elementum integer enim neque.', 'Metus vulputate eu scelerisque felis imperdiet.', 'Lectus magna fringilla urna porttitor rhoncus.', 'Id aliquet lectus proin nibh nisl condimentum id venenatis.', 'Egestas maecenas pharetra convallis posuere morbi leo urna molestie at.', 'Suspendisse in est ante in nibh mauris cursus mattis.', 'Sed egestas egestas fringilla phasellus faucibus scelerisque.', 'Sed vulputate mi sit amet mauris commodo quis.', 'Ipsum dolor sit amet consectetur.', 'Nibh praesent tristique magna sit.', 'Massa sed elementum tempus egestas sed sed risus pretium quam.', 'Pulvinar elementum integer enim neque volutpat.', 'Eu feugiat pretium nibh ipsum consequat.', 'Eu nisl nunc mi ipsum faucibus vitae.', 'Nisi vitae suscipit tellus mauris a diam maecenas sed.', 'Mattis nunc sed blandit libero volutpat sed.', 'Ultricies mi eget mauris pharetra et ultrices neque.', 'Ultrices mi tempus imperdiet nulla malesuada.', 'Morbi quis commodo odio aenean sed adipiscing diam.', 'Et pharetra pharetra massa massa ultricies mi quis.', 'Dis parturient montes nascetur ridiculus mus mauris vitae ultricies.', 'Sit amet dictum sit amet justo donec.', 'Pellentesque habitant morbi tristique senectus et.', 'Egestas erat imperdiet sed euismod nisi porta lorem mollis aliquam.', 'Risus at ultrices mi tempus imperdiet nulla.', 'Ac auctor augue mauris augue neque gravida in.', 'Porttitor eget dolor morbi non arcu.', 'Augue eget arcu dictum varius duis at consectetur.', 'Amet consectetur adipiscing elit ut aliquam purus.', 'Suspendisse interdum consectetur libero id faucibus.', 'Quam elementum pulvinar etiam non quam lacus suspendisse faucibus.', 'Aenean et tortor at risus viverra adipiscing at in tellus.', 'Dolor sed viverra ipsum nunc aliquet.', 'Enim tortor at auctor urna.', 'Malesuada nunc vel risus commodo viverra.', 'Sit amet volutpat consequat mauris nunc.', 'Vulputate dignissim suspendisse in est ante in nibh mauris.', 'Volutpat consequat mauris nunc congue.', 'At tempor commodo ullamcorper a lacus.', 'At imperdiet dui accumsan sit amet nulla.', 'Urna duis convallis convallis tellus id.', 'Quis enim lobortis scelerisque fermentum dui faucibus.', 'Lobortis scelerisque fermentum dui faucibus in ornare quam viverra.', 'Velit sed ullamcorper morbi tincidunt ornare massa.', 'Faucibus vitae aliquet nec ullamcorper sit amet risus nullam.', 'Arcu vitae elementum curabitur vitae nunc sed velit.', 'Adipiscing commodo elit at imperdiet dui accumsan sit.', 'Gravida rutrum quisque non tellus orci ac.', 'Nibh mauris cursus mattis molestie a iaculis at.', 'Elit pellentesque habitant morbi tristique senectus et netus et.', 'Non tellus orci ac auctor augue mauris augue neque.', 'Lacus vestibulum sed arcu non odio euismod lacinia at.', 'Ac turpis egestas integer eget aliquet nibh praesent.', 'Leo a diam sollicitudin tempor id eu.', 'Leo in vitae turpis massa sed elementum tempus egestas sed.', 'Mattis molestie a iaculis at erat pellentesque.', 'Urna porttitor rhoncus dolor purus.', 'Elementum integer enim neque volutpat ac tincidunt vitae.', 'Integer malesuada nunc vel risus commodo.', 'Eu tincidunt tortor aliquam nulla facilisi cras fermentum.', 'Mauris commodo quis imperdiet massa tincidunt.', 'Commodo quis imperdiet massa tincidunt nunc pulvinar sapien et.', 'Auctor augue mauris augue neque.', 'Justo donec enim diam vulputate.', 'Nulla facilisi morbi tempus iaculis.', 'Sit amet commodo nulla facilisi nullam vehicula ipsum a arcu.', 'Sit amet nisl suscipit adipiscing bibendum est.', 'Interdum varius sit amet mattis vulputate enim.', 'Donec adipiscing tristique risus nec feugiat in.', 'Integer malesuada nunc vel risus commodo viverra maecenas.', 'Natoque penatibus et magnis dis parturient montes nascetur ridiculus.', 'Gravida quis blandit turpis cursus in hac.', 'Sed faucibus turpis in eu mi bibendum neque.', 'At tempor commodo ullamcorper a lacus vestibulum sed.', 'Amet commodo nulla facilisi nullam.', 'A erat nam at lectus urna duis convallis convallis.', 'Tortor pretium viverra suspendisse potenti nullam ac.', 'Volutpat commodo sed egestas egestas fringilla phasellus.', 'Elit eget gravida cum sociis natoque penatibus.', 'Egestas sed sed risus pretium quam vulputate.', 'Ultrices dui sapien eget mi proin.', 'Vivamus arcu felis bibendum ut tristique et egestas quis.', 'Vulputate odio ut enim blandit volutpat.', 'Felis eget velit aliquet sagittis.', 'Vestibulum rhoncus est pellentesque elit.', 'Faucibus in ornare quam viverra orci sagittis eu.', 'Nulla pellentesque dignissim enim sit amet venenatis urna.', 'Turpis cursus in hac habitasse platea dictumst.', 'Proin libero nunc consequat interdum varius sit amet mattis vulputate.', 'Sed risus ultricies tristique nulla aliquet enim tortor.', 'Turpis nunc eget lorem dolor sed viverra ipsum nunc aliquet.', 'Odio aenean sed adipiscing diam donec adipiscing tristique risus.', 'Volutpat sed cras ornare arcu dui.', 'Elementum sagittis vitae et leo.', 'A arcu cursus vitae congue mauris rhoncus.', 'Nec ultrices dui sapien eget mi proin sed libero.', 'Eget magna fermentum iaculis eu.', 'Fringilla phasellus faucibus scelerisque eleifend donec pretium vulputate sapien.', 'Ultricies mi eget mauris pharetra et ultrices.', 'Elit sed vulputate mi sit amet.', 'Platea dictumst quisque sagittis purus sit.', 'Amet venenatis urna cursus eget.', 'Pharetra vel turpis nunc eget lorem dolor sed viverra ipsum.', 'Feugiat nibh sed pulvinar proin gravida hendrerit.', 'Sit amet tellus cras adipiscing enim eu turpis.', 'In hac habitasse platea dictumst quisque.', 'Elit scelerisque mauris pellentesque pulvinar pellentesque habitant morbi.', 'Eleifend mi in nulla posuere sollicitudin.', 'Diam sollicitudin tempor id eu nisl nunc mi ipsum faucibus.', 'Amet aliquam id diam maecenas.', 'Volutpat commodo sed egestas egestas fringilla.', 'A diam maecenas sed enim ut sem viverra aliquet.', 'Aliquam sem et tortor consequat id porta nibh venenatis cras.', 'Ac turpis egestas maecenas pharetra.', 'Adipiscing elit ut aliquam purus sit.', 'Iaculis urna id volutpat lacus laoreet non curabitur gravida.', 'Egestas fringilla phasellus faucibus scelerisque eleifend donec pretium.', 'Adipiscing tristique risus nec feugiat in fermentum posuere urna nec.', 'Mauris commodo quis imperdiet massa tincidunt nunc pulvinar sapien.', 'Egestas sed sed risus pretium quam vulputate.', 'Etiam erat velit scelerisque in dictum.', 'Elit pellentesque habitant morbi tristique senectus et netus.', 'Cursus mattis molestie a iaculis.', 'Eu feugiat pretium nibh ipsum consequat nisl.', 'Sed vulputate odio ut enim blandit volutpat.', 'Euismod nisi porta lorem mollis aliquam.', 'Mauris cursus mattis molestie a iaculis.', 'In aliquam sem fringilla ut morbi tincidunt augue.', 'Non diam phasellus vestibulum lorem sed risus ultricies tristique.', 'Tempus iaculis urna id volutpat lacus laoreet non curabitur gravida.', 'Praesent elementum facilisis leo vel fringilla est.', 'Netus et malesuada fames ac turpis.', 'Enim nunc faucibus a pellentesque sit amet porttitor.', 'Tortor consequat id porta nibh venenatis cras sed.', 'Ut enim blandit volutpat maecenas volutpat blandit aliquam etiam.', 'At quis risus sed vulputate odio ut enim blandit.', 'Pharetra pharetra massa massa ultricies mi quis hendrerit dolor.', 'Dictumst vestibulum rhoncus est pellentesque.', 'Fames ac turpis egestas integer eget aliquet nibh praesent tristique.', 'Dolor sit amet consectetur adipiscing elit duis tristique.', 'Sed libero enim sed faucibus turpis in eu mi.', 'Ut tortor pretium viverra suspendisse potenti nullam ac tortor vitae.', 'Non nisi est sit amet facilisis.', 'Est ullamcorper eget nulla facilisi etiam.', 'Erat imperdiet sed euismod nisi porta lorem mollis.', 'At erat pellentesque adipiscing commodo elit at imperdiet dui accumsan.', 'Orci nulla pellentesque dignissim enim sit amet.', 'Dignissim convallis aenean et tortor at risus.', 'Sed adipiscing diam donec adipiscing tristique risus nec feugiat.', 'Lorem ipsum dolor sit amet consectetur adipiscing elit pellentesque.', 'In aliquam sem fringilla ut morbi tincidunt augue.', 'Ut venenatis tellus in metus vulputate eu scelerisque.', 'Morbi tincidunt augue interdum velit.', 'Tristique magna sit amet purus gravida quis.', 'Quis enim lobortis scelerisque fermentum dui.', 'Eu mi bibendum neque egestas congue.', 'Curabitur gravida arcu ac tortor dignissim convallis aenean.', 'Amet mattis vulputate enim nulla.', 'Bibendum est ultricies integer quis auctor elit.', 'Ridiculus mus mauris vitae ultricies.', 'Id ornare arcu odio ut.', 'Pulvinar neque laoreet suspendisse interdum.', 'Porttitor leo a diam sollicitudin.', 'Vel facilisis volutpat est velit egestas.', 'Diam vel quam elementum pulvinar etiam non quam lacus.', 'Pretium vulputate sapien nec sagittis aliquam malesuada bibendum.', 'Sed turpis tincidunt id aliquet risus feugiat in ante.', 'Netus et malesuada fames ac.', 'Imperdiet dui accumsan sit amet nulla facilisi.', 'Imperdiet nulla malesuada pellentesque elit eget gravida.', 'Amet volutpat consequat mauris nunc congue nisi vitae suscipit.', 'Aliquet risus feugiat in ante.', 'Diam vel quam elementum pulvinar etiam non quam.', 'Volutpat ac tincidunt vitae semper quis lectus nulla at volutpat.', 'Scelerisque fermentum dui faucibus in ornare quam.', 'Lacinia quis vel eros donec ac odio tempor orci.', 'Odio tempor orci dapibus ultrices in iaculis nunc sed augue.', 'Montes nascetur ridiculus mus mauris vitae ultricies.', 'Proin sagittis nisl rhoncus mattis rhoncus.', 'Tortor pretium viverra suspendisse potenti nullam.', 'Nisi porta lorem mollis aliquam ut porttitor leo a diam.', 'Tortor at auctor urna nunc id.', 'Volutpat blandit aliquam etiam erat velit scelerisque in dictum.', 'Augue lacus viverra vitae congue eu consequat ac felis.', 'Viverra adipiscing at in tellus integer feugiat scelerisque varius.', 'Dui faucibus in ornare quam viverra orci.', 'Aenean et tortor at risus viverra adipiscing.', 'Suscipit adipiscing bibendum est ultricies integer quis auctor elit sed.', 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.', 'Mattis pellentesque id nibh tortor.', 'Venenatis tellus in metus vulputate eu scelerisque felis.', 'In dictum non consectetur a.', 'Morbi quis commodo odio aenean sed adipiscing.', 'Amet volutpat consequat mauris nunc congue nisi vitae.', 'Mauris in aliquam sem fringilla.', 'Quis hendrerit dolor magna eget est.', 'Eros in cursus turpis massa tincidunt dui ut ornare.', 'Sed risus ultricies tristique nulla aliquet enim tortor.', 'Nulla pellentesque dignissim enim sit.', 'Eget velit aliquet sagittis id.', 'Gravida cum sociis natoque penatibus et magnis dis parturient montes.', 'At elementum eu facilisis sed odio morbi.', 'Convallis aenean et tortor at risus viverra adipiscing at in.', 'Mi proin sed libero enim.', 'Amet est placerat in egestas erat.', 'Volutpat sed cras ornare arcu.', 'Platea dictumst quisque sagittis purus sit.', 'Quis risus sed vulputate odio ut enim blandit volutpat.', 'Quis hendrerit dolor magna eget.', 'Morbi tristique senectus et netus et malesuada fames ac turpis.', 'Netus et malesuada fames ac turpis egestas integer eget aliquet.', 'Et pharetra pharetra massa massa ultricies mi.', 'Et egestas quis ipsum suspendisse ultrices gravida dictum.', 'Sapien et ligula ullamcorper malesuada proin libero nunc consequat interdum.', 'Sit amet consectetur adipiscing elit duis tristique sollicitudin nibh.', 'Morbi leo urna molestie at elementum.', 'Gravida cum sociis natoque penatibus et magnis.', 'Mauris commodo quis imperdiet massa tincidunt.', 'Nunc consequat interdum varius sit amet mattis.', 'Luctus venenatis lectus magna fringilla urna porttitor rhoncus dolor purus.', 'Etiam non quam lacus suspendisse.', 'Tincidunt praesent semper feugiat nibh.', 'Tristique senectus et netus et.', 'Velit dignissim sodales ut eu sem integer vitae.', 'Ac auctor augue mauris augue.', 'Dolor sit amet consectetur adipiscing elit ut.', 'Et malesuada fames ac turpis egestas sed tempus urna et.', 'Sit amet est placerat in.', 'Odio eu feugiat pretium nibh ipsum consequat nisl vel pretium.', 'Commodo elit at imperdiet dui accumsan sit.', 'In eu mi bibendum neque egestas congue quisque.', 'Sed viverra ipsum nunc aliquet.', 'Tristique risus nec feugiat in.', 'Turpis in eu mi bibendum neque egestas congue.', 'Faucibus ornare suspendisse sed nisi lacus sed viverra tellus in.', 'Feugiat pretium nibh ipsum consequat nisl vel pretium lectus.', 'Vivamus at augue eget arcu dictum varius duis at.', 'Aliquet sagittis id consectetur purus ut faucibus pulvinar elementum integer.', 'Morbi quis commodo odio aenean sed.', 'Duis ut diam quam nulla porttitor massa id neque.', 'Cursus metus aliquam eleifend mi in.', 'Est lorem ipsum dolor sit amet consectetur.', 'Pharetra convallis posuere morbi leo urna molestie at elementum.', 'Amet est placerat in egestas erat.', 'Duis convallis convallis tellus id.', 'In dictum non consectetur a erat nam at lectus.', 'Sit amet volutpat consequat mauris nunc congue nisi vitae suscipit.', 'Quis lectus nulla at volutpat diam ut venenatis tellus in.', 'Quisque id diam vel quam elementum pulvinar.', 'Malesuada pellentesque elit eget gravida cum sociis natoque penatibus.', 'Vel fringilla est ullamcorper eget nulla facilisi etiam dignissim.', 'Lectus sit amet est placerat in.', 'Purus in massa tempor nec feugiat nisl.', 'Eros in cursus turpis massa tincidunt dui ut ornare lectus.', 'At urna condimentum mattis pellentesque id.', 'Aliquet lectus proin nibh nisl condimentum id venenatis a condimentum.', 'Augue ut lectus arcu bibendum at varius vel pharetra.', 'Nec feugiat in fermentum posuere urna.', 'Sed arcu non odio euismod lacinia at quis risus sed.', 'Tristique sollicitudin nibh sit amet.', 'Proin libero nunc consequat interdum varius sit amet mattis.', 'Magna fringilla urna porttitor rhoncus.', 'Tristique nulla aliquet enim tortor at auctor urna nunc id.', 'Etiam dignissim diam quis enim lobortis scelerisque fermentum dui.', 'Vitae sapien pellentesque habitant morbi tristique senectus et netus.', 'Ultricies mi quis hendrerit dolor magna eget est lorem ipsum.', 'Fringilla est ullamcorper eget nulla facilisi etiam.', 'Donec pretium vulputate sapien nec sagittis aliquam malesuada bibendum.', 'Quis blandit turpis cursus in hac habitasse.', 'Felis eget nunc lobortis mattis aliquam faucibus purus in massa.', 'Bibendum ut tristique et egestas quis ipsum.', 'Vel turpis nunc eget lorem dolor sed viverra ipsum nunc.', 'Leo in vitae turpis massa sed elementum tempus egestas.', 'Molestie ac feugiat sed lectus vestibulum mattis ullamcorper velit.', 'Nibh tellus molestie nunc non blandit massa enim.', 'Nisi lacus sed viverra tellus in hac habitasse platea.', 'Egestas sed tempus urna et pharetra pharetra.', 'Non quam lacus suspendisse faucibus interdum posuere lorem ipsum.', 'Augue lacus viverra vitae congue eu consequat ac.', 'In eu mi bibendum neque.', 'Id neque aliquam vestibulum morbi blandit cursus risus at ultrices.', 'Libero enim sed faucibus turpis in eu.', 'Enim diam vulputate ut pharetra.', 'Purus sit amet luctus venenatis.', 'At tellus at urna condimentum mattis pellentesque.', 'Sapien eget mi proin sed.', 'Augue ut lectus arcu bibendum at varius vel.', 'Amet est placerat in egestas erat.', 'Lacus sed viverra tellus in hac habitasse platea.', 'Mattis nunc sed blandit libero.', 'Viverra suspendisse potenti nullam ac tortor vitae purus faucibus.', 'Non diam phasellus vestibulum lorem sed risus ultricies tristique nulla.', 'Non consectetur a erat nam.', 'Magna sit amet purus gravida quis.', 'Tempor id eu nisl nunc mi.', 'Cras adipiscing enim eu turpis egestas pretium.', 'Rhoncus aenean vel elit scelerisque mauris.', 'Odio eu feugiat pretium nibh.', 'Pellentesque id nibh tortor id aliquet.', 'Nisi quis eleifend quam adipiscing vitae proin sagittis nisl rhoncus.', 'Arcu cursus vitae congue mauris rhoncus aenean vel.', 'Praesent elementum facilisis leo vel fringilla est ullamcorper eget nulla.', 'Amet cursus sit amet dictum sit amet justo donec enim.', 'Condimentum mattis pellentesque id nibh.', 'Posuere sollicitudin aliquam ultrices sagittis.', 'Orci porta non pulvinar neque laoreet suspendisse interdum consectetur.', 'Id aliquet risus feugiat in.', 'Dignissim convallis aenean et tortor at risus viverra.', 'Tincidunt dui ut ornare lectus.', 'Dictum non consectetur a erat.', 'Ligula ullamcorper malesuada proin libero.', 'Maecenas volutpat blandit aliquam etiam erat velit.', 'Sed viverra tellus in hac.', 'Volutpat maecenas volutpat blandit aliquam etiam erat velit.', 'Accumsan in nisl nisi scelerisque eu ultrices vitae auctor eu.', 'Gravida quis blandit turpis cursus in hac habitasse platea.', 'Tellus mauris a diam maecenas.', 'Ac felis donec et odio pellentesque diam volutpat.', 'Nisi est sit amet facilisis magna etiam tempor.', 'Adipiscing at in tellus integer feugiat scelerisque varius.', 'Mattis molestie a iaculis at.', 'Sit amet justo donec enim diam vulputate ut.', 'Ante metus dictum at tempor commodo ullamcorper a lacus.', 'Pellentesque adipiscing commodo elit at imperdiet dui.', 'Amet mattis vulputate enim nulla aliquet porttitor lacus luctus.', 'Viverra aliquet eget sit amet tellus.', 'Aliquam purus sit amet luctus venenatis.', 'Imperdiet massa tincidunt nunc pulvinar sapien et ligula ullamcorper.', 'Urna cursus eget nunc scelerisque viverra mauris.', 'Eget est lorem ipsum dolor sit amet.', 'In ante metus dictum at tempor.', 'Parturient montes nascetur ridiculus mus mauris vitae ultricies.', 'Nec dui nunc mattis enim ut tellus elementum.', 'Eget mauris pharetra et ultrices neque ornare aenean euismod elementum.', 'Id aliquet lectus proin nibh nisl condimentum id.', 'Orci porta non pulvinar neque laoreet suspendisse interdum consectetur.', 'Amet mauris commodo quis imperdiet massa tincidunt nunc.', 'Adipiscing elit duis tristique sollicitudin.', 'Viverra adipiscing at in tellus integer feugiat scelerisque varius.', 'Et tortor at risus viverra adipiscing at in tellus.', 'Magna etiam tempor orci eu lobortis elementum nibh.', 'Leo integer malesuada nunc vel risus commodo.', 'Quis eleifend quam adipiscing vitae proin.', 'Quis viverra nibh cras pulvinar mattis.', 'Nullam eget felis eget nunc lobortis mattis aliquam faucibus.', 'Nibh sed pulvinar proin gravida hendrerit lectus.', 'Ut consequat semper viverra nam.', 'Augue ut lectus arcu bibendum at varius vel pharetra vel.', 'Sit amet porttitor eget dolor morbi non.', 'Sollicitudin nibh sit amet commodo.', 'Morbi tristique senectus et netus et malesuada fames ac.', 'Dolor sit amet consectetur adipiscing elit ut aliquam.', 'Venenatis lectus magna fringilla urna porttitor rhoncus dolor.', 'Eu volutpat odio facilisis mauris sit.', 'In hendrerit gravida rutrum quisque non.', 'Curabitur gravida arcu ac tortor dignissim convallis aenean et.', 'Sit amet consectetur adipiscing elit ut aliquam purus sit.', 'Facilisi etiam dignissim diam quis.', 'Risus nec feugiat in fermentum posuere.', 'A arcu cursus vitae congue mauris.', 'Iaculis nunc sed augue lacus viverra vitae.', 'Vulputate dignissim suspendisse in est ante in.', 'Varius morbi enim nunc faucibus a pellentesque sit amet.', 'Lectus nulla at volutpat diam ut venenatis tellus in metus.', 'Cursus turpis massa tincidunt dui ut ornare.', 'Bibendum at varius vel pharetra vel turpis.', 'In hac habitasse platea dictumst vestibulum rhoncus est.', 'Arcu odio ut sem nulla pharetra diam sit amet nisl.', 'Consequat interdum varius sit amet mattis vulputate enim nulla.', 'Potenti nullam ac tortor vitae.', 'At auctor urna nunc id.', 'Pellentesque pulvinar pellentesque habitant morbi tristique senectus et netus.', 'At volutpat diam ut venenatis tellus in metus vulputate eu.', 'Tellus id interdum velit laoreet id donec.', 'Urna cursus eget nunc scelerisque.', 'Risus quis varius quam quisque id diam.', 'Lectus urna duis convallis convallis tellus.', 'Sollicitudin aliquam ultrices sagittis orci a.', 'Egestas tellus rutrum tellus pellentesque eu tincidunt tortor aliquam.', 'Hendrerit gravida rutrum quisque non tellus orci ac.', 'Lacinia at quis risus sed vulputate odio.', 'Lobortis feugiat vivamus at augue eget.', 'Lacus vestibulum sed arcu non odio euismod lacinia at quis.', 'Amet purus gravida quis blandit turpis cursus in.', 'Eleifend quam adipiscing vitae proin sagittis nisl rhoncus.', 'Sed nisi lacus sed viverra tellus.', 'Vitae semper quis lectus nulla at volutpat diam ut.', 'Egestas erat imperdiet sed euismod nisi.', 'Ut etiam sit amet nisl purus.', 'Id consectetur purus ut faucibus pulvinar.', 'Amet mattis vulputate enim nulla aliquet porttitor lacus.', 'Nibh mauris cursus mattis molestie a iaculis at erat.', 'Lacus sed turpis tincidunt id aliquet risus feugiat in.', 'Dui id ornare arcu odio ut sem nulla.', 'Feugiat nibh sed pulvinar proin gravida hendrerit.', 'Sit amet consectetur adipiscing elit.', 'Nulla at volutpat diam ut venenatis tellus.', 'Eget mauris pharetra et ultrices neque ornare aenean euismod elementum.', 'Amet purus gravida quis blandit turpis cursus.', 'Tortor consequat id porta nibh venenatis cras sed.', 'Feugiat in ante metus dictum at tempor commodo.', 'Eget duis at tellus at urna condimentum mattis.', 'Commodo nulla facilisi nullam vehicula.', 'Consectetur lorem donec massa sapien faucibus et.', 'Tellus cras adipiscing enim eu turpis.', 'Elementum eu facilisis sed odio morbi quis.', 'Id ornare arcu odio ut sem nulla pharetra.', 'Sem integer vitae justo eget magna fermentum iaculis.', 'Nisl condimentum id venenatis a.', 'Dui ut ornare lectus sit.', 'Convallis convallis tellus id interdum.', 'Lorem dolor sed viverra ipsum nunc aliquet bibendum enim.', 'In iaculis nunc sed augue lacus viverra vitae congue.', 'Orci nulla pellentesque dignissim enim sit amet venenatis urna cursus.', 'Ac feugiat sed lectus vestibulum mattis.', 'Dignissim convallis aenean et tortor at.', 'Senectus et netus et malesuada.', 'Adipiscing tristique risus nec feugiat in fermentum posuere urna.', 'Ut venenatis tellus in metus vulputate.', 'Amet volutpat consequat mauris nunc congue nisi vitae suscipit.', 'Accumsan sit amet nulla facilisi morbi.', 'Quis enim lobortis scelerisque fermentum dui faucibus in ornare quam.', 'Tristique et egestas quis ipsum suspendisse.', 'Feugiat in fermentum posuere urna nec tincidunt praesent semper feugiat.', 'Et tortor consequat id porta nibh venenatis cras.', 'Dignissim convallis aenean et tortor.', 'Fames ac turpis egestas sed.', 'Magnis dis parturient montes nascetur ridiculus.', 'Non blandit massa enim nec dui nunc mattis enim ut.', 'Sit amet consectetur adipiscing elit ut aliquam purus.', 'Elit sed vulputate mi sit amet mauris commodo.', 'Et molestie ac feugiat sed lectus vestibulum mattis ullamcorper velit.', 'Ultricies leo integer malesuada nunc vel risus commodo viverra.', 'Eros in cursus turpis massa tincidunt dui ut.', 'Tempus imperdiet nulla malesuada pellentesque elit eget gravida cum sociis.', 'Pharetra convallis posuere morbi leo urna.', 'Lectus proin nibh nisl condimentum id venenatis a condimentum vitae.', 'Lectus proin nibh nisl condimentum id venenatis.', 'Sit amet luctus venenatis lectus magna fringilla.', 'Tristique nulla aliquet enim tortor at auctor urna nunc.', 'Suspendisse sed nisi lacus sed viverra.', 'Non quam lacus suspendisse faucibus interdum posuere.', 'Eu augue ut lectus arcu bibendum.', 'Amet cursus sit amet dictum sit.', 'Felis eget nunc lobortis mattis aliquam faucibus.', 'Tristique nulla aliquet enim tortor at auctor urna nunc id.', 'Neque aliquam vestibulum morbi blandit cursus risus at ultrices mi.', 'Velit euismod in pellentesque massa placerat duis ultricies.', 'Imperdiet sed euismod nisi porta lorem mollis aliquam.', 'Aliquam purus sit amet luctus venenatis lectus magna.', 'Nibh praesent tristique magna sit amet.', 'Nisl purus in mollis nunc sed id semper risus in.', 'Non quam lacus suspendisse faucibus interdum.', 'Suspendisse ultrices gravida dictum fusce ut placerat orci.', 'Nunc congue nisi vitae suscipit tellus mauris a diam.', 'Semper eget duis at tellus at urna condimentum mattis.', 'Felis imperdiet proin fermentum leo vel orci porta non pulvinar.', 'Etiam dignissim diam quis enim lobortis.', 'Diam in arcu cursus euismod quis.', 'Suscipit tellus mauris a diam maecenas.', 'Porta nibh venenatis cras sed felis eget velit aliquet.', 'In ante metus dictum at.', 'Elit sed vulputate mi sit amet mauris commodo quis.', 'Arcu ac tortor dignissim convallis aenean et tortor at risus.', 'Turpis nunc eget lorem dolor sed.', 'Luctus accumsan tortor posuere ac ut consequat semper viverra nam.', 'Ipsum dolor sit amet consectetur adipiscing elit.', 'Justo nec ultrices dui sapien eget mi proin sed.', 'Odio facilisis mauris sit amet massa vitae tortor condimentum.', 'Magnis dis parturient montes nascetur ridiculus mus mauris vitae ultricies.', 'Vitae tempus quam pellentesque nec.', 'Cras tincidunt lobortis feugiat vivamus at augue eget arcu.', 'Sit amet purus gravida quis blandit.', 'Augue mauris augue neque gravida in fermentum et sollicitudin ac.', 'Viverra suspendisse potenti nullam ac tortor vitae purus faucibus.', 'Blandit turpis cursus in hac habitasse platea dictumst.', 'Ultricies integer quis auctor elit sed vulputate.', 'Gravida arcu ac tortor dignissim convallis aenean et tortor at.', 'Sodales ut eu sem integer vitae justo eget.', 'Nisl suscipit adipiscing bibendum est ultricies integer quis auctor elit.', 'Egestas purus viverra accumsan in.', 'Fermentum odio eu feugiat pretium nibh ipsum.', 'Venenatis cras sed felis eget.', 'Purus gravida quis blandit turpis cursus in hac habitasse platea.', 'Viverra nibh cras pulvinar mattis nunc sed blandit libero volutpat.', 'Nunc faucibus a pellentesque sit.', 'Sit amet consectetur adipiscing elit duis tristique sollicitudin.', 'Nisl condimentum id venenatis a condimentum vitae sapien pellentesque.', 'Augue eget arcu dictum varius.', 'Suspendisse in est ante in.', 'Tincidunt ornare massa eget egestas purus viverra accumsan in.', 'Venenatis tellus in metus vulputate eu scelerisque felis imperdiet.', 'Vitae tortor condimentum lacinia quis vel eros.', 'Vel pharetra vel turpis nunc eget lorem dolor sed viverra.', 'Risus at ultrices mi tempus imperdiet nulla malesuada pellentesque elit.', 'Ipsum faucibus vitae aliquet nec ullamcorper.', 'Est sit amet facilisis magna etiam tempor orci eu.', 'Vel orci porta non pulvinar neque laoreet suspendisse interdum consectetur.', 'Sit amet facilisis magna etiam.', 'Amet aliquam id diam maecenas ultricies.', 'Quis vel eros donec ac odio.', 'Dui vivamus arcu felis bibendum ut tristique et egestas quis.', 'Egestas sed sed risus pretium quam.', 'Turpis massa tincidunt dui ut ornare lectus.', 'Dictumst vestibulum rhoncus est pellentesque.', 'Sit amet cursus sit amet dictum sit amet justo donec.', 'Eros donec ac odio tempor orci dapibus ultrices.', 'Donec et odio pellentesque diam volutpat commodo sed egestas egestas.', 'Ultrices tincidunt arcu non sodales.', 'Risus quis varius quam quisque id diam.', 'Vestibulum lorem sed risus ultricies tristique nulla.', 'Lorem sed risus ultricies tristique nulla aliquet.', 'Tortor dignissim convallis aenean et tortor at risus viverra.', 'Tortor dignissim convallis aenean et tortor at risus.', 'Maecenas pharetra convallis posuere morbi leo urna molestie at.', 'Tincidunt arcu non sodales neque.', 'Id aliquet risus feugiat in.', 'Congue quisque egestas diam in arcu cursus euismod.', 'Porttitor rhoncus dolor purus non.', 'Facilisis leo vel fringilla est ullamcorper eget nulla facilisi.', 'Massa id neque aliquam vestibulum morbi.', 'Sit amet consectetur adipiscing elit ut aliquam.', 'Orci phasellus egestas tellus rutrum.', 'Ac turpis egestas sed tempus urna et pharetra pharetra massa.', 'Dignissim cras tincidunt lobortis feugiat vivamus at augue.', 'Mi tempus imperdiet nulla malesuada.', 'Senectus et netus et malesuada fames ac turpis.', 'At erat pellentesque adipiscing commodo elit.', 'A iaculis at erat pellentesque.', 'Nunc sed blandit libero volutpat sed cras ornare.', 'Facilisis magna etiam tempor orci eu.', 'Metus aliquam eleifend mi in nulla posuere.', 'Tempor orci eu lobortis elementum nibh tellus molestie nunc.', 'Amet nulla facilisi morbi tempus iaculis.', 'Massa vitae tortor condimentum lacinia quis vel eros donec.', 'Sit amet aliquam id diam maecenas.', 'Donec et odio pellentesque diam volutpat commodo.', 'Tristique senectus et netus et malesuada fames ac turpis egestas.', 'Consequat semper viverra nam libero justo laoreet sit.', 'Aenean et tortor at risus viverra adipiscing at in.', 'Nascetur ridiculus mus mauris vitae ultricies leo.', 'Feugiat in ante metus dictum.', 'Nisl purus in mollis nunc sed.', 'Suspendisse potenti nullam ac tortor vitae purus.', 'Velit egestas dui id ornare arcu.', 'Lacus luctus accumsan tortor posuere ac ut consequat semper.', 'Massa id neque aliquam vestibulum morbi blandit cursus risus at.', 'Odio facilisis mauris sit amet massa vitae tortor condimentum.', 'A iaculis at erat pellentesque.', 'Blandit aliquam etiam erat velit scelerisque in dictum.', 'Velit euismod in pellentesque massa placerat duis ultricies lacus.', 'Felis bibendum ut tristique et egestas quis ipsum suspendisse.', 'Venenatis urna cursus eget nunc scelerisque.', 'Sodales ut etiam sit amet.', 'Id semper risus in hendrerit.', 'Faucibus a pellentesque sit amet porttitor eget dolor.', 'Eget est lorem ipsum dolor sit amet consectetur adipiscing elit.', 'Pretium quam vulputate dignissim suspendisse in est ante in nibh.', 'Odio ut enim blandit volutpat maecenas volutpat blandit aliquam etiam.', 'Ullamcorper a lacus vestibulum sed arcu non.', 'Ipsum a arcu cursus vitae congue mauris rhoncus aenean.', 'Massa eget egestas purus viverra.', 'Scelerisque varius morbi enim nunc faucibus a pellentesque.', 'Vel facilisis volutpat est velit egestas dui id ornare.', 'Donec ac odio tempor orci dapibus ultrices in iaculis.', 'In hac habitasse platea dictumst quisque sagittis purus.', 'Id diam vel quam elementum.', 'Tempor orci eu lobortis elementum nibh tellus molestie.', 'Ut lectus arcu bibendum at varius.', 'Aenean sed adipiscing diam donec adipiscing tristique risus nec feugiat.', 'Arcu bibendum at varius vel pharetra vel turpis nunc eget.', 'Et ligula ullamcorper malesuada proin libero nunc consequat interdum.', 'Diam quam nulla porttitor massa id neque aliquam vestibulum morbi.', 'At tempor commodo ullamcorper a lacus vestibulum sed arcu.', 'Pellentesque habitant morbi tristique senectus et netus et malesuada fames.', 'Porttitor eget dolor morbi non.', 'Sed augue lacus viverra vitae congue eu consequat ac.', 'Lorem dolor sed viverra ipsum nunc aliquet bibendum enim.', 'Tincidunt lobortis feugiat vivamus at augue eget arcu dictum.', 'Lacinia at quis risus sed vulputate odio ut enim.', 'Duis convallis convallis tellus id interdum velit laoreet.', 'Volutpat diam ut venenatis tellus.', 'Mi quis hendrerit dolor magna eget est.', 'Massa vitae tortor condimentum lacinia quis vel.', 'Egestas sed tempus urna et pharetra pharetra massa.', 'Dictum fusce ut placerat orci nulla pellentesque dignissim.', 'Posuere ac ut consequat semper.', 'Mi bibendum neque egestas congue quisque egestas.', 'Nunc mi ipsum faucibus vitae aliquet nec ullamcorper sit.', 'Odio euismod lacinia at quis.', 'Sem nulla pharetra diam sit amet nisl suscipit adipiscing bibendum.', 'Eget lorem dolor sed viverra ipsum nunc.', 'In nisl nisi scelerisque eu ultrices vitae auctor eu.', 'Diam in arcu cursus euismod quis viverra nibh cras pulvinar.', 'Nibh tellus molestie nunc non blandit massa enim nec.', 'Neque volutpat ac tincidunt vitae semper quis.', 'Platea dictumst vestibulum rhoncus est pellentesque elit ullamcorper.', 'Enim tortor at auctor urna nunc id cursus metus.', 'Sit amet nisl suscipit adipiscing bibendum est ultricies integer quis.', 'Pellentesque dignissim enim sit amet venenatis urna cursus.', 'Magnis dis parturient montes nascetur ridiculus.', 'Interdum velit euismod in pellentesque massa.', 'Dictum fusce ut placerat orci nulla pellentesque dignissim enim.', 'Elementum integer enim neque volutpat ac tincidunt.', 'Consequat interdum varius sit amet mattis.', 'Laoreet id donec ultrices tincidunt arcu non sodales neque.', 'Dignissim diam quis enim lobortis scelerisque fermentum dui.', 'Metus aliquam eleifend mi in.', 'Tortor vitae purus faucibus ornare suspendisse sed nisi lacus.', 'Orci phasellus egestas tellus rutrum.', 'Interdum varius sit amet mattis vulputate enim nulla aliquet porttitor.', 'Tellus rutrum tellus pellentesque eu.', 'Morbi tincidunt ornare massa eget egestas.', 'Congue mauris rhoncus aenean vel elit scelerisque.', 'Vitae et leo duis ut diam quam nulla porttitor.', 'Sed turpis tincidunt id aliquet risus feugiat in ante.', 'Quis auctor elit sed vulputate mi sit amet.', 'Phasellus vestibulum lorem sed risus ultricies tristique nulla aliquet enim.', 'Fermentum dui faucibus in ornare quam viverra.', 'Fermentum et sollicitudin ac orci phasellus egestas.', 'Leo vel orci porta non pulvinar.', 'Tellus integer feugiat scelerisque varius morbi enim nunc faucibus a.', 'Ut ornare lectus sit amet est placerat.', 'Egestas quis ipsum suspendisse ultrices gravida dictum fusce ut placerat.', 'Egestas egestas fringilla phasellus faucibus scelerisque eleifend.', 'Et pharetra pharetra massa massa ultricies mi quis hendrerit dolor.', 'Venenatis lectus magna fringilla urna porttitor rhoncus.', 'Leo integer malesuada nunc vel risus commodo viverra maecenas accumsan.', 'Sit amet luctus venenatis lectus magna.', 'In fermentum posuere urna nec tincidunt.', 'Maecenas volutpat blandit aliquam etiam erat velit.', 'Felis imperdiet proin fermentum leo vel orci porta non.', 'Faucibus in ornare quam viverra orci sagittis.', 'Diam quam nulla porttitor massa id neque aliquam vestibulum.', 'Vitae sapien pellentesque habitant morbi tristique senectus et netus.', 'Maecenas sed enim ut sem viverra.', 'Mauris pellentesque pulvinar pellentesque habitant morbi tristique.', 'Amet massa vitae tortor condimentum lacinia quis vel eros.', 'Viverra nibh cras pulvinar mattis nunc.', 'Sit amet mattis vulputate enim nulla aliquet porttitor.', 'Placerat vestibulum lectus mauris ultrices eros in.', 'Dui vivamus arcu felis bibendum ut.', 'Eros in cursus turpis massa tincidunt dui ut ornare.', 'Neque convallis a cras semper auctor.', 'Sagittis purus sit amet volutpat consequat mauris.', 'Vestibulum morbi blandit cursus risus at ultrices.', 'Sagittis orci a scelerisque purus semper eget duis at tellus.', 'Viverra orci sagittis eu volutpat odio.', 'Luctus venenatis lectus magna fringilla.', 'Lacus vestibulum sed arcu non.', 'Dui accumsan sit amet nulla.', 'Ullamcorper dignissim cras tincidunt lobortis.', 'Quis lectus nulla at volutpat diam.', 'A diam sollicitudin tempor id eu nisl nunc.', 'Morbi tristique senectus et netus et malesuada fames ac turpis.', 'Dignissim enim sit amet venenatis urna cursus.', 'Porttitor lacus luctus accumsan tortor posuere ac.', 'Amet justo donec enim diam vulputate ut pharetra.', 'Nunc sed velit dignissim sodales ut eu sem integer vitae.', 'Vestibulum lorem sed risus ultricies tristique nulla aliquet enim tortor.', 'Dictumst vestibulum rhoncus est pellentesque elit ullamcorper dignissim cras tincidunt.', 'Sed vulputate mi sit amet mauris.', 'Ut morbi tincidunt augue interdum velit euismod.', 'In ornare quam viverra orci sagittis eu volutpat.', 'Pellentesque dignissim enim sit amet venenatis urna.', 'Viverra aliquet eget sit amet tellus.', 'Aliquam purus sit amet luctus venenatis lectus magna fringilla urna.', 'Nunc faucibus a pellentesque sit amet porttitor eget.', 'Dui vivamus arcu felis bibendum ut tristique et.', 'Tempor id eu nisl nunc.', 'Vivamus at augue eget arcu dictum varius duis at consectetur.', 'Risus in hendrerit gravida rutrum quisque non tellus.', 'Morbi enim nunc faucibus a pellentesque.', 'Pellentesque eu tincidunt tortor aliquam nulla facilisi cras.', 'At augue eget arcu dictum.', 'Tellus elementum sagittis vitae et.', 'Vitae elementum curabitur vitae nunc sed velit.', 'Tellus id interdum velit laoreet id donec ultrices tincidunt arcu.', 'Placerat vestibulum lectus mauris ultrices eros in.', 'Nunc non blandit massa enim nec dui nunc mattis.', 'Mi ipsum faucibus vitae aliquet nec.', 'Rhoncus urna neque viverra justo nec ultrices dui sapien eget.', 'Quis viverra nibh cras pulvinar mattis nunc sed.', 'Tellus id interdum velit laoreet id.', 'Dictum fusce ut placerat orci nulla pellentesque dignissim enim.', 'Libero enim sed faucibus turpis in eu mi.', 'Ornare lectus sit amet est placerat in egestas.', 'Facilisi etiam dignissim diam quis.', 'Orci dapibus ultrices in iaculis nunc sed augue lacus.', 'Amet luctus venenatis lectus magna fringilla urna porttitor rhoncus dolor.', 'Nisi est sit amet facilisis magna etiam tempor orci eu.', 'At auctor urna nunc id cursus metus aliquam.', 'Massa massa ultricies mi quis hendrerit dolor magna eget.', 'Mauris sit amet massa vitae.', 'Id ornare arcu odio ut sem nulla pharetra diam sit.', 'Odio ut enim blandit volutpat maecenas.', 'Sit amet justo donec enim diam vulputate ut pharetra sit.', 'Velit euismod in pellentesque massa placerat duis ultricies lacus.', 'Egestas integer eget aliquet nibh praesent tristique magna.', 'Vitae congue eu consequat ac felis donec.', 'Porttitor eget dolor morbi non arcu.', 'Tortor dignissim convallis aenean et tortor.', 'Enim nec dui nunc mattis enim ut tellus.', 'Amet nisl suscipit adipiscing bibendum est.', 'Rhoncus urna neque viverra justo nec ultrices dui sapien eget.', 'Tincidunt eget nullam non nisi est sit.', 'Ac odio tempor orci dapibus ultrices in iaculis nunc.', 'Arcu odio ut sem nulla.', 'Nunc lobortis mattis aliquam faucibus.', 'Nisl nunc mi ipsum faucibus vitae aliquet nec ullamcorper.', 'Nulla aliquet enim tortor at auctor urna nunc.', 'Ultricies tristique nulla aliquet enim tortor at auctor urna nunc.', 'Eget nunc lobortis mattis aliquam faucibus purus in.', 'Mauris augue neque gravida in fermentum et sollicitudin ac.', 'Convallis convallis tellus id interdum velit laoreet id donec ultrices.', 'Purus faucibus ornare suspendisse sed nisi lacus sed.', 'Viverra accumsan in nisl nisi scelerisque eu ultrices.', 'Imperdiet nulla malesuada pellentesque elit eget.', 'A arcu cursus vitae congue mauris rhoncus aenean vel elit.', 'In nisl nisi scelerisque eu ultrices vitae auctor eu augue.', 'Habitant morbi tristique senectus et netus et.', 'Varius sit amet mattis vulputate enim.', 'Nisl nisi scelerisque eu ultrices vitae auctor eu augue.', 'Nulla aliquet porttitor lacus luctus accumsan tortor posuere.', 'Eget mi proin sed libero enim sed.', 'Cursus vitae congue mauris rhoncus aenean vel elit scelerisque.', 'Arcu ac tortor dignissim convallis aenean et tortor at risus.', 'Faucibus nisl tincidunt eget nullam non nisi est sit.', 'Ut etiam sit amet nisl purus.', 'Praesent elementum facilisis leo vel fringilla est ullamcorper eget.', 'Massa ultricies mi quis hendrerit dolor magna eget est.', 'Purus semper eget duis at tellus.', 'Vitae turpis massa sed elementum tempus egestas sed sed.', 'Vestibulum mattis ullamcorper velit sed ullamcorper.', 'Ut sem nulla pharetra diam sit.', 'Pellentesque elit eget gravida cum sociis natoque.', 'Dolor sit amet consectetur adipiscing.', 'Volutpat est velit egestas dui id ornare arcu odio.', 'Neque laoreet suspendisse interdum consectetur libero.', 'Adipiscing enim eu turpis egestas.', 'Nulla facilisi morbi tempus iaculis urna.', 'Congue quisque egestas diam in arcu cursus.', 'In iaculis nunc sed augue lacus viverra.', 'Semper quis lectus nulla at.', 'Ut venenatis tellus in metus.', 'Aliquet lectus proin nibh nisl condimentum id venenatis.', 'Turpis in eu mi bibendum neque.', 'Ipsum consequat nisl vel pretium lectus quam.', 'Risus sed vulputate odio ut enim blandit volutpat maecenas.', 'Aliquet risus feugiat in ante metus.', 'Aliquam nulla facilisi cras fermentum odio eu.', 'Egestas purus viverra accumsan in.', 'Hendrerit dolor magna eget est lorem ipsum dolor.', 'Pharetra convallis posuere morbi leo urna molestie at.', 'Pellentesque id nibh tortor id aliquet.', 'Aenean vel elit scelerisque mauris pellentesque pulvinar pellentesque habitant.', 'Ultricies mi quis hendrerit dolor magna eget est.', 'Turpis egestas maecenas pharetra convallis posuere morbi leo urna molestie.', 'Eget aliquet nibh praesent tristique magna sit amet.', 'Tincidunt vitae semper quis lectus nulla at volutpat diam ut.', 'Tellus pellentesque eu tincidunt tortor aliquam nulla facilisi cras fermentum.', 'Massa ultricies mi quis hendrerit dolor magna eget est lorem.', 'Feugiat nisl pretium fusce id velit ut tortor.', 'Amet risus nullam eget felis eget nunc lobortis.', 'Sit amet porttitor eget dolor morbi.', 'Vestibulum morbi blandit cursus risus.', 'Dignissim enim sit amet venenatis urna cursus.', 'Lectus arcu bibendum at varius vel pharetra vel turpis nunc.', 'Arcu bibendum at varius vel pharetra vel turpis.', 'Ut morbi tincidunt augue interdum velit.', 'Sit amet dictum sit amet justo.', 'Dolor morbi non arcu risus quis varius quam quisque.', 'Fusce id velit ut tortor pretium viverra suspendisse potenti.', 'Lobortis feugiat vivamus at augue.', 'Tellus at urna condimentum mattis pellentesque id nibh tortor id.', 'Quam quisque id diam vel quam.', 'Quisque egestas diam in arcu cursus euismod quis.', 'Consectetur libero id faucibus nisl.', 'Felis donec et odio pellentesque.', 'In hac habitasse platea dictumst quisque sagittis.', 'Et tortor at risus viverra adipiscing at in tellus integer.', 'Lobortis elementum nibh tellus molestie nunc non.', 'Mauris sit amet massa vitae tortor condimentum lacinia quis vel.', 'Erat pellentesque adipiscing commodo elit at imperdiet.', 'Quis viverra nibh cras pulvinar mattis nunc sed blandit.', 'Ut ornare lectus sit amet est.', 'Elementum integer enim neque volutpat ac tincidunt.', 'Enim neque volutpat ac tincidunt vitae semper quis lectus.', 'Massa ultricies mi quis hendrerit dolor magna eget.', 'Dictum at tempor commodo ullamcorper.', 'Nulla pharetra diam sit amet nisl.', 'Dignissim suspendisse in est ante in nibh.', 'Mattis rhoncus urna neque viverra justo nec ultrices dui sapien.', 'Luctus accumsan tortor posuere ac ut consequat semper.', 'In arcu cursus euismod quis.', 'Tincidunt augue interdum velit euismod in.', 'Eleifend donec pretium vulputate sapien nec.', 'Nec sagittis aliquam malesuada bibendum arcu vitae.', 'Tellus cras adipiscing enim eu turpis egestas pretium aenean pharetra.', 'Viverra vitae congue eu consequat ac felis donec.', 'Hac habitasse platea dictumst quisque sagittis.', 'Egestas dui id ornare arcu.', 'Tortor condimentum lacinia quis vel eros.', 'Elementum integer enim neque volutpat ac tincidunt vitae semper.', 'Viverra vitae congue eu consequat ac felis donec et odio.', 'Massa id neque aliquam vestibulum morbi blandit cursus risus.', 'Aliquet lectus proin nibh nisl condimentum.', 'Ac tortor vitae purus faucibus ornare.', 'Cursus eget nunc scelerisque viverra mauris in.', 'Orci a scelerisque purus semper.', 'Ut diam quam nulla porttitor massa id neque aliquam vestibulum.', 'Lorem ipsum dolor sit amet.', 'Massa vitae tortor condimentum lacinia quis vel.', 'Ultrices mi tempus imperdiet nulla malesuada.', 'Posuere ac ut consequat semper viverra.', 'Odio facilisis mauris sit amet massa vitae tortor.', 'Porta lorem mollis aliquam ut porttitor leo a diam.', 'Est ante in nibh mauris.', 'Fringilla ut morbi tincidunt augue interdum velit euismod in.', 'Phasellus faucibus scelerisque eleifend donec pretium vulputate sapien nec.', 'Non pulvinar neque laoreet suspendisse interdum.', 'At auctor urna nunc id cursus.', 'In nisl nisi scelerisque eu ultrices vitae auctor.', 'Quis eleifend quam adipiscing vitae.', 'Laoreet id donec ultrices tincidunt arcu non sodales.', 'Condimentum vitae sapien pellentesque habitant morbi tristique senectus et.', 'Maecenas sed enim ut sem viverra aliquet.', 'Amet consectetur adipiscing elit ut aliquam purus sit amet.', 'In nisl nisi scelerisque eu ultrices vitae auctor eu augue.', 'Urna cursus eget nunc scelerisque viverra mauris in.', 'Enim sed faucibus turpis in eu.', 'Aliquet lectus proin nibh nisl.', 'Sed lectus vestibulum mattis ullamcorper velit sed ullamcorper morbi tincidunt.', 'Rutrum quisque non tellus orci ac auctor augue.', 'Tellus at urna condimentum mattis pellentesque.', 'Massa tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada.', 'Semper feugiat nibh sed pulvinar proin gravida hendrerit lectus.', 'Et malesuada fames ac turpis.', 'Sagittis nisl rhoncus mattis rhoncus.', 'Faucibus in ornare quam viverra.', 'Turpis massa sed elementum tempus egestas sed.', 'Nec ultrices dui sapien eget mi proin sed.', 'Mi sit amet mauris commodo quis imperdiet massa.', 'Viverra ipsum nunc aliquet bibendum enim.', 'A pellentesque sit amet porttitor eget dolor.', 'Dui vivamus arcu felis bibendum ut tristique et.', 'Augue eget arcu dictum varius duis at consectetur.', 'Diam sollicitudin tempor id eu nisl nunc mi.', 'Congue nisi vitae suscipit tellus.', 'Blandit turpis cursus in hac habitasse platea dictumst quisque sagittis.', 'Est lorem ipsum dolor sit amet consectetur.', 'Egestas fringilla phasellus faucibus scelerisque eleifend donec pretium vulputate.', 'Aliquet lectus proin nibh nisl condimentum id venenatis.', 'Augue neque gravida in fermentum et.', 'Sem integer vitae justo eget.', 'Vitae et leo duis ut diam quam nulla porttitor.', 'Maecenas ultricies mi eget mauris pharetra.', 'Scelerisque in dictum non consectetur a.', 'Sit amet facilisis magna etiam.', 'A pellentesque sit amet porttitor eget dolor.', 'Arcu risus quis varius quam quisque.', 'Nisl suscipit adipiscing bibendum est ultricies.', 'A condimentum vitae sapien pellentesque habitant morbi tristique senectus et.', 'Ipsum consequat nisl vel pretium lectus quam id leo.', 'Risus in hendrerit gravida rutrum quisque non.', 'Ultrices gravida dictum fusce ut placerat.', 'Convallis posuere morbi leo urna molestie.', 'Ac tincidunt vitae semper quis lectus nulla.', 'Semper feugiat nibh sed pulvinar proin gravida hendrerit lectus.', 'Eros donec ac odio tempor orci dapibus ultrices in iaculis.', 'Amet porttitor eget dolor morbi.', 'Sit amet mauris commodo quis imperdiet.', 'Nec ultrices dui sapien eget.', 'Mi tempus imperdiet nulla malesuada pellentesque elit eget.', 'Fringilla ut morbi tincidunt augue interdum velit euismod in pellentesque.', 'Arcu non odio euismod lacinia at.', 'Aliquam faucibus purus in massa tempor nec feugiat nisl.', 'Nulla pharetra diam sit amet nisl.', 'Pharetra sit amet aliquam id diam.', 'Turpis egestas integer eget aliquet nibh praesent tristique.', 'Id diam vel quam elementum pulvinar.', 'Faucibus scelerisque eleifend donec pretium.', 'Accumsan in nisl nisi scelerisque.', 'Urna duis convallis convallis tellus id interdum.', 'Massa id neque aliquam vestibulum.', 'Hendrerit gravida rutrum quisque non tellus orci ac.', 'Risus in hendrerit gravida rutrum quisque.', 'Lacus vel facilisis volutpat est velit egestas.', 'Quisque egestas diam in arcu cursus euismod quis viverra.', 'Molestie nunc non blandit massa enim nec dui nunc mattis.', 'Orci phasellus egestas tellus rutrum tellus pellentesque eu tincidunt tortor.', 'Nunc faucibus a pellentesque sit amet porttitor eget.', 'Lacus vel facilisis volutpat est.', 'Aliquet eget sit amet tellus cras adipiscing.', 'Viverra tellus in hac habitasse platea dictumst vestibulum rhoncus est.', 'Lobortis elementum nibh tellus molestie nunc non blandit massa.', 'Duis at consectetur lorem donec massa sapien faucibus et.', 'Quam viverra orci sagittis eu volutpat.', 'Cursus eget nunc scelerisque viverra mauris in aliquam.', 'Risus nec feugiat in fermentum posuere urna.', 'Tincidunt id aliquet risus feugiat in ante.', 'Massa massa ultricies mi quis hendrerit dolor magna eget.', 'Nunc aliquet bibendum enim facilisis.', 'Mi bibendum neque egestas congue quisque egestas diam in arcu.', 'Massa massa ultricies mi quis hendrerit dolor magna eget est.', 'Cursus in hac habitasse platea.', 'Quam vulputate dignissim suspendisse in est.', 'Semper feugiat nibh sed pulvinar.', 'Nam at lectus urna duis convallis convallis tellus.', 'Potenti nullam ac tortor vitae purus faucibus.', 'Leo in vitae turpis massa sed elementum tempus.', 'Est ullamcorper eget nulla facilisi etiam dignissim diam quis enim.', 'Vehicula ipsum a arcu cursus vitae congue.', 'Commodo sed egestas egestas fringilla phasellus faucibus scelerisque eleifend donec.', 'Non pulvinar neque laoreet suspendisse.', 'Semper eget duis at tellus.', 'Erat imperdiet sed euismod nisi porta lorem mollis.', 'Mi proin sed libero enim.', 'Lacus luctus accumsan tortor posuere ac ut consequat semper viverra.', 'Viverra orci sagittis eu volutpat odio facilisis mauris sit.', 'Mattis enim ut tellus elementum sagittis.', 'Non odio euismod lacinia at quis.', 'Viverra ipsum nunc aliquet bibendum enim.', 'Egestas fringilla phasellus faucibus scelerisque eleifend.', 'Dolor sed viverra ipsum nunc aliquet.', 'Nec feugiat in fermentum posuere urna.', 'Consequat interdum varius sit amet mattis.', 'Amet volutpat consequat mauris nunc congue nisi vitae suscipit.', 'Viverra justo nec ultrices dui sapien eget.', 'Neque convallis a cras semper auctor.', 'Netus et malesuada fames ac turpis egestas integer.', 'Ut porttitor leo a diam sollicitudin.', 'Porttitor lacus luctus accumsan tortor posuere ac ut.', 'Varius vel pharetra vel turpis nunc eget lorem dolor.', 'Est placerat in egestas erat imperdiet sed.', 'Laoreet non curabitur gravida arcu ac tortor dignissim convallis.', 'Ut tortor pretium viverra suspendisse potenti nullam ac.', 'Et tortor at risus viverra adipiscing.', 'Accumsan in nisl nisi scelerisque.', 'Faucibus vitae aliquet nec ullamcorper sit amet risus nullam.', 'Eget mi proin sed libero enim sed faucibus turpis in.', 'Fringilla est ullamcorper eget nulla.', 'Faucibus scelerisque eleifend donec pretium vulputate sapien.', 'Facilisi nullam vehicula ipsum a arcu cursus vitae congue.', 'Gravida in fermentum et sollicitudin ac orci phasellus.', 'Rutrum tellus pellentesque eu tincidunt.', 'Nunc aliquet bibendum enim facilisis gravida.', 'Quam nulla porttitor massa id neque.', 'Aliquam purus sit amet luctus.', 'Luctus accumsan tortor posuere ac ut consequat semper viverra.', 'Facilisi cras fermentum odio eu feugiat pretium nibh ipsum.', 'Cras tincidunt lobortis feugiat vivamus at augue eget.', 'Semper eget duis at tellus at urna condimentum.', 'Ipsum a arcu cursus vitae congue mauris rhoncus.', 'Amet purus gravida quis blandit turpis cursus in.', 'A condimentum vitae sapien pellentesque habitant morbi tristique.', 'Lacus luctus accumsan tortor posuere ac ut consequat semper viverra.', 'Dolor purus non enim praesent elementum.', 'Sed odio morbi quis commodo odio.', 'Scelerisque varius morbi enim nunc faucibus a pellentesque sit.', 'Ipsum suspendisse ultrices gravida dictum.', 'Sagittis nisl rhoncus mattis rhoncus urna neque viverra.', 'Leo vel fringilla est ullamcorper eget nulla facilisi etiam.', 'Semper auctor neque vitae tempus quam pellentesque.', 'At quis risus sed vulputate odio ut enim blandit.', 'Id ornare arcu odio ut sem nulla.', 'Dictumst quisque sagittis purus sit amet volutpat.', 'Sollicitudin ac orci phasellus egestas tellus rutrum tellus.', 'Eleifend donec pretium vulputate sapien nec sagittis aliquam.', 'Justo laoreet sit amet cursus sit amet.', 'Hac habitasse platea dictumst vestibulum rhoncus est pellentesque.', 'Scelerisque eleifend donec pretium vulputate sapien nec.', 'Donec enim diam vulputate ut pharetra sit.', 'In vitae turpis massa sed elementum tempus.', 'Vitae suscipit tellus mauris a diam.', 'Sed id semper risus in hendrerit gravida.', 'Ultrices in iaculis nunc sed.', 'Convallis posuere morbi leo urna molestie at elementum eu.', 'Pulvinar mattis nunc sed blandit libero volutpat sed.', 'Ornare aenean euismod elementum nisi quis eleifend quam.', 'Vitae justo eget magna fermentum iaculis eu non diam phasellus.', 'Fusce id velit ut tortor pretium viverra suspendisse.', 'Maecenas ultricies mi eget mauris pharetra et.', 'Risus at ultrices mi tempus imperdiet nulla malesuada.', 'Non consectetur a erat nam at lectus.', 'Vel facilisis volutpat est velit egestas dui id.', 'Porttitor lacus luctus accumsan tortor.', 'Aliquam id diam maecenas ultricies mi eget.', 'Praesent tristique magna sit amet purus gravida quis blandit turpis.', 'Dolor sed viverra ipsum nunc aliquet bibendum.', 'Interdum varius sit amet mattis vulputate enim.', 'Volutpat est velit egestas dui id ornare arcu odio ut.', 'Pellentesque id nibh tortor id aliquet lectus proin nibh nisl.', 'Eu feugiat pretium nibh ipsum consequat.', 'Nisi porta lorem mollis aliquam ut porttitor leo a diam.', 'Faucibus nisl tincidunt eget nullam non nisi est sit.', 'Tempor commodo ullamcorper a lacus vestibulum sed arcu non odio.', 'Quam adipiscing vitae proin sagittis nisl rhoncus mattis rhoncus urna.', 'In ornare quam viverra orci sagittis eu.', 'In egestas erat imperdiet sed euismod nisi porta lorem.', 'Nulla at volutpat diam ut venenatis tellus.', 'Tortor vitae purus faucibus ornare suspendisse sed nisi lacus.', 'Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula.', 'Velit euismod in pellentesque massa placerat duis ultricies.', 'Sapien pellentesque habitant morbi tristique senectus.', 'Risus nec feugiat in fermentum posuere urna.', 'Sem fringilla ut morbi tincidunt augue interdum velit euismod.', 'Cum sociis natoque penatibus et magnis dis.', 'A lacus vestibulum sed arcu non odio euismod.', 'Elementum nibh tellus molestie nunc non blandit massa enim nec.', 'Iaculis urna id volutpat lacus laoreet.', 'Neque convallis a cras semper auctor neque vitae tempus.', 'Orci porta non pulvinar neque laoreet suspendisse.', 'Molestie a iaculis at erat pellentesque adipiscing commodo elit.', 'Quis risus sed vulputate odio ut enim.', 'Nam at lectus urna duis convallis.', 'Adipiscing elit pellentesque habitant morbi tristique senectus.', 'Sed lectus vestibulum mattis ullamcorper velit sed ullamcorper.', 'Blandit volutpat maecenas volutpat blandit aliquam etiam erat velit scelerisque.', 'Ornare quam viverra orci sagittis eu volutpat.', 'Posuere ac ut consequat semper viverra nam.', 'Mollis nunc sed id semper risus in.', 'Lectus proin nibh nisl condimentum.', 'Tincidunt augue interdum velit euismod in pellentesque massa.', 'Commodo elit at imperdiet dui accumsan sit amet nulla.', 'Ultrices mi tempus imperdiet nulla malesuada pellentesque elit eget.', 'Pellentesque diam volutpat commodo sed egestas egestas fringilla phasellus faucibus.', 'Etiam non quam lacus suspendisse faucibus.', 'Consectetur libero id faucibus nisl.', 'Sit amet est placerat in egestas erat imperdiet sed.', 'Neque laoreet suspendisse interdum consectetur libero id.', 'Id cursus metus aliquam eleifend mi in.', 'Dignissim cras tincidunt lobortis feugiat vivamus at augue eget.', 'Id aliquet risus feugiat in.', 'Tortor aliquam nulla facilisi cras fermentum odio.', 'Consectetur lorem donec massa sapien.', 'Id velit ut tortor pretium viverra suspendisse.', 'Massa sed elementum tempus egestas sed.', 'Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula.', 'Eget velit aliquet sagittis id consectetur purus.', 'Etiam erat velit scelerisque in dictum non consectetur a.', 'Dignissim suspendisse in est ante in nibh.', 'Tincidunt praesent semper feugiat nibh sed pulvinar proin gravida hendrerit.', 'Convallis aenean et tortor at risus viverra adipiscing at.', 'Lacinia quis vel eros donec ac odio tempor orci.', 'Interdum varius sit amet mattis vulputate enim.', 'Eget felis eget nunc lobortis mattis aliquam faucibus.', 'Quisque egestas diam in arcu cursus euismod quis viverra nibh.', 'Sed viverra tellus in hac habitasse platea.', 'Nulla pharetra diam sit amet nisl suscipit adipiscing.', 'Consequat nisl vel pretium lectus quam.', 'Habitant morbi tristique senectus et.', 'Pretium nibh ipsum consequat nisl vel pretium.', 'Egestas tellus rutrum tellus pellentesque eu tincidunt tortor aliquam nulla.', 'Pellentesque adipiscing commodo elit at imperdiet dui accumsan sit.', 'Habitant morbi tristique senectus et netus et malesuada fames.', 'Vitae suscipit tellus mauris a diam maecenas sed.', 'Lorem ipsum dolor sit amet.', 'Mollis nunc sed id semper risus in hendrerit.', 'Lectus arcu bibendum at varius vel pharetra vel turpis.', 'Enim ut sem viverra aliquet eget sit amet.', 'Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere.', 'Interdum velit euismod in pellentesque.', 'Enim sed faucibus turpis in eu mi bibendum neque egestas.', 'Eu sem integer vitae justo.', 'Curabitur gravida arcu ac tortor dignissim.', 'Donec pretium vulputate sapien nec sagittis.', 'Nulla posuere sollicitudin aliquam ultrices sagittis.', 'Habitant morbi tristique senectus et netus et.', 'Feugiat scelerisque varius morbi enim nunc faucibus.', 'Hac habitasse platea dictumst vestibulum rhoncus est pellentesque elit.', 'Posuere urna nec tincidunt praesent.', 'Sed turpis tincidunt id aliquet risus feugiat in.', 'Sapien et ligula ullamcorper malesuada proin libero nunc consequat.', 'Sem viverra aliquet eget sit amet tellus cras.', 'Tellus id interdum velit laoreet id.', 'Ipsum faucibus vitae aliquet nec ullamcorper sit.', 'Mi tempus imperdiet nulla malesuada pellentesque elit eget gravida.', 'Fusce id velit ut tortor.', 'Sed enim ut sem viverra aliquet eget sit amet tellus.', 'Risus nullam eget felis eget nunc.', 'Nec sagittis aliquam malesuada bibendum arcu vitae elementum curabitur vitae.', 'Vitae auctor eu augue ut lectus arcu bibendum at varius.', 'Et malesuada fames ac turpis.', 'Est pellentesque elit ullamcorper dignissim cras.', 'Ornare massa eget egestas purus viverra.', 'Bibendum est ultricies integer quis auctor elit sed vulputate mi.', 'In fermentum et sollicitudin ac orci.', 'Scelerisque mauris pellentesque pulvinar pellentesque habitant morbi tristique.', 'Venenatis cras sed felis eget velit.', 'At lectus urna duis convallis convallis.', 'Diam donec adipiscing tristique risus nec.', 'Erat nam at lectus urna duis convallis convallis.', 'Diam quis enim lobortis scelerisque fermentum dui faucibus in ornare.', 'Convallis tellus id interdum velit laoreet id donec ultrices tincidunt.', 'Tincidunt eget nullam non nisi.', 'Pellentesque nec nam aliquam sem.', 'Ipsum dolor sit amet consectetur adipiscing elit pellentesque habitant.', 'Vulputate sapien nec sagittis aliquam malesuada.', 'In metus vulputate eu scelerisque felis imperdiet proin fermentum.', 'Malesuada bibendum arcu vitae elementum curabitur vitae nunc sed velit.', 'Nec ultrices dui sapien eget mi proin.', 'Eu facilisis sed odio morbi quis commodo odio.', 'Sit amet luctus venenatis lectus magna.', 'Ac odio tempor orci dapibus ultrices in iaculis.', 'At tempor commodo ullamcorper a.', 'Eu mi bibendum neque egestas congue quisque egestas.', 'Donec adipiscing tristique risus nec feugiat in fermentum.', 'Semper risus in hendrerit gravida rutrum quisque non.', 'Elementum nibh tellus molestie nunc non blandit massa.', 'Viverra nibh cras pulvinar mattis nunc sed blandit.', 'Dignissim enim sit amet venenatis.', 'Adipiscing bibendum est ultricies integer quis auctor.', 'Morbi leo urna molestie at elementum eu facilisis sed odio.', 'Eget est lorem ipsum dolor sit amet.', 'Ut venenatis tellus in metus vulputate eu scelerisque felis imperdiet.', 'Quam pellentesque nec nam aliquam sem et tortor consequat id.', 'Est ante in nibh mauris.', 'Viverra suspendisse potenti nullam ac tortor vitae.', 'Nulla facilisi nullam vehicula ipsum a arcu.', 'At volutpat diam ut venenatis tellus.', 'Convallis a cras semper auctor neque vitae.', 'Vitae semper quis lectus nulla.', 'In fermentum et sollicitudin ac orci phasellus.', 'Justo donec enim diam vulputate ut pharetra sit.', 'Mauris vitae ultricies leo integer malesuada nunc.', 'Eget egestas purus viverra accumsan.', 'Pellentesque pulvinar pellentesque habitant morbi.', 'Aliquam sem et tortor consequat id.', 'Cum sociis natoque penatibus et.', 'Volutpat maecenas volutpat blandit aliquam.', 'Urna nec tincidunt praesent semper feugiat nibh sed pulvinar proin.', 'Eget sit amet tellus cras adipiscing.', 'Sed adipiscing diam donec adipiscing tristique risus.', 'Morbi blandit cursus risus at ultrices mi tempus.', 'Viverra nibh cras pulvinar mattis nunc sed blandit libero volutpat.', 'Justo donec enim diam vulputate ut pharetra sit amet.', 'Nunc consequat interdum varius sit.', 'Adipiscing tristique risus nec feugiat in.', 'Pretium nibh ipsum consequat nisl vel.', 'Amet nulla facilisi morbi tempus iaculis urna id volutpat lacus.', 'Non odio euismod lacinia at quis.', 'Elit ullamcorper dignissim cras tincidunt lobortis feugiat.', 'Sit amet porttitor eget dolor.', 'Malesuada pellentesque elit eget gravida cum.', 'Viverra accumsan in nisl nisi scelerisque.', 'Lectus magna fringilla urna porttitor rhoncus dolor purus non.', 'Vel risus commodo viverra maecenas accumsan.', 'Purus in massa tempor nec feugiat.', 'Volutpat diam ut venenatis tellus.', 'Mi eget mauris pharetra et.', 'Morbi enim nunc faucibus a.', 'Vel orci porta non pulvinar neque laoreet suspendisse.', 'Eget sit amet tellus cras adipiscing enim eu.', 'Ac felis donec et odio pellentesque diam.', 'Amet mauris commodo quis imperdiet massa tincidunt nunc pulvinar.', 'Sit amet mauris commodo quis imperdiet massa tincidunt nunc.', 'Orci dapibus ultrices in iaculis nunc sed augue lacus.', 'Integer eget aliquet nibh praesent.', 'Research Article Deep Learning Based Abstractive Text Summarization: Approaches, Datasets, Evaluation Measures, and Challenges Dima Suleiman and Arafat Awajan Princess Sumaya University for Technology, Amman, Jordan Correspondence should be addressed to Dima Suleiman; d.', 'jo Received 24 April 2020; Revised 1 July 2020; Accepted 25 July 2020; Published 24 August 2020 Academic Editor: Dimitris Mourtzis Copyright © 2020 Dima Suleiman and Arafat Awajan.', '*is is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.', 'In recent years, the volume of textual data has rapidly increased, which has generated a valuable resource for extracting and analysing information.', 'To retrieve useful knowledge within a reasonable time period, this information must be summarised.', '*is paper reviews recent approaches for abstractive text summarisation using deep learning models.', 'In addition, existing datasets for training and validating these approaches are reviewed, and their features and limitations are presented.', '*e Gigaword dataset is commonly employed for single-sentence summary approaches, while the Cable News Network (CNN)/Daily Mail dataset is commonly employed for multisentence summary approaches.', 'Furthermore, the measures that are utilised to evaluate the quality of summarisation are investigated, and Recall-Oriented Understudy for Gisting Evaluation 1 (ROUGE1), ROUGE2, and ROUGEL are determined to be the most commonly applied metrics.', '*e challenges that are encountered during the summarisation process and the solutions proposed in each approach are analysed.', '*e analysis of the several approaches shows that recurrent neural networks with an attention mechanism and long short-term memory (LSTM) are the most prevalent techniques for abstractive text summarisation.', '*e experimental results show that text summarisation with a pretrained encoder model achieved the highest values for ROUGE1, ROUGE2, and ROUGE-L (43.', 'Furthermore, it was determined that most abstractive text summarisation models faced challenges such as the unavailability of a golden token at testing time, outof-vocabulary (OOV) words, summary sentence repetition, inaccurate sentences, and fake facts.', 'Introduction Currently, there are vast quantities of textual data available, including online documents, articles, news, and reviews that contain long strings of text that need to be summarised [1].', '*e importance of text summarisation is due to several reasons, including the retrieval of significant information from a long text within a short period, easy and rapid loading of the most important information, and resolution of the problems associated with the criteria needed for summary evaluation [2].', 'Due to the evolution and growth of automatic text summarisation methods, which have provided significant results in many languages, these methods need to be reviewed and summarised.', '*erefore, in this review, we surveyed the most recent methods and focused on the techniques, datasets, evaluation measures, and challenges of each approach, in addition to the manner in which each method addressed challenges.', 'Applications such as search engines and news websites use text summarisation [1].', 'In search engines, previews are produced as snippets, and news websites generate headlines to describe the news to facilitate knowledge retrieval [3, 4].', 'Text summarisation can be divided into several categories based on function, genre, summary context, type of summarizer, and number of documents [5]; one specific text summarisation classification approach divides the summarisation process into extractive and abstractive categories [6].', 'Extractive summarisation extracts or copies some parts from the original text based on scores computed using either statistical features or linguistic features, while abstractive summarisation rephrases the original text to generate new phrases that may not be in the original text, which is Hindawi Mathematical Problems in Engineering Volume 2020, Article ID 9365340, 29 pages https://doi.', '1155/2020/9365340considered a difficult task for a computer.', 'As abstractive text summarisation requires an understanding of the document to generate the summary, advanced machine learning techniques and extensive natural language processing (NLP) are required.', '*us, abstractive summarisation is harder than extractive summarisation since abstractive summarisation requires real-word knowledge and semantic class analysis [7].', 'However, abstractive summarisation is also better than extractive summarisation since the summary is an approximate representation of a human-generated summary, which makes it more meaningful [8].', 'For both types, acceptable summarisation should have the following: sentences that maintain the order of the main ideas and concepts presented in the original text, minimal to no repetition, sentences that are consistent and coherent, and the ability to remember the meaning of the text, even for long sentences [7].', 'In addition, the generated summary must be compact while conveying important information about the original text [2, 9].', 'Abstractive text summarisation approaches include structured and semantic-based approaches.', 'Structured approaches encode the crucial features of documents using several types of schemas, including tree, ontology, lead and body phrases, and template and rule-based schemas, while semantic-based approaches are more concerned with the semantics of the text and thus rely on the information representation of the document to summarise the text.', 'Semantic-based approaches include the multimodal semantic method, information item method, and semantic graph-based method [10–17].', 'Deep learning techniques were employed in abstractive text summarisation for the first time in 2015 [18], and the proposed model was based on the encoder-decoder architecture.', 'For these applications, deep learning techniques have provided excellent results and have been extensively employed in recent years.', 'surveyed several abstractive text summarisation processes in general [19].', '*eir study differentiated between different model architectures, such as reinforcement learning (RL), supervised learning, and attention mechanism.', 'In addition, comparisons in terms of word embedding, data processing, training, and validation had been performed.', 'However, there are no comparisons of the quality of several models that generated summaries.', 'Furthermore, both extractive and abstractive summarisation models were summarised in [20, 21].', 'In [20], the classification of summarisation tasks was based on three factors: input factors, purpose factors, and output factors.', 'Dong and Mahajani et al.', 'surveyed only five abstractive summarisation models each.', 'On the other hand, Mahajani et al.', 'focused on the datasets and training techniques in addition to the architecture of several abstractive summarisation models [21].', 'However, the quality of the generated summary of the different techniques and the evaluation measures were not discussed.', 'presented a comprehensive survey of several abstractive text summarisation models, which are based on sequence-to-sequence encoder-decoder architecture for convolutional and RNN seq2seq models.', '*e focus was the structure of the network, training strategy, and the algorithms employed to generate the summary [22].', 'Although several papers have analysed abstractive summarisation models, few papers have performed a comprehensive study [23].', 'Moreover, most of the previous surveys covered the techniques until 2018, even though surveys were published in 2019 and 2020, such as [20, 21].', 'In this review, we addressed most of the recent deep learning-based RNN abstractive text summarisation models.', 'Furthermore, this survey is the first to address recent techniques applied in abstractive summarisation, such as Transformer.', '*is paper provides an overview of the approaches, datasets, evaluation measures, and challenges of deep learningbased abstractive text summarisation, and each topic is discussed and analysed.', 'We classified the approaches based on the output type into: single-sentence summary and multisentence summary approaches.', 'Also, within each classification, we compared between the approaches in terms of architecture, dataset, dataset preprocessing, evaluation, and results.', '*e remainder of this paper is organised as follows: Section 2 introduces a background of several deep learning models and techniques, such as the recurrent neural network (RNN), bidirectional RNN, attention mechanisms, long short-term memory (LSTM), gated recurrent unit (GRU), and sequenceto-sequence models.', 'Section 3 describes the most recent singlesentence summarisation approaches, while the multisentence summarisation approaches are covered in Section 4.', 'Section 5 and Section 6 investigate datasets and evaluation measures, respectively.', 'Section 7 discusses the challenges of the summarisation process and solutions to these challenges.', 'Conclusions and discussion are provided in Section 8.', 'Background Deep learning analyses complex problems to facilitate the decision-making process.', 'Deep learning attempts to imitate what the human brain can achieve by extracting features at different levels of abstraction.', 'Typically, higher-level layers have fewer details than lower-level layers [24].', '*e output layer will produce an output by nonlinearly transforming the input from the input layer.', '*e hierarchical structure of deep learning can support learning.', '*e level of abstraction of a certain layer will determine the level of abstraction of the next layer since the output of one layer will be the input of the next layer.', 'In addition, the number of layers determines the deepness, which affects the level of learning [25].', 'Deep learning is applied in several NLP tasks since it facilitates the learning of multilevel hierarchal representations of data using several data processing layers of nonlinear units [24, 26–28].', 'Various deep learning models have been employed for abstractive summarisation, including RNNs, convolutional neural networks (CNNs), and sequence-tosequence models.', 'We will cover deep learning models in more detail in this section.', 'RNN encoderdecoder architecture is based on the sequence-to-sequence model.', '*e sequence-to-sequence model maps the input sequence in the neural network to a similar sequence that consists of characters, words, or phrases.', '*is model is 2 Mathematical Problems in Engineeringutilised in several NLP applications, such as machine translation and text summarisation.', 'In text summarisation, the input sequence is the document that needs to be summarised, and the output is the summary [29, 30], as shown in Figure 1.', 'An RNN is a deep learning model that is applied to process data in sequential order such that the input of a certain state depends on the output of the previous state [31, 32].', 'For example, in a sentence, the meaning of a word is closely related to the meaning of the previous words.', 'An RNN consists of a set of hidden states that are learned by the neural network.', 'An RNN may consist of several layers of hidden states, where states and layers learn different features.', '*e last state of each layer represents the whole inputs of the layer since it accumulates the values of all previous states [5].', 'For example, the first layer and its state can be employed for part-of-speech tagging, while the second layer learns to create phrases.', 'In text summarisation, the input for the RNN is the embedding of words, phrases, or sentences, and the output is the word embedding of the summary [5].', 'In the RNN encoder-decoder model, at the encoder side, at certain hidden states, the vector representation of the current input word and the output of the hidden states of all previous words are combined and fed to the next hidden state.', 'As shown in Figure 1, the vector representation of the word W3 and the output of the hidden states he1 and he2 are combined and fed as input to the hidden states he3.', 'After feeding all the words of the input string, the output generated from the last hidden state of the encoder are fed to the decoder as a vector referred to as the context vector [29].', 'In addition to the context vector, which is fed to the first hidden state of the decoder, the start-of-sequence symbol 〈SOS〉 is fed to generate the first word of the summary from the headline (assume W5, as shown in Figure 1).', 'In this case, W5 is fed as the input to the next decoder hidden state.', 'Each generated word is passed as an input to the next decoder hidden state to generate the next word of the summary.', '*e last generated word is the end-of-sequence symbol 〈EOS〉.', 'Before generating the summary, each output from the decoder will take the form of a distributed representation before it is sent to the softmax layer and attention mechanism to generate the next summary [29].', 'Bidirectional RNN consists of forward RNNs and backward RNNs.', 'Forward RNNs generate a sequence of hidden states after reading the input sequence from left to right.', 'On the other hand, the backward RNNs generate a sequence of hidden states after reading the input sequence from right to left.', '*e representation of the input sequence is the concatenation of the forward and backward RNNs [33].', '*erefore, the representation of each word depends on the representation of the preceding (past) and following (future) words.', 'In this case, the context will contain the words to the left and the words to the right of the current word [34].', 'Using bidirectional RNN enhances the performance.', 'For example, if we have the following input text “Sara ate a delicious pizza at dinner tonight,” in this case, assume that we want to predict the representation of the word “dinner,” using bidirectional RNN and the forward LSTMs represent “Sara ate a delicious pizza at” while the backward LSTM represents “tonight.', '” Considering the word “tonight” when representing the word “dinner” provides better results.', 'On the other hand, using the bidirectional RNN at the decoder size minimizes the probability of the wrong prediction.', '*e reason for this is that the unidirectional RNN only considers the previous prediction and reason only about the past.', '*erefore, if there is an error in previous prediction, the error will accumulate in all subsequent predictions, and this problem can be addressed using the bidirectional RNN [35].', 'Gated Recurrent Neural Networks (LSTM and GRU).', 'Gated RNNs are employed to solve the problem of vanishing gradients, which occurs when training a long sequence using an RNN.', '*is problem can be solved by allowing the gradients to backpropagate along a linear path using gates, where each gate has a weight and a bias.', 'Gates can control and modify the amount of information that flows between hidden states.', 'During training, the weights and biases of the gates are updated.', '*e most popular gated RNNs are LSTM [36] and GRU [37], which are two variants of an RNN.', 'Long Short-Term Memory (LSTM).', '*e repeating unit of the LSTM architecture consists of input/read, memory/ update, forget, and output gates [5, 7], but the chaining structure is the same as that of an RNN.', '*e four gates share information with each other; thus, information can flow in loops for a long period of time.', '*e four gates of each LSTM unit, which are shown in Figures 2 and 3, are discussed here.', 'In the first timestep, the input is a vector that is initialised randomly, while in subsequent steps, the input of the current step is the output (content of the memory cell) of the previous step.', 'In all cases, the input is subject to element-wise multiplication with the output of the forget gate.', '*e multiplication result is added to the current memory gate output.', 'A forget gate is a neural network with one layer and a sigmoid activation function.', '*e value of the sigmoid function will determine if the information of the previous state should be forgotten or remembered.', 'If the sigmoid value is 1, then the previous state will be remembered, but if the sigmoid value is 0, then the previous state will be forgotten.', 'In language modelling, for example, the forget gate remembers the gender of the subject to produce the proper pronouns until it finds a new subject.', '*ere are four inputs for the forget gate: the output of the previous block, the input vector, the remembered information from the previous block, and the bias.', '*e memory gate controls the effect of the remembered information on the new information.', '*e memory gate consists of two neural networks.', '*e first network has the same structure as the forget gate but a different bias, and the second neural network has a tanh activation function and is utilised to generate the new Mathematical Problems in Engineering 3+ + + + Ct–1 X + Ct Ht –1 X σ t σ σ X Tanh Tanh X Ht Ht Input vector Memory from previous block Output of previous block Output of current block Element-wise multiplication Element-wise summation/ concatenation Memory from current block Sigmoid Hyperbolic tangent Bias 0 1 2 3 Figure 2: LSTM unit architecture [5].', 'he1 W1 W2 W3 W4 he2 he3 he4 hd1 <SOS> W5 W5 W6 W7 <EOS> W6 W7 hd2 hd3 hd4 Context vector Encoder Decoder Figure 1: Sequence-to-sequence; the last hidden state of the encoder is fed as input to the decoder with the symbol EOS [51].', '+ + + + X σ σ Ht Ct–1 + Ct Ht –1 X t 0 1 2 3 σ Tanh Tanh X Ht X (a) + + + + X σ σ Ht Ct–1 + Ct Ht –1 X t 0 1 2 3 σ Tanh Tanh X Ht X (b) Figure 3: Continued.', '4 Mathematical Problems in Engineeringinformation.', '*e new information is formed by adding the old information to the result of the element-wise multiplication of the output of the two memory gate neural networks.', '*e output gates control the amount of new information that is forwarded to the next LSTM unit.', '*e output gate is a neural network with a sigmoid activation function that considers the input vector, the previous hidden state, the new information, and the bias as input.', '*e output of the sigmoid function is multiplied by the tanh of the new information to produce the output of the current block.', 'Gated Recurrent Unit (GRU).', 'A GRU is a simplified LSTM with two gates, a reset gate and an update gate, and there is no explicit memory.', '*e previous hidden state information is forgotten when all the reset gate elements approach zero; then, only the input vector affects the candidate hidden state.', 'In this case, the update gate acts as a forget gate.', 'LSTM and GRU are commonly employed for abstractive summarisation since LSTM has a memory unit that provides extra control; however, the computation time of the GRU is reduced [38].', 'In addition, while it is easier to tune the parameters with LSTM, the GRU takes less time to train [30].', '*e attention mechanism was employed for neural machine translation [33] before being utilised for NLP tasks such as text summarisation [18].', 'A basic encoder-decoder architecture may fail when given long sentences since the size of encoding is fixed for the input string; thus, it cannot consider all the elements of a long input.', 'To remember the input that has a significant impact on the summary, the attention mechanism was introduced [29].', '*e attention mechanism is employed at each output word to calculate the weight between the output word and every input word; the weights add to one.', '*e advantage of using weights is to show which input word must receive attention with respect to the output word.', '*e weighted average of the last hidden layers of the decoder in the current step is calculated after passing each input word and fed to the softmax layer along the last hidden layers [39].', 'Beam search and greedy search are very similar; however, while greedy search considers only the best hypothesis, beam search considers b hypotheses, where b represents the beam width or beam size [5].', 'In text summarisation tasks, the decoder utilises the final encoder representation to generate the summary from the target vocabulary.', 'In each step, the output of the decoder is a probability distribution over the target word.', '*us, to obtain the output word from the learned probability, several methods can be applied, including (1) greedy sampling, which selects the distribution mode, (2) 1-best or beam search, which selects the best output, and (3) n-best or beam search, which select several outputs.', 'When n-best beam search is employed, the top b most relevant target words are selected from the distribution and fed to the next decoder state.', '*e decoder keeps only the top k1 of k words from the different inputs and discards the rest.', 'Distributed Representation (Word Embedding).', 'A word embedding is a word distributional vector representation that represents the syntax and semantic features of words [40].', 'Words must be converted to vectors to handle various NLP challenges such that the semantic similarity between words can be calculated using cosine similarity, Euclidean distance, etc.', 'In NLP tasks, the word embeddings of the words are fed as inputs to neural network models.', 'In the recurrent neural network encoder-decoder architecture, which is employed to generate the summaries, the input of the model is the word embedding of the text, and the output is the word embedding of the summary.', 'In NLP, there are several word embedding models, such as Word2Vec, GloVe, FastText, and Bidirectional Encoder Representations from Transformers (BERT), which are the most recently employed word embedding models [41, 44–47].', '*e Word2Vec model consists of two approaches, skip-gram and continuous bag-of-words (CBOW), which both depend on the context window [41].', 'On the other hand, GloVe represents the + + + + X σ σ Ht Ct-1 + Ct Ht -1 X t 0 1 2 3 σ Tanh Tanh X H t X (c) + + + + X σ σ Ht Ct–1 + Ct H t–1 X t 0 1 2 3 σ Tanh Tanh X Ht X (d) Figure 3: LSTM unit gates [5]: (a) input gate; (b) forget gate; (c) memory gate; (d) output gate.', 'Mathematical Problems in Engineering 5global vector, which is based on statistics of the global corpus instead of the context window [44].', 'FastText extends the skipgram of the Word2Vec model by using the subword internal information to address the out-of-vocabulary (OOV) terms [46].', 'In FastText, the subword components are composed to build the vector representation of the words, which facilitates representation of the word morphology and lexical similarity.', '*e BERT word embedding model is based on a multilayer bidirectional transformer encoder [47, 48].', 'Instead of using sequential recurrence, the transformer neural network utilises parallel attention layers.', 'BERTcreates a single large transformer by combining the representations of the words and sentences.', 'Furthermore, BERT is pretrained with an unsupervised objective over a large amount of text.', '*e contextual representations of language are learned from large corpora.', 'One of the new language representations, which extend word embedding models, is referred to as BERTmentioned in the previous section [48].', 'In BERT, two tokens are inserted to the text.', '*e first token (CLS) is employed to aggregate the whole text sequence information.', '*e second token is (SEP); this token is inserted at the end of each sentence to represent it.', '*e resultant text consists of tokens, where each token is assigned three types of embeddings: token, segmentation, and position embeddings.', 'Token embedding is applied to indicate the meaning of a token.', 'Segmentation embedding identifies the sentences, and position embedding determines the position of the token.', '*e sum of the three embeddings is fed to the bidirectional transformer as a single vector.', 'Pretrained word embedding vectors are more precise and rich with semantic features.', 'BERT has the advantage of fine-tuning (based on the objectives of certain tasks) and feature-based methods.', 'Moreover, transformers compute the presentation of the input and output by using self-attention, where the self-attention enables the learning of the relevance between the “word-pair” [47].', 'Single-Sentence Summary Recently, the RNN has been employed for abstractive text summarisation and has provided significant results.', '*erefore, we focus on abstractive text summarisation based on deep learning techniques, especially the RNN [49].', 'We discussed the approaches that have applied deep learning for abstractive text summarisation since 2015.', 'RNN with an attention mechanism was mostly utilised for abstractive text summarisation.', 'We classified the research according to summary type (i.', ', single-sentence or multisentence summary), as shown in Figure 4.', 'We also compared the approaches in terms of encoder-decoder architecture, word embedding, dataset and dataset preprocessing, and evaluations and results.', '*is section covers single-sentence summary methods, while Section 4 covers multisentence summary methods.', 'Single-sentence summary methods include a neural attention model for abstractive sentence summarisation [18], abstractive sentence summarisation with attentive RNN (RAS) [39], quasi-RNN [50], a method for generating news headlines with RNNs [29], abstractive text summarisation using an attentive sequence-to-sequence RNN [38], neural text summarisation [51], selective encoding for abstractive sentence summarisation (SEASS) [52], faithful to the original: fact aware neural abstractive summarization (FTSumg) [53], and the improving transformer with sequential context [54].', 'Abstractive Summarization Architecture 3.', 'Neural networks were first employed for abstractive text summarisation by Rush et al.', 'in 2015, where a local attention-based model was utilised to generate summary words by conditioning it to input sentences [18].', '*ree types of encoders were applied: the bag-ofwords encoder, the convolution encoder, and the attentionbased encoder.', '*e bag-of-words model of the embedded input was used to distinguish between stop words and content words; however, this model had a limited ability to represent continuous phrases.', '*us, a model that utilised the deep convolutional encoder was employed to allow the words to interact locally without the need for context.', '*e convolutional encoder model can alternate between temporal convolution and max-pooling layers using the standard time-delay neural network (TDNN) architecture; however, it is limited to a single output representation.', '*e limitation of the convolutional encoder model was overcome by the attention-based encoder.', '*e attention-based encoder was utilised to exploit the learned soft alignment to weight the input based on the context to construct a representation of the output.', 'Furthermore, the beam-search decoder was applied to limit the number of hypotheses in the summary.', 'RNN Encoder-Decoder Architecture (1) LSTM-RNN.', 'An abstractive sentence summarisation model that employed a conditional recurrent neural network (RNN) to generate the summary from the input is referred to as a recurrent attentive summariser (RAS) [39].', 'A RAS is an extension of the work in [18].', 'In [18], the model employed a feedforward neural network, while the RAS employed an RNNLSTM.', '*e encoder and decoder in both models were trained using sentence-summary pair datasets, but the decoder of the RAS improved the performance since it considered the position information of the input words.', 'Furthermore, previous words and input sentences were employed to produce the next word in the summary during the training phase.', 'Lopyrev [29] proposed a simplified attention mechanism that was utilised in an encoder-decoder RNN to generate headlines for news articles.', '*e news article was fed into the encoder one word at a time and then passed through the embedding layer to generate the word representation.', '*e experiments were conducted using simple and complex attention mechanisms.', 'In the simple attention mechanism, the last layer after processing the input in the encoding was divided into two parts: one part for calculating the attention weight vector, and one part for calculating the context vector, as shown in Figure 5(a).', 'However, in the complex attention mechanism, the last layer was employed to calculate the attention weight vector and context vector without 6 Mathematical Problems in Engineeringfragmentation, as shown in Figure 5(b).', 'In both figures, the solid lines indicate the part of the hidden state of the last layer that is employed to compute the context vector, while the dashed lines indicate the part of the hidden state of the last layer that is applied to compute the attention weight vector.', '*e same difference was seen on the decoder side: in the simple attention mechanism, the last layer was divided into two parts (one part was passed to the softmax layer, and the other part was applied to calculate the attention weight), while in the complex attention mechanism, no such division was made.', 'A beam search at the decoder side was performed during testing to extend the sequence of the probability.', 'Abstractive summarization apporaches based on recurrent neural network + attention mechanism Single-sentence summary A neural attention model for abstractive sentence summarization (ABS) Abstractive sentence summarization with attentive recurrent neural networks (RAS) Quasi-recurrent neural network (QRNN) + CNN Generating news headlines with recurrent neural networks Abstractive text summarization using attentive sequence-to-sequence RNNs Selective encoding for abstractive sentence summarization (SEASS) Faithful to the original: fact aware neural abstractive summarization (FTSumg) Improving transformer with sequential context representations for abstractive text summarization (RCT) Multisentence summary Get to the point: summarization with pointer-generator networks Reinforcement learning (RL) Generative adversarial network for abstractive text summarization Exploring semantic phrases (ATSDL) + CNN Bidirectional attentional encoder-decoder model and bidirectional beam search Key information guide network Improving abstraction in text summarization Dual encoding for abstractive text summarization (DEATS) Bidirectional decoder (BiSum) Text summarization with pretrained encoders A text abstraction summary model based on BERT word embedding and reinforcement learning Transformer-based model for single-document neural summarization Text summarization method based on double attention pointer network (DAPT) Figure 4: Taxonomy of several approaches that use a recurrent neural network and attention mechanism in abstractive text summarisation based on the summary type.', 'Mathematical Problems in Engineering 7*e encoder-decoder RNN and sequence-to-sequence models were utilised in [55], which mapped the inputs to the target sequences; the same approach was also employed in [38, 51].', '*ree different methods for global attention were proposed for calculating the scoring functions, including dot product scoring, the bilinear form, and the scalar value calculated from the projection of the hidden states of the RNN encoder [38].', '*e model applied LSTM cells instead of GRU cells (both LSTM and GRU are commonly employed for abstractive summarisation tasks since LSTM has a memory unit that provides control but the computation time of GRU is lower).', '*ree models were employed: the first model applied unidirectional LSTM in both the encoder and the decoder; the second model was implemented using bidirectional LSTM in the encoder and unidirectional LSTM in the decoder; and the third model utilised a bidirectional LSTM encoder and an LSTM decoder with global attention.', '*e first hidden state of the decoder is the concatenation of all backward and forward hidden states of the encoder.', '*e use of attention in an encoder-decoder neural network generates a context vector at each timestep.', 'For the local attention mechanism, the context vector is conditioned on a subset of the encoder’s hidden states, while for the global attention mechanism, the vector is conditioned on all the encoder’s hidden states.', 'After generating the first decoder output, the next decoder input is the word embedding of the output of the previous decoder step.', '*e affine transformation is used to convert the output of the decoder LSTM to a dense vector prediction due to the long training time needed before the number of hidden states is the same as the number of words in the vocabulary.', 'Khandelwal [51] employed a sequence-to-sequence model that consists of an LSTM encoder and LSTM decoder for abstractive summarisation of small datasets.', '*e decoder generated the output summary after reading the hidden representations generated by the encoder and passing them to the softmax layer.', '*e sequence-to-sequence model does not memorize information, so generalization of the model is not possible.', '*us, the proposed model utilised imitation learning to determine whether to choose the golden token (i.', ', reference summary token) or the previously generated output at each step.', 'A combination of the elements of the RNN and convolutional neural network (CNN) was employed in an encoder-decoder model that is referred to as a quasi-recurrent neural network (QRNN) [50].', 'In the QRNN, the GRU was utilised in addition to the attention mechanism.', '*e QRNN was applied to address the limitation of parallelisation, which aimed to obtain the dependencies of the words in previous steps via convolution and “fo-pooling,” which were performed in parallel, as shown in Figure 6.', '*e convolution in the QRNN can be either mass convolution (considering previous timesteps only) or centre convolution (considering future timesteps).', '*e encoder-decoder model employed two neural networks: the first network applied the centre convolution of QRNN and consisted of multiple hidden layers that were fed by the vector representation of the words, and the second network comprised neural attention and considered as input the encoder hidden layers to generate one word of a headline.', '*e decoder accepted the previously generated headline word and produced the next word of the headline; this process continued until the headline was completed.', 'SEASS is an extension of the sequence-to-sequence recurrent neural network that was proposed in [52].', '*e selective encoding for the abstractive sentence summarisation (SEASS) approach includes a selective encoding model that consists of an encoder for sentences, a selective gate network, and a decoder with an attention mechanism, as shown in Figure 7.', '*e encoder uses a bidirectional GRU, while the decoder uses a unidirectional GRU with an attention mechanism.', '*e encoder reads the input words and their representations.', '*e meaning of the sentences is applied by the selective gate to choose the word representations for generating the word representations of the sentence.', 'To produce an excellent summary and accelerate the decoding process, a beam search was selected as the decoder.', 'On the other hand, dual attention was applied in [53].', '*e proposed dual attention approach consists of three modules: two bidirectional GRU encoders and one dual attention decoder.', '*e decoder has a gate network for context selection, as shown in Figure 8, and employs copying and coverage mechanisms.', '*e outputs of the encoders are two context vectors: one context vector for sentences and one context vector for the relation, where the relation may be Context Attention weight Word1 Word2 Word3 <EOS> Headline (a) Context Word1 Word2 Word3 <EOS> Headline Attention weight (b) Figure 5: (a) Simple attention and (b) complex attention [29].', '8 Mathematical Problems in Engineering---- ---- ---- ---- ---- ---- → ---- ---- ---- ---- ---- ---- → Convolutional Max-pool Convolutional Max-pool Convolutional fo-pool Convolutional fo-pool LSTM CNN QRNN Linear LSTM/ linear Linear LSTM/ linear Figure 6: Comparison of the CNN, LSTM, and QRNN models [50].', 'Encoder Word1 Word2 Word4 Word3 Word5 MLP h i S Selective gate network Attention Ct Somax Yt Maxout GRU S t C t–1 St–1 Yt –1 Decoder Figure 7: Selective encoding for abstractive sentence summarisation (SEASS) [52].', 'Sentence encoder Word1 Word2 Word4 Word3 Word5 Attention Somax Yt GRU S t C t–1 St–1 C xt C t r Context selection MLP Rel1 Rel2 Rel4 Rel3 Rel5 Attention Relation encoder C xt C t r C t Dual attention decoder Yt –1 Figure 8: Faithful to the original [53].', 'Mathematical Problems in Engineering 9a triple or tuple relation.', 'A triple relation consists of the subject, predicate, and object, while the tuple relation consists of either (subject and predicate) or (predicate and subject).', 'Sometimes the triple relation cannot be extracted; in this case, two tuple relations are utilised.', '*e decoder gate merges both context vectors based on their relative association.', '*e long-sequence poor semantic representation of abstractive text summarisation approaches, which are based on an RNN encoder-decoder framework, was addressed using the RC-Transformer (RCT) [54].', 'An RCT is an RNN-based abstractive text summarisation model that is composed of two encoders (RC encoder and transformer encoder) and one decoder.', '*e transformer shows an advantage in parallel computing in addition to retrieving the global context semantic relationships.', 'On the other hand, sequential context representation was achieved by a second encoder of the RCT-Transformer.', 'Word ordering is very crucial for abstractive text summarisation, which cannot be obtained by positioning encoding.', '*erefore, an RCT utilised two encoders to address the problem of a shortage of sequential information at the word level.', 'A beam search was utilised at the decoder.', 'Furthermore, Cai et al.', 'compared the speed of the RCT model and that of the RNN-based model and concluded that the RCT is 1.', 'In the QRNN model, GloVe word embedding, which was pretrained using the Wikipedia and Gigaword datasets, was performed to represent the text and summary [50].', 'In the first model, the proposed model by Jobson et al.', ', the word embedding, randomly initialised and updated during training, while GloVe word embedding was employed to represent the words in the second and third models [38].', 'In a study by Cai et al.', ', Transformer was utilised [54].', 'Dataset and Dataset Preprocessing.', 'In the model that was proposed by Rush et al.', ', datasets were preprocessed via PTB tokenization by using “#” to replace all digits, conversion of all letters to lowercase letters, and the use of “UNK” to replace words that occurred fewer than 5 times [18].', '*e model was trained with any input-output pairs due to the shortage of constraints for generating the output.', '*e training process was carried out on the Gigaword datasets, while the summarisation evaluation was conducted on DUC2003 and DUC2004 [18].', 'Furthermore, the proposed model by Chopra et al.', 'was trained using the Gigaword corpus with sentence separation and tokenisation [39].', 'To form sentence-summary pairs, each headline of the article was paired with the first sentence of the article.', '*e same preprocessing steps of the data in [18] were performed in [39].', 'Moreover, the Chopra et al.', 'model was evaluated using the DUC2004 dataset, which consists of 500 pairs.', 'Gigaword datasets were also employed by the QRNN model [50].', 'Furthermore, articles that started with sentences that contained more than 50 words or headlines with more than 25 words were removed.', 'Moreover, the words in the articles and their headlines were converted to lowercase words, and the data points were split into short, medium, and long sentences, based on the lengths of the sentences, to avoid extra padding.', 'Lopyrev and Jobson et al.', 'trained the model using Gigaword after processing the data.', 'In the Lopyrev model, the most crucial preprocessing steps for both the text and the headline were tokenisation and character conversion to lowercase [29].', 'In addition, only the characters of the first paragraph were retained, and the length of the headline was fixed between 25 and 50 words.', 'Moreover, the no-headline articles were disregarded, and the 〈unk〉 symbol was used to replace rare words.', 'Khandelwal employed the Association for Computational Linguistics (ACL) Anthology Reference Corpus, which consists of 16,845 examples for training and 500 examples for testing, and they were considered small datasets, in experiments [51].', '*e abstract included the first three sentences, and the unigram that overlaps between the title and the abstract was also calculated.', '*ere were 25 tokens in the summary, and there were a maximum of 250 tokens in the input text.', '*e English Gigaword dataset, DUC2004 corpus, and MSR-ATC were selected to train and test the SEASS model [52].', 'Moreover, the experiments of the Cao et al.', 'model were conducted using the Gigaword dataset [53].', '*e same preprocessing steps of the data in [18] were performed in [52, 53].', 'Moreover, RCT also employed the Gigaword and DUC2004 datasets in experiments [54].', 'Recall-Oriented Understudy for Gisting Evaluation 1 (ROUGE1), ROUGE2, and ROUGE-L were utilised to evaluate the Rush et al.', 'model, and values of 28.', '81, respectively, were obtained [18].', '*e experimental results of the Chopra et al.', 'model showed that although DUC2004 was too complex for the experiments, on the Gigaword corpus, the proposed model outperformed state-of-the-art methods in terms of ROUGE1, ROUGE2, and ROUGE-L [39].', '*e values of ROUGE1, ROUGE2, and ROUGE-L were 28.', 'On the other hand, BLEU was employed to evaluate the Lopyrev model [29], while Khandelwal utilised perplexity [51].', '*e SEASS model was evaluated using ROUGE1, ROUGE2, and ROUGE-L, and the results of the three measures were 36.', 'Moreover, ROUGE1, ROUGE2, and ROUGE-L were selected for evaluating the Cao et al.', '*e values of ROUGE1, ROUGE2, and ROUGE-L were 37.', '24, respectively, and the results showed that fake summaries were reduced by 80%.', 'In addition, the RCT was evaluated using ROUGE1, ROUGE2, and ROUGE-L with values 37.', '62 compared with the Gigaword dataset.', '*e results showed that the RCT model outperformed other models by generating a high-quality summary that contains silent information [54].', '10 Mathematical Problems in Engineering4.', 'Multisentence Summary In this section, multisentence summary and deep learningbased abstractive text summarisation are discussed.', 'Multisentence summary methods include the get to the point method (summarisation with pointer-generator networks) [56], a deep reinforced model for abstractive summarization (RL) [57], generative adversarial network for abstractive text summarization [58], semantic phrase exploration (ATSDL) [30], bidirectional attention encoder-decoder and bidirectional beam search [35], key information guide network [59], text summarisation abstraction improvement [60], dual encoding for abstractive text summarisation (DEATS) [61], and abstractive document summarisation via bidirectional decoder (BiSum) [62], the text abstraction summary model based on BERT word embedding and RL [63], transformerbased model for single documents neural summarisation [64], text summarisation with pretrained encoders [65], and text summarisation method based on the double attention pointer network [49].', '*e pointer-generator [55] includes singlesentence and multisentence summaries.', 'Additional details are presented in the following sections.', 'Abstractive Summarization Architecture 4.', 'A novel abstractive summarisation method was proposed in [56]; it generated a multisentence summary and addressed sentence repetition and inaccurate information.', 'proposed a model that consists of a singlelayer bidirectional LSTM encoder, a single-layer unidirectional LSTM decoder, and the sequence-to-sequence attention model proposed by [55].', '*e See et al.', 'model generates a long text summary instead of headlines, which consists of one or two sentences.', 'Moreover, the attention mechanism was employed, and the attention distribution facilitated the production of the next word in the summary by telling the decoder where to search in the source words, as shown in Figure 9.', '*is mechanism constructed the weighted sum of the hidden state of the encoder that facilitated the generation of the context vector, where the context vector is the fixed size representation of the input.', '*e probability (Pvocab) produced by the decoder was employed to generate the final prediction using the context vector and the decoder’s last step.', 'Furthermore, the value of Pvocab was equal to zero for OOV words.', 'RL was employed for abstractive text summarisation in [57].', '*e proposed method in [57], which combined RL with supervised word prediction, was composed of a bidirectional LSTM-RNN encoder and a single LSTM decoder.', 'Two models—generative and discriminative models—were trained simultaneously to generate abstractive summary text using the adversarial process [58].', '*e maximum likelihood estimation (MLE) objective function employed in previous sequence-sequence models suffers from two problems: the difference between the training loss and the evaluation metric, and the unavailability of a golden token at testing time, which causes errors to accumulate during testing.', 'To address the previous problems, the proposed approach exploited the adversarial framework.', 'In the first step of the adversarial framework, reinforcement learning was employed to optimize the generator, which generates the summary from the original text.', 'In the second step, the discriminator, which acts as a binary classifier, classified the summary as either a ground-truth summary or a machine-generated summary.', '*e bidirectional LSTM encoder and attention mechanism were employed, as shown in [56].', 'Abstract text summarisation using the LSTM-CNN model based on exploring semantic phrases (ATSDL) was proposed in [30].', 'ATSDL is composed of two phases: the first phase extracts the phrases from the sentences, while the second phase learns the collocation of the extracted phrases using the LSTM model.', 'To generate sentences that are general and natural, the input and output of the ATSDL model were phrases instead of words, and the phrases were divided into three main types, i.', ', subject, relation, and object phrases, where the relation phrase represents the relation between the input phrase and the output phrase.', '*e phrase was represented using a CNN layer.', '*ere are two main reasons for choosing the CNN: first, the CNN was efficient for sentence-level applications, and second, training was efficient since long-term dependency was unnecessary.', 'Furthermore, to obtain several vectors for a phrase, multiple kernels with different widths that represent the dimensionality of the features were utilised.', 'Within each kernel, the maximum feature was selected for each row in the kernel via maximum pooling.', '*e resulting values were added to obtain the final value for each word in a phrase.', 'Bidirectional LSTM was employed instead of a GRU on the encoder side since parameters are easy to tune with LSTM.', 'Moreover, the decoder was divided into two modes: a generate mode and a copy mode.', '*e generate mode generated the next phrase in the summary based on previously generated phrases and the hidden layers of the input on the encoder side, while the copy mode copied the phrase after the current input phrase if the current generated phrase was not suitable for the previously generated phrases in the summary.', 'Figure 10 provides additional details.', 'Bidirectional encoder and decoder LSTM-RNNs were employed to generate abstractive multisentence summaries [35].', '*e proposed approach considered past and future context on the decoder side when making a prediction as it employed a bidirectional RNN.', 'Using a bidirectional RNN on the decoder side addressed the problem of summary imbalance.', 'An unbalanced summary could occur due to noise in a previous prediction, which will reduce the quality of all subsequent summaries.', '*e bidirectional decoder consists of two LSTMs: the forward decoder and the backward decoder.', '*e forward decoder decodes the information from left to right, while the backward decoder decodes the information from right to left.', '*e last hidden state of the forward decoder is fed as the initial input to the backward decoder, and vice versa.', 'Moreover, the researcher proposed a bidirectional beam-search method that generates summaries from the proposed bidirectional model.', 'Bidirectional beam search combined information from the past and future to produce a better summary.', '*erefore, the Mathematical Problems in Engineering 11output summary was balanced by considering both past and future information and by using a bidirectional attention mechanism.', 'In addition, the input sequence was read in reverse order based on the conclusion that LSTM learns better when reading the source in reverse order while remembering the order of the target [66, 67].', 'A softmax layer was employed on the decoder side to obtain the probability of each target word in the summary over the vocabulary distribution by taking the output of the decoder as input for the softmax layer.', '*e decoder output depends on the internal representation of the encoder, i.', ', the context vector, the current hidden state of the decoder, and the summary words previously generated by the decoder hidden states.', '*e objective of training is to maximise the probability of the alignment between the sentence and the summary from both directions.', 'During training, the input of the forward decoder is the previous reference summary token.', 'However, during testing, the input of the forward decoder is the token generated in the previous step.', '*e same situation is true for the backward decoder, where the input during training is the future token from the summary.', 'Nevertheless, the bidirectional decoder has difficulty during testing since the complete summary must be known in advance; thus, the full backward decoder was generated and fed to the forward decoder using a unidirectional backward beam search.', 'A combination of abstractive and extractive methods was employed in the guiding generation model proposed by [59].', '*e extractive method generates keywords that are encoded by a key information guide network (KIGN) to represent key information.', 'Furthermore, to predict the final summary of the long-term value, the proposed method applied a prediction guide mechanism [68].', 'A prediction Word1 Word2 Word3 Word4 Word5 Word6 Word7 Context vector …….', 'Attention distribution Encoder hidden states <Start>Word_Sum1 …….', 'Decoder hidden states Vocabulary distribution Figure 9: Baseline sequence-to-sequence model with attention mechanism [56].', 'Phrase encoder Phrases Hidden states Encoder Decoder Phrase vector Words in a phrase Convolution Max-pooling Figure 10: Semantic-unit-based LSTM model [30].', '12 Mathematical Problems in Engineeringguide mechanism is a feedforward single-layer neural network that predicts the key information of the final summary during testing.', '*e encoder-decoder architecture baseline of the proposed model is similar to that proposed by Nallapati et al.', '[55], where both the bidirectional LSTM encoder and the unidirectional LSTM decoder were employed.', 'Both models applied the attention mechanism and softmax layer.', 'Moreover, the process of generating the summary was improved by proposing KIGN, which considers as input the keywords extracted using the TextRank algorithm.', 'In KIGN, key information is represented by concatenating the last forward hidden state and first backward hidden state.', 'KIGN employs the attention mechanism and pointer mechanism.', 'In general, the attention mechanism hardly identifies the keywords; thus, to identify keywords, the output of KIGN will be fed to the attention mechanism.', 'As a result, the attention mechanism will be highly affected by the keywords.', 'However, to enable the pointer network to identify the keywords, which are the output of KIGN, the encoder context vector and hidden state of the decoder will be fed to the pointer network, and the output will be employed to calculate the soft switch.', '*e soft switch determines whether to copy the target from the original text or generate it from the vocabulary of the target, as shown in Figure 11.', '*e level of abstraction in the generated summary of the abstractive summarisation models was enhanced via the two techniques proposed in [60]: decoder decomposition and the use of a novel metric for optimising the overlap between the n-gram summary and the ground-truth summary.', '*e decoder was decomposed into a contextual network and pretrained language model, as shown in Figure 12.', '*e contextual network applies the source document to extract the relevant parts, and the pretrained language model is generated via prior knowledge.', '*is decomposition method facilitates the addition of an external pretrained language model that is related to several domains.', 'Furthermore, a novel metric was employed to generate an abstractive summary by including words that are not in the source document.', 'Bidirectional LSTM was utilised in the encoder, and the decoder applied 3-layer unidirectional weightdropped LSTM.', 'In addition, the decoder utilised a temporal attention mechanism, which applied the intra-attention mechanism to consider previous hidden states.', 'Furthermore, a pointer network was introduced to alternate between copying the output from the source document and selecting it from the vocabulary.', 'As a result, the objective function combined between force learning and maximum likelihood.', 'A bidirectional decoder with a sequence-to-sequence architecture, which is referred to as BiSum, was employed to minimise error accumulation during testing [62].', 'Errors accumulate during testing as the input of the decoder is the previously generated summary word, and if one of the generated word summaries is incorrect, then the error will propagate through all subsequent summary words.', 'In the bidirectional decoder, there are two decoders: a forward decoder and a backward decoder.', '*e forward decoder generates the summary from left to right, while the backward decoder generates the summary from right to left.', '*e forward decoder considers a reference from the backward decoder.', 'However, there is only a single-layer encoder.', '*e encoder and decoder employ an LSTM unit, but while the encoder utilises bidirectional LSTM, the decoders use unidirectional LSTM, as shown in Figure 13.', 'To understand the summary generated by the backward decoder, the attention mechanism is applied in both the backward decoder and the encoder.', 'Moreover, to address the problem of out-of-vocabulary words, an attention mechanism is employed in both decoders.', 'A double attention pointer network, which is referred to as (DAPT), was applied to generate an abstractive text summarisation model [49].', '*e encoder utilised bidirectional LSTM, while the decoder utilised unidirectional LSTM.', '*e encoder key features were extracted using a selfattention mechanism.', 'At the decoder, the beam search was employed.', 'Moreover, more coherent and accurate summaries were generated.', '*e repetition problem was addressed using an improved coverage mechanism with a truncation parameter.', '*e model was optimised by generating a training model that is based on RL and scheduled sampling.', 'Dual encoding using a sequence-to-sequence RNN was proposed as the DEATS method [61].', '*e dual encoder consists of two levels of encoders, i.', ', primary and secondary encoders, in addition to one decoder, and all of them employ a GRU.', '*e primary encoder considers coarse encoding, while the secondary encoder considers fine encoding.', '*e primary encoder and decoder are the same as the standard encoder-decoder model with an attention mechanism, and the secondary encoder generates a new context vector that is based on previous output and input.', 'Moreover, an additional context vector provides meaningful information for the output.', '*us, the repetition problem of the generated summary that was encountered in previous approaches is addressed.', '*e semantic vector is generated on both levels of encoding: in the primary encoder, the semantic vector is generated for each input, while in the secondary encoder, the semantic vector is recalculated after the importance of each input word is calculated.', '*e fixed-length output is partially generated at each stage in the decoder since it decodes in stages.', 'Figure 14 elaborates the DEATS process.', '*e primary encoder produces a hidden state hpj for each input j and content representation cp.', 'Next, the decoder decodes a fixedlength output, which is referred to as the decoder content representation cd.', '*e weight αj can be calculated using the hidden states hp j and the content representations cp and cd.', 'In this stage, the secondary encoder generates new hidden states or semantic context vectors hs m, which are fed to the decoder.', 'Moreover, DEATS uses several advanced techniques, including a pointer-generator, copy mechanism, and coverage mechanism.', 'proposed a hybrid extractive-abstractive text summarisation model, which is based on combining the reinforcement learning with BERT word embedding [63].', 'In this hybrid model, a BERTfeature-based strategy was used to Mathematical Problems in Engineering 13generate contextualised token embedding.', '*is model consists of two submodels: abstractive agents and extractive agents, which are bridged using RL.', 'Important sentences are extracted using the extraction model and rewritten using the abstraction model.', 'A pointer-generator network was utilised to copy some parts of the original text, where the sentencelevel and word-level attentions are combined.', 'In addition, a beam search was performed at the decoder.', 'In abstractive and extractive models, the encoder consists of a bidirectional GRU, while the decoder consists of a unidirectional GRU.', '*e training process consists of pretraining and full training phases.', 'proposed to use sequence-to-sequence and transformer models to generate abstractive summaries [64].', '*e proposed summarisation model consists of two modules: an extractive model and an abstractive model.', '*e encoder transformer has the same architecture shown in [48]; however, instead of receiving the document representation as input, it receives sentence-level representation.', '*e architecture of the abstractive model consists of a singleWord1Word2Word3Word4Word5Word6Word7 …….', 'Encoder hidden states Word_ <Start> Sum1 Word_ Sum2 …….', '+ Word_ <Start> Sum1 Word_ Sum2 + …….', 'Decoder Language model Contextual model h t dec c t temp c t int Fusion layer Figure 12: Decoder decomposed into a contextual model and a language model [60].', 'Word1Word2 Word3 Word4 Word5 Word6 Word7 Attention …….', 'Encoder hidden states <Start> Word_Sum1 …….', 'Decoder hidden states Key Word1 K Key Word2 Key Word3 Key Word4 …….', 'Key information guide network Pointer Somax S t C t Figure 11: Key information guide network [59].', '14 Mathematical Problems in Engineeringlayer unidirectional GRU at the encoder and single-layer unidirectional GRU at the decoder.', '*e input of the encoder is the output of the transformer.', 'A beam search was performed during inference at the decoder, while greedydecoding was employed during training and validation.', 'BERT is employed to represent the sentences of the document to express its semantic [65].', 'proposed abstractive and extractive summarisation models that are based on encoder-decoder architecture.', '*e encoder used a BERT pretrained document-level encoder, while the decoder utilised a transformer that is randomly initialised and trained from scratch.', 'In the abstractive model, the optimisers of the encoder and decoder are separated.', 'Moreover, two stages of fine-tuning are utilised at the encoder: one stage in extractive summarisation and one stage in abstractive summarisation.', 'At the decoder side, a beam search was performed; however, the coverage and copy mechanisms were not employed since these two mechanisms need additional tuning of the hyperparameters.', '*e repetition problem was addressed by producing different summaries by using trigram-blocking.', '*e OOV words rarely appear in the generated summary.', '*e word embedding of the input for the See et al.', 'model was learned from scratch instead of using a pretrained word embedding model [56].', 'On the other hand, both the input and output tokens applied the same embedding matrix Wemb, which was generated using the GloVe word embedding model in the Paulus et al.', 'Another word embedding matrix referred to as Wout was applied in the token generation layer.', 'Additionally, a sharing weighting matrix was employed by both the shared embedding matrix Wemb and the Wout matrix.', '*e sharing weighting matrixes improved the process of generating tokens since they considered the embedding syntax and semantic information.', 'Word_ Sum1 <Start> Word_ Sum2 Word1Word2Word3Word4Word5Word6Word7 Primary encoder hidden states Word1Word2Word3Word4 Word5 Word6 Word7 Secondary encoder hidden states Decoder hidden states Cp Cd α1 α2 α3 α4 α5 α6 α7 h s1 h s2 h s3 h s4 h s5 h s6 h s7 h p1 h p2 h p3 h p4 h p5 h p6 h p7 Figure 14: Dual encoding model [61].', 'Word1 Word2 Word3 Word4 Word5 Word6 Word7 …….', 'Encoder Word_ <Start> Sum1 Word_ Sum2 Forward decoder …….', 'Context vector Word_ Sumn-1 Word_ Sumn Backward decoder …….', 'Context vector Context vector Figure 13: Abstractive document summarisation via bidirectional decoder (BiSum) [62].', 'Mathematical Problems in Engineering 15*e discriminator input sequence of the Liu et al.', 'model was encoded using a maximum pooling CNN, where the result was passed to the softmax layer [58].', 'On the other hand, the word embedding that was applied in the Al-Sabahi et al.', 'model was learned from scratch using the CNN/Daily Mail datasets with 128 dimensions [35].', '[64] used pretrained GloVe word embedding.', 'BERT word embedding was utilised in the models proposed by Wang et al.', '[63] and Liu et al.', 'Dataset and Dataset Preprocessing.', 'Experiments were conducted with the See et al.', '[56], Al-Sabahi et al.', '[35], and Li et al.', '[59] models using CNN/Daily Mail datasets, which consist of 781 tokens paired with 56 tokens on average; 287,226 pairs, 13,368 pairs, and 11,490 pairs were utilised for training, validation, and testing, respectively [56].', 'In the model proposed by Paulus et al.', ', the document was preprocessed using the same method applied in [55].', '*e proposed model was evaluated using two datasets: the CNN/ Daily News dataset and the New York Times dataset.', '*e CNN/Daily Mail dataset was utilised by Liu et al.', 'for training their model [58].', '*e ATSDL model consisted of three stages: text preprocessing, phrase extractions, and summary generation [30].', 'During text preprocessing, the CoreNLP tool was employed to segment the words, reduce the morphology, and resolve the coreference.', '*e second stage of the ATSDL model was phrase extraction, which included the acquisition, refinement and combination of phrases.', 'In addition, multiorder semantic parsing (MOSP), which was proposed to create multilayer binary semantics, was applied for phrase extraction.', '*e first step of MOSP was to perform Stanford NLP parsing, which is a specialised tool that retrieved the lexical and syntactic features from the preprocessed sentences.', 'Next, dependency parsing was performed to create a binary tree by determining the root of the tree, which represents the relational phrase.', 'If the child node has children, then the child is considered a new root with children; this process continues recursively until there are no children for the root.', 'In this case, the tree structure is completed.', 'Accordingly, the compound phrases can be explored via dependency parsing.', 'However, one of the important stages of phrase extraction is refinement, during which redundant and incorrect phrases are refined before training by applying simple rules.', 'First, the phrase triples at the topmost level are exploited since they carry the most semantic information.', 'Second, triple phrases with subject and object phrases and no nouns are deleted since the noun contains a considerable amount of conceptual information.', 'Triple phrases without a verb in a relational phrase are deleted.', 'Moreover, phrase extraction includes phrase combination, during which phrases with the same meaning are combined to minimise redundancy and the time required to train the LSTM-RNN.', 'To achieve the goal of the previous task and determine whether two phrases can be combined, a set of artificial rules are applied.', '*e experiments were conducted using the CNN and Daily Mail datasets, which consisted of 92,000 text sources and 219,000 text sources, respectively.', '*e Kry´scin´ski et al.', '[60] model was trained using a CNN/ Daily Mail dataset, which was preprocessed using the method from 56[55, 56].', '*e experiments of DEATS were conducted using the CNN/Daily Mail dataset and DUC2004 corpus [61].', '*e experiments of the BiSum model were performed using the CNN/Daily Mail dataset [62].', 'In the Wang et al.', 'proposed model, CNN/Daily Mail and DUC2002 were employed in experiments [63] while the Egonmwan et al.', 'model employed the CNN/Daily and Newsroom datasets in experiments [64].', 'Experiments were conducted with the Liu et al.', '[65] model using three benchmark datasets, including CNN/Daily Mail, New York Times Annotated Corpus (NYT), and XSum.', 'Experiments were also conducted with the DAPT model using the CNN/Daily Mail and LCSTS datasets [49].', '*e evaluation metrics ROUGE1, ROUGE2, and ROUGE-L, with values of 39.', '38, respectively, were applied to measure the performance of the See et al.', 'model [56], which outperformed previous approaches by at least two points in terms of the ROUGE metrics.', 'Reinforcement learning with the intra-attention model achieved the following results: ROUGE1, 41.', '75; and ROUGE-L, 39.', '*e results for the maximum likelihood model were 39.', '9 for ROUGE1, ROUGE2, and ROUGEL, respectively.', 'Overall, the proposed approach yielded highquality generated summaries [57].', 'ROUGE1, ROUGE2, and ROUGE-L were utilised to evaluate the Liu et al.', 'model, which obtained values of 39.', 'In addition, a manual qualitative evaluation was performed to evaluate the quality and readability of the summary.', 'Two participants evaluated the summaries of 50 test examples that were selected randomly from the datasets.', 'Each summary was given a score from 1 to 5, where 1 indicates a low level of readability and 5 indicates a high level of readability.', 'ROUGE1 and ROUGE2 were used to evaluate the ATSDL model [30].', '*e value of ROUGE1 was 34.', '9, and the value of ROUGE2 was 17.', 'Furthermore, ROUGE1, ROUGE2, and ROUGE-L were applied as evaluation metrics of the Al-Sabahi et al.', 'and Li et al.', 'models, and the values of 42.', '5, respectively, were obtained for the AlSabahi et al.', 'model [35], while the values of 38.', '68, respectively, were obtained for the Li et al.', '*e evaluation of the Kry´scinski et al.', 'model was con- ´ ducted using quantitative and qualitative evaluations [60].', '*e quantitative evaluations included ROUGE1, ROUGE2, and ROUGE-L, and the values of 40.', '52, respectively, were obtained.', 'Additionally, a novel score related to the n-gram was employed to measure the level of abstraction in the summary.', '*e qualitative evaluation involved the manual evaluation of the proposed model.', 'Five participants evaluated 100 full-text summaries in terms of relevance and readability by giving each document a value from 1 to 10.', 'Furthermore, for comparison purposes, fulltext summaries from two previous studies [56, 58] were selected.', '*e evaluators graded the output summaries without knowing which model generated them.', '16 Mathematical Problems in EngineeringMoreover, ROUGE1, ROUGE2, and ROUGE-L were applied for evaluating DEATS, and the values of 40.', '13, respectively, were obtained for the CNN/Daily Mail dataset [61].', '*e experimental results of the BiSum model showed that the values of ROUGE1, ROUGE2, and ROUGE-L were 37.', 'Several variations in the Wang et al.', '*e best results were achieved by the BEAR (large + WordPiece) model, where the WordPiece tokeniser was utilised.', '*e values of ROUGE1, ROUGE2, and ROUGE-L were 41.', 'In Egonmwan et al.', 'model, the values of ROUGE1 and ROUGE2 were 41.', '90, respectively, while the value of ROUGE3 was 38.', 'Several variations in the Liu et al.', '[65] model were evaluated using ROUGE1, ROUGE2, and ROUGE-L, where the best model, which is referred to as BERTSUMEXT (large), achieved the values of 43.', '90 for ROUGE1, ROUGE2, and ROUGE-L, respectively, over the CNN/Daily Mail datasets.', 'Moreover, the model was evaluated by a human via a question and answering paradigm, where 20 documents were selected for evaluation.', '*ree values were chosen for evaluating the answer: a score of 1 indicates the correct answer; a score of 0.', '5 indicates a partially correct answer; and a score of 0 indicates a wrong answer.', 'ROUGE1, ROUGE2, and ROUGE-L for the DAPT model over the CNN/Daily Mail datasets were 40.', 'Finally, the pointer-generator approach was applied on the single-sentence and multisentence summaries.', 'Attention encoder-decoder RNNs were employed to model the abstractive text summaries [55].', 'Both the encoder and decoder have the same number of hidden states.', 'Additionally, the proposed model consists of a softmax layer for generating the words based on the vocabulary of the target.', '*e encoder and decoder differ in terms of their components.', '*e encoder consists of two bidirectional GRURNNs—a GRU-RNN for the word level and a GRU-RNN for the sentence level—while the decoder uses a unidirectional GRU-RNN, as shown in Figure 15.', 'Furthermore, the decoder uses batching, where the vocabulary at the decoder for each minibatch is restricted to the words in the batch of the source document.', 'Instead of considering every vocabulary, only certain vocabularies were added based on the frequency of the vocabulary in the target dictionary to decrease the size of the decoder softmax layer.', 'Several linguistic features were considered in addition to the word embedding of the input words to identify the key entities of the document.', 'Linguistic and statistical features included TF-IDF statistics and the part-of-speech and named-entity tags of the words.', 'Specifically, the part-of-speech tags were stored in matrixes for each tag type that was similar to word embedding, while the TF-IDF feature was discretised in bins with a fixed number, where one-hot representation was employed to represent the value of the bins.', '*e onehot matrix consisted of the number of bin entries, where only one entry was set to one to indicate the value of the TFIDF of a certain word.', '*is process permitted the TF-IDF to be addressed in the same way as any other tag by concatenating all the embeddings into one long vector, as shown in Figure 16.', '*e experiments were conducted using the annotated Gigaword corpus with 3.', '8 M training examples, the DUC corpus, and the CNN/Daily Mail corpus.', '*e preprocessing methods included tokenisation and part-of-speech and name-entity generation.', 'Additionally, the Word2Vec model with 200 dimensions was applied for word embedding and trained using the Gigaword corpus.', 'Additionally, the hidden states had 400 dimensions in both the encoder and the decoder.', 'Furthermore, datasets with multisentence summaries were utilised in the experiments.', '*e values of ROUGE1, ROUGE2, and ROUGE-L were higher than those of previous work on abstractive summarisation, with values of 35.', 'Finally, for both single-sentence summary and multisentence summary models, the components of the encoder and decoder of each approach are displayed in Table 1.', 'Furthermore, dataset preprocessing and word embedding of several approaches are appeared in Table 2 while training, optimization, mechanism, and search at the decoder are presented in Table 3.', 'Datasets for Text Summarization Various datasets were selected for abstractive text summarisation, including DUC2003, DUC2004 [69], Gigaword [70], and CNN/Daily Mail [71].', '*e DUC datasets are produced for the Document Understanding Conference; although their quality is high, they are small datasets that are typically employed to evaluate summarisation models.', '*e DUC2003 and DUC2004 datasets consist of 500 articles.', '*e Gigaword dataset from the Stanford University Linguistics Department was the most common dataset for model training in 2015 and 2016.', 'Gigaword consists of approximately 10 million documents from seven news sources, including the New York Times, Associated Press, and Washington Post.', 'Gigaword is one of the largest and most diverse summarisation datasets even though it contains headlines instead of summaries; thus, it is considered to contain single-sentence summaries.', 'Recent studies utilised the CNN/Daily Mail datasets for training and evaluation.', '*e CNN/Daily Mail datasets consist of bullet points that describe the articles, where multisentence summaries are created by concatenating the bullet points of the article [5].', 'CNN/Daily Mail datasets that are applied in abstractive summarisation were presented by Nallapati et al.', '*ese datasets were created by modifying the CNN/Daily Mail datasets that were generated by Hermann et al.', '*e Hermann et al.', 'datasets were utilised for extractive summarisation.', '*e abstractive summarisation CNN/Daily Mail datasets have 286,817 pairs for training and 13,368 pairs for validation, while 11,487 pairs were applied in testing.', 'In training, the source documents have 766 words (on average 29.', '74 sentences), while the summaries have 53 words (on average 3.', 'In April 2018, NEWSROOM, a summarisation dataset that consists of 1.', '3 million articles collected from social media metadata from 1998 to 2017, was produced [72].', '*e Mathematical Problems in Engineering 17W1 POS NER TF IDF W1 POS NER TF IDF W1 POS NER TF IDF W1 POS NER TF IDF W1 POS NER TF IDF Word_ Sum1 <Start> Word_ Sum2 Encoder hidden states Decoder hidden states W1 POS NER TF IDF Figure 16: Word embedding concatenated with discretized TF-IDF, POS, and NER one-embedding vectors [55].', 'Word_ Sum1 <Start> Word_ Sum2 Word1 Word2 Word3 Word4 Word5 Word6 Word-level hidden states Decoder hidden states Encoder hidden states Sentence-level hidden states Figure 15: Word-level and sentence-level bidirectional GRU-RNN [55].', 'Table 1: Encoder and decoder components.', 'Reference Year Encoder Decoder [18] 2015 Bag-of-words, convolutional, and attention-based [29] 2015 RNN with LSTM units and attention RNN with LSTM units and attention [39] 2016 RNN-LSTM decoder RNN Word-based [50] 2016 GRU + QRNN + attention GRU + RNN QRNN [38] 2016 Unidirectional RNN attentive encoder-decoder LSTM Unidirectional RNN attentive encoder-decoder LSTM Bidirectional LSTM Unidirectional LSTM Bidirectional LSTM Decoder that had global attention [51] 2016 LSTM-RNN LSTM-RNN [55] 2016 Two bidirectional GRU-RNN GRU-RNN unidirection [52] 2017 Bidirectional GRU Unidirectional GRU [53] 2017 Bidirectional GRU Unidirectional GRU [56] 2017 Single-layer bidirectional LSTM + attention Single-layer unidirectional LSTM [57] 2017 Bidirectional LSTM-RNN + intra-attention single LSTM decoder + intra-attention [58] 2018 Bidirectional LSTM Unidirectional LSTM [30] 2018 Bidirectional LSTM Unidirectional LSTM [35] 2018 Bidirectional LSTM Bidirectional LSTM [59] 2018 Bidirectional LSTM Unidirectional LSTM [60] 2018 Bidirectional LSTM 3-layer unidirectional LSTM [61] 2018 Bidirectional GRU Unidirectional GRU [62] 2018 Bidirectional LSTM Two-decoder unidirectional LSTM [63] 2019 Bidirectional GRU Unidirectional GRU [64] 2019 Unidirectional GRU Unidirectional GRU [49] 2020 Bidirectional LSTM Unidirectional LSTM 18 Mathematical Problems in EngineeringNEWSROOM dataset consists of 992,985 pairs for training and 108,612 and 108,655 pairs for validation and testing, respectively [22].', '*e quality of the summaries is high, and the style of the summarisation is diverse.', 'Figure 17 displays the number of surveyed papers that applied each of the datasets.', 'Nine research papers utilised Gigaword, fourteen papers employed the CNN/Daily Mail datasets (largest number of papers on the list), and one study applied the ACL Anthology Reference, DUC2002, DUC2004, New York Times Annotated Corpus (NYT), and XSum datasets.', 'Table 2: Dataset preprocessing and word embedding.', 'Reference Authors Dataset preprocessing Input (word embedding) [18] Rush et al.', 'PTB tokenization by using “#” to replace all digits, converting all letters to lower case, and “UNK” to replace words that occurred fewer than 5 times Bag-of-words of the input sentence embedding [39] Chopra et al.', 'PTB tokenization by using “#” to replace all digits, converting all letters to lower case, and “UNK” to replace words that occurred fewer than 5 times Encodes the position information of the input words [55] Nallapati et al.', 'Part-of-speech and name-entity tags generating and tokenization (i) Encodes the position information of the input words (ii) *e input text was represented using the Word2Vec model with 200 dimensions that was trained using Gigaword corpus (iii) Continuous features such as TF-IDF were represented using bins and one-hot representation for bins (iv) Lookup embedding for part-of-speech tagging and name-entity tagging [52] Zhou et al.', 'PTB tokenization by using “#” to replace all digits, converting all letters to lower case, and “UNK” to replace words that occurred fewer than 5 times Word embedding with size equal to 300 [53] Cao et al.', 'Normalization and tokenization, using the “#” to replace digits, convert the words to lower case, and “UNK” to replace the least frequent words.', 'GloVe word embedding with dimension size equal to 200 [54] Cai et al.', 'Byte pair encoding (BPE) was used in segmentation Transformer [50] Adelson et al.', 'Converting the article and their headlines to lower case letters GloVe word embedding [29] Lopyrev Tokenization, converting the article and their headlines to lower case letters, using the symbol 〈unk〉 to replace rare words *e input was represented using the distributed representation [38] Jobson et al.', '*e word embedding randomly initialised and updated during training while GloVe word embedding was used to represent the words in the second and third models [56] See et al.', '*e word embedding of the input for was learned from scratch instead of using a pretrained word embedding model [57] Paulus et al.', '*e same as in [55] GloVe [58] Liu et al.', 'CNN maximum pooling was used to encode the discriminator input sequence [30] Song et al.', '*e words were segmented using CoreNLP tool, resolving the coreference and reducing the morphology Convolutional neural network was used to represent the phrases [35] Al-Sabahi et al.', '*e word embedding is learned from scratch during training with a dimension of 128 [59] Li et al.', '*e same as in [55] Learned from scratch during training [60] Kry´scinski ´ et al.', '*e same as in [55] Embedding layer with a dimension of 400 [61] Yao et al.', '*e word embedding is learned from scratch during training with a dimension of 128 [62] Wan et al.', 'No word segmentation Embedding layer learned during training [65] Liu et al.', 'BERT [63] Wang et al.', 'Using WordPiece tokenizer BERT [64] Egonmwan et al.', 'GloVe word embedding with dimension size equal to 300 Mathematical Problems in Engineering 19Table 4 lists the datasets that are used to train and validate the summarisation methods in the research papers listed in this work.', 'Evaluation Measures *e package ROUGE is employed to evaluate the text summarisation techniques by comparing the generated summary with a manually generated summary [73].', '*e package consists of several measures to evaluate the performance of text summarisation techniques, such as ROUGE-N (ROUGE1 and ROUGE2) and ROUGE-L, which were employed in several studies [38].', 'ROUGE-N is n-gram recall such that ROUGE1 and ROUGE2 are related to unigrams and bigrams, respectively, while ROUGE-L is related to the longest common substring.', 'Since the manual Table 3: Training, optimization, mechanism, and search at the decoder.', 'Reference Authors Training and optimization Mechanism Search at decoder (siz) [18] Rush et al.', 'Stochastic gradient descent to minimise negative log-likelihood Beam search [39] Chopra et al.', 'Minimizing negative log-likelihood using end-to-end using stochastic gradient descent Encodes the position information of the input words Beam search [55] Nallapati et al.', 'Optimize the conditional likelihood using Adadelta Pointer mechanism Beam search (5) [52] Zhou et al.', 'Stochastic gradient descent, Adam optimizer, optimizing the negative log-likelihood Attention mechanism Beam search (12) [53] Cao et al.', 'Adam optimizer, optimizing the negative log-likelihood Copy mechanism, coverage mechanism, dual-attention decoder Beam search (6) [54] Cai et al.', 'Cross entropy is used as the loss function Attention mechanism Beam search (5) [50] Adelson et al.', 'Adam Attention mechanism [29] Lopyrev RMSProp adaptive gradient method Simple and complex attention mechanism Beam search [38] Jobson et al.', 'Adadelta, minimising the negative log probability of prediction word Bilinear attention mechanism, pointer mechanism [56] See et al.', 'Adadelta Coverage mechanism, attention mechanism, pointer mechanism Beam search (4) [57] Paulus et al.', 'Adam, RL Intradecoder attention mechanism, pointer mechanism, copy mechanism, RL Beam search (5) [58] Liu et al.', 'Adadelta stochastic gradient descent Attention mechanism, pointer mechanism, copy mechanism, RL [30] Song et al.', 'Attention mechanism, copy mechanism [35] Al-Sabahi et al.', 'Adagrad mechanism, copy mechanism Pointer mechanism, coverage Bidirectional beam search [59] Li et al.', 'Adadelta Attention mechanism, pointer mechanism, copy mechanism, prediction guide mechanism Beam search [60] Kry´scinski ´ et al.', 'Asynchronous gradient descent optimizer Temporal attention and intraattention pointer mechanism, RL Beam search [61] Yao et al.', 'RL, Adagrad Attention mechanism, pointer mechanism, copy mechanism, coverage mechanism, RL Beam search (4) [62] Wan et al.', 'Adagrad Attention mechanism, pointer mechanism Beam-search backward (2) and forward (4) [65] Liu et al.', 'Adam Self-attention mechanism Beam search (5) [63] Wang et al.', 'Gradient of reinforcement learning, Adam, cross-entropy loss function Attention mechanism, pointer mechanism, copy mechanism, new coverage mechanism Beam search [64] Egonmwan et al.', 'Adam Self-attention mechanism Greedy-decoding during training and validation.', 'Beam search at decoding during testing [49] Peng et al.', 'Adam, gradient descent, crossentropy loss Coverage mechanism, RL, double attention pointer network (DAPT) Beam search (5) 20 Mathematical Problems in Engineeringevaluation of automatic text summarisation is a time-consuming process and requires extensive effort, ROUGE is employed as a standard for evaluating text summarisation.', 'ROUGE-N is calculated using the following equation: ROUGE − N � \\U0010ff50 S ∈ {REFERENCE SUMMARIES} \\U0010ff50 gramn ∈ Countmatch \\U0010ff01 gramn \\U0010ff50 S ∈ {REFERENCE SUMMARIES} \\U0010ff50 gramn ∈ Count gram  \\U0010ff01 n , (1) where S is the reference summary, n is the n-gram length, and Countmatch (gramn) is the maximum number of matching n-gram words between the reference summary and the generated summary.', 'Count (gramn) is the total number of n-gram words in the reference summary [73].', 'ROUGE-L is the longest common subsequence (LCS), which represents the maximum length of the common matching words between the reference summary and the generated summary.', 'LCS calculation does not necessarily require the match words to be consecutive; however, the order of occurrence is important.', 'In addition, no predefined number of match words is required.', 'LCS considers only the main in-sequence, which is one of its disadvantages since the final score will not include other matches.', 'For example, assume that the reference summary R and the automatic summary A are as follows: Gigaword CNN/Daily Mail ACL DUC2004 DUC2002 Newsroom NYT XSum Datasets Number of researches 86420 10 12 14 16 Datasets Figure 17: *e number of research papers that used the Gigaword, CNN/Daily Mail, ACL DUC2002, DUC2004, NYT, Newsroom, and XSum datasets [61].', 'Table 4: Abstractive summarisation datasets.', 'Reference Training Summarization Evaluation [18] Gigaword DUC2003 and DUC2004 [39] Gigaword DUC2004 [50] Gigaword Gigaword [29] Gigaword Articles from BBC, *e Wall Street Journal, Guardian, Huffington Post, and Forbes [38] Gigaword — [54] Gigaword and DUC2004 Gigaword and DUC2004 [51] ACL anthology reference ACL anthology reference [52] Gigaword and DUC2004 Gigaword and DUC2004 [53] Gigaword and DUC2004 Gigaword and DUC2004 [56] CNN/Daily Mail CNN/Daily Mail [57] CNN/Daily and New York Times CNN/Daily and New York Times [58] CNN/Daily Mail CNN/Daily Mail [30] CNN/Daily Mail CNN/Daily Mail [35] CNN/Daily Mail CNN/Daily Mail [59] CNN/Daily Mail CNN/Daily Mail [60] CNN/Daily Mail CNN/Daily Mail [61] CNN/Daily Mail CNN/Daily Mail [55] Gigaword DUC CNN/Daily Mail Gigaword DUC CNN/Daily Mail [62] CNN/Daily Mail CNN/Daily Mail [65] CNN/Daily Mail, NYT, and XSum CNN/Daily Mail, NYT, and XSum [63] CNN/Daily Mail and DUC2002 CNN/Daily Mail and DUC2002 [64] CNN/Daily Mail and Newsroom CNN/Daily Mail and Newsroom [49] CNN/Daily Mail CNN/Daily Mail Mathematical Problems in Engineering 21R: Ahmed ate the apple.', 'A: the apple Ahmed ate.', 'In this case, ROUGE-L will consider either “Ahmed ate” or “the apple” but not both, similar to LCS.', 'Tables 5 and 6 present the values of ROUGE1, ROUGE2, and ROUGE-L for the text summarisation methods in the various studies reviewed in this research.', 'In addition, Perplexity was employed in [18, 39, 51], and BLEU was utilised in [29].', '*e models were evaluated using various datasets.', '*e other models applied ROUGE1, ROUGE2, and ROUGE-L for evaluation.', 'It can be seen that the highest values of ROUGE1, ROUGE2, and ROUGE-L for text summarisation with the pretrained encoder model were 43.', 'Even though ROUGE was employed to evaluate abstractive summarisation, it is better to obtain new methods to evaluate the quality of summarisation.', '*e new evaluation metrics must consider novel words and semantics since the generated summary contains words that do not exist in the original text.', 'However, ROUGE was very suitable for extractive text summarisation.', 'Based on our taxonomy, we divided the results of ROUGE1, ROUGE2, and ROUGE-L into two groups.', '*e first group considered single-sentence summary approaches, while the second group considered multisentence summary approaches.', 'Figure 18 compares several deep learning techniques in terms of ROUGE1, ROUGE2, and ROUGE-L for the Gigaword datasets, consisting of single-sentence summary documents.', '*e highest values for ROUGE1, ROUGE2, and ROUGE-L were achieved by the RCT model [54].', '*e values for ROUGE1, ROUGE2, and ROUGE-L were 37.', 'Furthermore, Figure 19 compares the ROUGE1, ROUGE2, and ROUGE-L values for abstractive text summarisation methods for the CNN/Daily Mail datasets, which consist of multisentence summary documents.', '*e highest values of ROUGE1, ROUGE2, and ROUGE-L were achieved for text summarisation with a pretrained encoder model.', '*e values for ROUGE1, ROUGE2, and ROUGE-L were 43.', 'It can be clearly seen that the best model in the single-sentence summary and multisentence summary is the models that employed BERT word embedding and were based on transformers.', '*e ROUGE values for the CNN/Daily Mail datasets are larger than those for the Gigaword dataset, as Gigaword is utilised for single-sentence summaries as it contains headlines that are treated as summaries, while the CNN/Daily Mail datasets are multisentence summaries.', '*us, the summaries in the CNN/Daily Mail datasets are longer than the summaries in Gigaword.', 'selected two human elevators to evaluate the readability of the generated summary of 50 test examples of 5 models [58].', '*e value of 5 indicates that the generated summary is highly readable, while the value of 1 indicates that the generated summary has a low level of readability.', 'It can be clearly seen from the results that the Liu et al.', 'model was better than the other four models in terms of ROUGE1, ROUGE2, and human evaluation, even though the model is not optimal with respect to the ROUGE-L value.', 'In addition to quantitative measures, qualitative evaluation measures are important.', 'also performed qualitative ´ evaluation to evaluate the quality of the generated summary [60].', 'Five human evaluators evaluated the relevance and readability of 100 randomly selected test examples, where two values are utilised: 1 and 10.', '*e value of 1 indicates that the generated summary is less readable and less relevance while the value of 10 indicates that the generated summary is readable and very relevance.', '*e results showed that, in terms of readability, the model proposed by Kry´scinski et al.', '´ is slightly inferior to See et al.', '[56] and Liu et al.', '[58] models with mean values of 6.', '79 for Kry´scinski et al.', ', ´ to See et al.', 'and Liu et al.', 'On the other hand, with respect to the relevance, the means values of the three models are relevance with values of 6.', '74 for Kry´scinski et al.', ', to See et al.', 'and Liu et al.', '´ However, the Kry´scinski et al.', 'model was the best in terms of ´ ROUGE1, ROUGE2, and ROUGE-L.', 'evaluated the quality of the generated summary in terms of succinctness, informativeness, and fluency in addition to measuring the level of retaining key information, which was achieved by human evaluation [65].', 'In addition, qualitative evaluation evaluated the output in terms of grammatical mistakes.', '*ree values were selected for evaluating 20 test examples: 1 indicates a correct answer, 0.', '5 indicates a partially correct answer, and 0 indicates an incorrect answer.', 'We can conclude that quantitative evaluations, which include ROUGE1, ROUGE2, and ROUGE-L, are not enough for evaluating the generated summary of abstractive text summarisation, especially when measuring readability, relevance, and fluency.', '*erefore, qualitative measures, which can be achieved by manual evaluation, are very important.', 'However, qualitative measures without quantitative measures are not enough due to the small number of testing examples and evaluators.', 'Challenges and Solutions Text summarisation approaches have faced various challenges; although some have been solved, others still need to be addressed.', 'In this section, these challenges and their possible solutions are discussed.', 'Unavailability of the Golden Token during Testing.', 'Due to the availability of golden tokens (i.', ', reference summary tokens) during training, previous tokens in the headline can be input into the decoder at the next step.', 'However, during testing, the golden tokens are not available; thus, the input for the next step in the decoder will be limited to the previously generated output word.', 'To solve this issue, which becomes more challenging when addressing small datasets, different solutions have been proposed.', 'For example, in reference [51], the data-as-demonstrator (DaD) model [74] is utilised.', 'In DaD, at each step, based on a coin flip, either a gold token is utilised during training or the previous step is employed during both testing and training.', 'In this manner, at least the training step receives the same 22 Mathematical Problems in Engineeringinput as testing.', 'In all cases, the first input of the decoder is the 〈EOS〉 token, and the same calculations are applied to compute the loss.', 'In [29], teacher forcing is employed to address this challenge: during training, instead of feeding the expected word from the headline, 10% of the time, the generated word of the previous step is fed back [75, 76].', 'Moreover, the mass convolution of the QRNN is applied in [50] since the dependency of words generated in the future is difficult to determine.', 'One of the challenges that may occur during testing is that the central words of the test document may be rare or unseen during training; these words are referred to as OOV words.', 'In 61[55, 61], a switching decoder/pointer was employed to address OOV words by using pointers to point to their original positions in the source document.', '*e switch on the decoder side is used to alternate between generating a word and using a pointer, as shown in Figure 20 [55].', 'When the switch is turned off, the decoder will use the pointer to point to the word in the source to copy it to the memory.', 'When the switch is turned on, the decoder will generate a word from the target vocabularies.', 'Conversely, researchers in [56] addressed OOV words via probability generation Pgen, where the value is calculated from the context vector and decoder state, as shown in Figure 21.', 'To generate the output word, Pgen switches between copying the output words from the input sequence and generating them from the vocabulary.', 'Furthermore, the pointer-generator technique is applied to point to input words to copy them.', '*e combination between the words in the input and the vocabulary is referred to the extended vocabulary.', 'In addition, in [57], to generate the tokens on the decoder side, the decoder utilised the switch function at each timestep to switch between generating the token using the softmax layer and using the pointer mechanism to point to the input sequence position for Table 5: Evaluation measures of several deep learning abstractive text summarisation methods over the Gigaword dataset.', 'Reference Year Authors Model ROUGE1 ROUGE2 ROUGE-L [18] 2015 Rush et al.', '81 [39] 2016 Chopra et al.', 'RAS-Elman (k � 10) 28.', '06 [55] 2016 Nallapati et al.', '24 [52] 2017 Zhou et al.', '63 [53] 2018 Cao et al.', '24 [54] 2019 Cai et al.', '62 Table 6: Evaluation measures of several abstractive text summarisation methods over the CNN/Daily Mail datasets.', 'Reference Year Authors Model ROUGE1 ROUGE2 ROUGE-L [55] 2016 Nallapati et al.', '65 [56] 2017 See et al.', 'Pointer-generator + coverage 39.', '38 [57] 2017 Paulus et al.', 'Reinforcement learning, with intra-attention 41.', '08 [57] 2017 Paulus et al.', 'Maximum-likelihood + RL, with intra-attention 39.', '90 [58] 2018 Liu et al.', '71 [30] 2018 Song et al.', '8 — [35] 2018 Al-Sabahi et al.', 'Bidirectional attentional encoder-decoder 42.', '5 [59] 2018 Li et al.', 'Key information guide network 38.', '68 [60] 2018 Kry´scinski et al.', 'ML + RL ROUGE + Novel, with LM ´ 40.', '52 [61] 2018 Yao et al.', '13 [62] 2018 Wan et al.', '66 [63] 2019 Wang et al.', 'BEAR (large + WordPiece) 41.', '49 [64] 2019 Egonmwan et al.', 'TRANS-ext + filter + abs 41.', '92 [65] 2020 Liu et al.', '90 [49] 2020 Peng et al.', 'DAPT + imp-coverage (RL + MLE (ss)) 40.', '35 ROUGE1 ROUGE2 ROUGE-L 5 0 10 15 20 25 30 35 40 ABS+ RAS-Elman (k = 10) SEASS Words-lvt5k-1sent (Gigaword) FTSumg RCT Figure 18: ROUGE1, ROUGE2, and ROUGE-L scores of several deep learning abstractive text summarisation methods for the Gigaword dataset.', 'Mathematical Problems in Engineering 23unseen tokens to copy them.', 'Moreover, in [30], rare words were addressed by using the location of the phrase, and the resulting summary was more natural.', 'Moreover, in 35 [35, 58, 60], the OOV problem was addressed by using the pointer-generator technique employed in [56], which alternates between generating a new word and coping the word from the original input text.', 'Summary Sentence Repetition and Inaccurate Information Summary.', '*e repetition of phrases and generation of incoherent phrases in the generated output summary are two challenges that must be considered.', 'Both challenges are due to the summarisation of long documents and the production of long summaries using the attention-based encoder-decoder RNN [57].', 'In [35, 56], repetition was addressed by using the coverage model to create the coverage vector by aggregating the attention over all previous timesteps.', 'In [57], repetition was addressed by using the key attention mechanism, where for each input token, the encoder intratemporal attention records the weights of the previous attention.', 'Furthermore, the intratemporal attention uses the hidden states of the decoder at a certain timestep, the previously generated words, and the specific part of the encoded input sequence, as shown in Figure 22, to prevent repetition and attend to the same sequence of the input at a different step of the decoder.', 'However, the intra-attention encoder mechanism cannot address all the repetition challenges, especially when a long sequence is generated.', '*us, the intradecoder attention mechanism was proposed to allow the decoder to consider more previously generated words.', 'Moreover, the proposed intradecoder attention mechanism is applicable to any type of the RNN decoder.', 'Repetition was also addressed by using an objective function that combines the cross-entropy loss maximum likelihood and gradient reinforcement learning to minimise the exposure bias.', 'In addition, the probability of trigram p (yt) was proposed to address repetition in the generated summary, where yt is the trigram sequence.', 'In this case, the value of p (yt) is 0 during a beam search in the decoder when the same trigram sequence was already generated in the output summary.', 'Furthermore, in [60], the heuristic proposed by [57] was employed to reduce repetition in the summary.', 'Moreover, in [61], the proposed approach addressed repetition by exploiting the encoding features generated using a secondary encoder to remember the previously generated decoder output, and the coverage mechanism is utilised.', 'Abstractive summarisation may generate summaries with fake facts, and 30% of summaries generated from abstractive text summarisation suffer from this problem [53].', 'With fake facts, there may be a mismatch between the subject and the object of the predicates.', '*us, to address this problem, dependency parsing and open information extraction (e.', ', open information extraction (OpenIE)) are performed to extract facts.', '*erefore, the sequence-to-sequence framework with dual attention was proposed, where the generated summary was conditioned by the input text and description of the extracted facts.', 'OpenIE facilitates entity extraction from a relation, and Stanford CoreNLP was employed to provide the proposed approach with OpenIE and the dependency parser.', 'Moreover, the decoder utilised copying and coverage mechanisms.', '*e main issue of the abstractive text summarisation dataset is the quality of the reference summary (Golden summary).', 'In the CNN/Daily Mail dataset, the reference summary is the highlight of the news.', 'Every highlight represents a sentence in the summary; therefore, 5 0 10 15 20 25 30 35 40 45 50 ROUGE1 ROUGE2 ROUGE-L Pointer-generator + coverage Reinforcement learning, with intra-attention Maximum-likelihood + RL, with intra-attention Adversarial network ATSDL Bidirectional attentional encoder-decoder Key information guide network ML + RL ROUGE + novel, with LM DEATS Words-lvt2k-temp-att (CNN/Daily Mail) BiSum BERTSUMEXT (large) BEAR (large + wordPiece) TRANS-ext + filter + abs DAPT + imp-coverage (RL + MLE (ss)) Figure 19: ROUGE1, ROUGE2, and ROUGE-L scores of abstractive text summarisation methods for the CNN/Daily Mail datasets.', '24 Mathematical Problems in Engineeringthe number of sentences in the summary is equal to the number of highlights.', 'Sometimes, the highlights do not address all crucial points in the summary.', '*erefore, a highquality dataset needs high effort to become available.', 'Moreover, in some languages, such as Arabic, the multisentence dataset for abstractive summarisation is not available.', 'Single-sentence abstractive Arabic text summarisation is available but is not free.', 'Another issue of abstractive summarisation is the use of ROUGE for evaluation.', 'ROUGE provides reasonable results in the case of extractive summarisation.', 'However, in abstractive summarisation, ROUGE is not enough as ROUGE depends on exact matching between words.', 'For example, the words book and books are considered different using any one of the ROUGE metrics.', '*erefore, a new evaluation measure must be proposed to consider the context of the words (words that have the same meaning must be considered the same even if they have a different surface form).', 'In this case, we propose to use METEOR which was used recently in evaluating machine translation and automatic summarisation models [77].', 'Moreover, METEOR considers stemming, morphological variants, and synonyms.', 'In addition, in flexible order language, it is better to use ROUGE without caring about the order of the words.', '*e quality of the generated summary can be improved using linguistic features.', 'For example, we proposed the use Encoder hidden states Decoder hidden states Word1 Word2 Word3 Word4 Word5 Word6 Word7 G P P G G Figure 20: *e generator/pointer switching model [55].', 'Word1 Word2 Word3 Word4 Word5 Word6 Word7 Context vector …….', 'Attention distribution Encoder hidden states <Start> Word_Sum1 …….', 'Decoder hidden states Vocabulary distribution X (1 – P gen) X Pgen P gen Figure 21: Pointer-generator model [56].', 'Mathematical Problems in Engineering 25of dependency parsing at the encoder in a separate layer at the top of the first hidden state layer.', 'We proposed the use of the word embedding, which was built by considering the dependency parsing or part-of-speech tagging.', 'At the decoder side, the beam-search quality can be improved by considering the part-of-speech tagging of the words and its surrounding words.', 'Based on the new trends and evaluation results, we think that the most promising feature among all the features is the use of the BERT pretrained model.', '*e quality of the models that are based on the transformer is high and will yield promising results.', 'Conclusion and Discussion In recent years, due to the vast quantity of data available on the Internet, the importance of the text summarisation process has increased.', 'Text summarisation can be divided into extractive and abstractive methods.', 'An extractive text summarisation method generates a summary that consists of words and phrases from the original text based on linguistics and statistical features, while an abstractive text summarisation method rephrases the original text to generate a summary that consists of novel phrases.', '*is paper reviewed recent approaches that applied deep learning for abstractive text summarisation, datasets, and measures for evaluation of these approaches.', 'Moreover, the challenges encountered when employing various approaches and their solutions were discussed and analysed.', '*e overview of the reviewed approaches yielded several conclusions.', '*e RNN and attention mechanism were the most commonly employed deep learning techniques.', 'Some approaches applied LSTM to solve the gradient vanishing problem that was encountered when using an RNN, while other approaches applied a GRU.', 'Additionally, the sequence-to-sequence model was utilised for abstractive summarisation.', 'Several datasets were employed, including Gigaword, CNN/Daily Mail, and the New York Times.', 'Gigaword was selected for single-sentence summarisation, and CNN/Daily Mail was employed for multisentence summarisation.', 'Furthermore, ROUGE1, ROUGE2, and ROUGE-L were utilised to evaluate the quality of the summaries.', '*e experiments showed that the highest values of ROUGE1, ROUGE2, and ROUGE-L were obtained in text summarisation with a pretrained encoder mode, with values of 43.', '*e best results were achieved by the models that apply Transformer.', '*e most common challenges faced during the summarisation process were the unavailability of a golden token at testing time, the presence of OOV words, summary sentence repetition, sentence inaccuracy, and the presence of fake facts.', 'In addition, there are several issues that must be considered in abstractive summarisation, including the dataset, evaluation measures, and quality of the generated summary.', 'Data Availability No data were used to support this study.', 'Conflicts of Interest *e authors declare no conflicts of interest.', ', “Text summarization techniques: a brief survey,” International Journal of Advanced Computer Science and Applications, vol.', 'Menai, “Automatic Arabic text summarization: a survey,” Artificial Intelligence Review, vol.', 'Williams, “Fast generation of result snippets in web search,” in Proceedings of the 30th Annual international ACM SIGIR Conference on Word1 Word2 Word3 Word4 Word5 Word6 Word7 …….', 'Encoder + Word_ <Start> Sum1 Word_ Sum2 + C C H Decoder …….', 'Figure 22: A new word is added to the output sequence by combining the current hidden state “H” of the decoder and the two context vectors, marked as “C” [57].', '26 Mathematical Problems in EngineeringResearch and Development in information Retrieval-SIGIR’07, p.', '127, Amsterdam, *e Netherlands, 2007.', 'Trippe, “A vision for health informatics: introducing the SKED framework an extensible architecture for scientific knowledge extraction from data,” 2017, http://arxiv.', 'Syed, Abstractive Summarization of Social Media Posts: A case Study using Deep Learning, Master’s thesis, Bauhaus University, Weimar, Germany, 2017.', 'Awajan, “Deep learning based extractive text summarization: approaches, datasets and evaluation measures,” in Proceedings of the 2019 Sixth International Conference on Social Networks Analysis, Management and Security (SNAMS), pp.', '204–210, Granada, Spain, 2019.', 'Bataineh, “A hybrid approach for Arabic text summarization using domain knowledge and genetic algorithms,” Cognitive Computation, vol.', 'Ganesh, “A study on abstractive summarization techniques in Indian languages,” Procedia Computer Science, vol.', 'McKeown, “Introduction to the special issue on summarization,” Computational Linguistics, vol.', 'Salim, “A review on abstractive summarization methods,” Journal of @eoretical and Applied Information Technology, vol.', 'Chitrakala, “A survey on abstractive text summarization,” in Proceedings of the 2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT), pp.', '1–7, Nagercoil, India, 2016.', 'Govilkar, “A survey of text summarization techniques for Indian regional languages,” International Journal of Computer Applications, vol.', 'Mathur, “A survey on methods of abstractive text summarization,” International Journal for Research in Emerging Science andTechnology, vol.', 'Kartheek Rachabathuni, “A survey on abstractive summarization techniques,” in Proceedings of the 2017 International Conference on Inventive Computing and Informatics (ICICI), pp.', 'Afjal, “Study of abstractive text summarization techniques,” American Journal of Engineering Research, vol.', ', “Abstractive text summarization based on improved semantic graph approach,” International Journal of Parallel Programming, vol.', 'Bouzoubaa, “Towards a new hybrid approach for abstractive summarization,” Procedia Computer Science, vol.', 'Weston, “A neural attention model for abstractive sentence summarization,” in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 2015.', 'Daniel, “Survey on abstractive text summarization,” in Proceedings of the 2018 International Conference on Communication and Signal Processing (ICCSP), pp.', 'Dong, “A survey on neural network-based summarization methods,” 2018, http://arxiv.', 'Sharma, “A comprehensive survey on extractive and abstractive techniques for text summarization,” in Ambient Communications and Computer Systems, Y.', '339–351, Springer, Singapore, 2019.', 'Reddy, Neural Abstractive Text Summarization with Sequence-ToSequence Models: A Survey, http://arxiv.', 'de Le´on, “Deep learning based text summarization: approaches, databases and evaluation measures,” in Proceedings of the International Conference of Applications of Intelligent Systems, Spain, 2018.', 'Hinton, “Deep learning,” Nature, vol.', 'Al Etaiwi, “*e use of hidden Markov model in natural Arabic language processing: a survey,” Procedia Computer Science, vol.', 'Zeng, “Fusing logical relationship information of text in neural network for text classification,” Mathematical Problems in Engineering, vol.', 'Wan, “A novel text clustering approach using deep-learning vocabulary network,” Mathematical Problems in Engineering, vol.', 'Cambria, “Recent trends in deep learning based natural language processing [review article],” IEEE Computational Intelligence Magazine, vol.', 'Lopyrev, Generating news headlines with recurrent neural networks, p.', 'Ruan, “Abstractive text summarization using LSTM-CNN Based Deep Learning,” Multimedia Tools and Applications, 2018.', 'Williams, “Dynamic recurrent neural networks: theory and applications,” IEEE Transactions on Neural Networks, vol.', 'Robinson, “An application of recurrent nets to phone probability estimation,” IEEE Transactions on Neural Networks, vol.', 'Bengio, “Neural machine translation by jointly learning to align and translate,” in Proceedings of the International Conference on Learning Representations, Canada, 2014, http://arxiv.', 'Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol.', 'Kang, Bidirectional Attentional Encoder-Decoder Model and Bidirectional Beam Search for Abstractive Summarization, Cornell University, Ithaca, NY, USA, 2018, http://arxiv.', 'Schmidhuber, “Long short-term memory,” Neural Computation, vol.', 'Cho, “Learning phrase representations using RNN encoder–decoder for statistical machine translation,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.', '1724–1734, Doha, Qatar, 2014.', 'Guti´errez, Abstractive Text Summarization Using Attentive Sequence-To-Sequence RNNs, p.', 'Mathematical Problems in Engineering 27[39] S.', 'Rush, “Abstractive sentence summarization with attentive recurrent neural networks,” in Proceedings of the NAACL-HLT16, pp.', '93–98, San Diego, CA, USA, 2016.', 'Guo, “Leverage label and word embedding for semantic sparse web service discovery,” Mathematical Problems in Engineering, vol.', '2020, Article ID 5670215, 8 pages, 2020.', 'Dean, “Efficient estimation of word representations in vector space,” 2013, http://arxiv.', 'Al-Madi, “Deep learning based technique for Plagiarism detection in Arabic texts,” in Proceedings of the 2017 International Conference on New Trends in Computing Sciences (ICTCS), pp.', '216–222, Amman, Jordan, 2017.', 'Awajan, “Comparative study of word embeddings models and their usage in Arabic language applications,” in Proceedings of the 2018 International Arab Conference on Information Technology (ACIT), pp.', '1–7, Werdanye, Lebanon, 2018.', 'Manning, “Glove: global vectors for word representation,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.', '1532–1543, Doha, Qatar, 2014.', 'Awajan, “Using part of speech tagging for improving Word2vec model,” in Proceedings of the 2019 2nd International Conference on new Trends in Computing Sciences (ICTCS), pp.', '1–7, Amman, Jordan, 2019.', 'Jegou, and ´ T.', 'zip: compressing text classification models,” 2016, http://arxiv.', ', “Attention is all you need,” Advances in Neural Information Processing Systems, pp.', 'Toutanova, “Pretraining of deep bidirectional transformers for language understanding,” in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.', '4171–4186, Minneapolis, MN, USA, 2019.', 'Ma, “Text summarization method based on double attention pointer network,” IEEE Access, vol.', 'Socher, Quasi-recurrent neural networks, https://arxiv.', 'Jurafsky, Neural Text Summarization, Stanford University, Stanford, CA, USA, 2016.', 'Zhou, “Selective encoding for abstractive sentence summarization,” in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pp.', '1095–1104, Vancouver, Canada, July 2017.', 'Li, “Faithful to the original: fact aware neural abstractive summarization,” in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), New Orleans, LA, USA, February 2018.', 'Dai, “Improving transformer with sequential context representations for abstractive text summarization,” in Natural Language Processing and Chinese Computing, J.', '512–524, Springer International Publishing, Cham, Switzerland, 2019.', 'Xiang, “Abstractive text summarization using sequence-tosequence RNNs and beyond,” in Proceedings of the CoNLL-16, Berlin, Germany, August 2016.', 'Manning, “Get to the point: summarization with pointer-generator networks,” in Proceedings of the 55th ACL, pp.', '1073–1083, Vancouver, Canada, 2017.', 'Socher, “A deep reinforced model for abstractive summarization,” 2017, http://arxiv.', 'Li, “Delineation of the intimate details of the backbone conformation of pyridine nucleotide coenzymes in aqueous solution,” Biochemical and Biophysical Research Communications, vol.', 'Gao, “Guiding generation for abstractive text summarization based on key information guide network,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.', '55–60, New Orleans, LA, USA, 2018.', 'Socher, “Im- ´ proving abstraction in text summarization,” in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Brussels, Belgium, November 2018.', 'Wu, “Dual encoding for abstractive text summarization,” IEEE Transactions on Cybernetics, pp.', 'Shi, “Abstractive document summarization via bidirectional decoder,” in Advanced Data Mining and Applications, G.', '364–377, Springer International Publishing, Cham, Switzerland, 2018.', 'Zhang, “A text abstraction summary model based on BERT word embedding and reinforcement learning,” Applied Sciences, vol.', 'Chali, “Transformer-based model for single documents neural summarization,” in Proceedings of the 3rd Workshop on Neural Generation and Translation, pp.', '70–79, Hong Kong, 2019.', 'Lapata, “Text summarization with pretrained encoders,” 2019, http://arxiv.', 'Ney, “Bidirectional decoder networks for attention-based end-to-end offline handwriting recognition,” in Proceedings of the 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR), pp.', '361–366, Shenzhen, China, 2016.', 'Le, “Sequence to Sequence Learning with Neural Networks,” in Proceedings of the Advances in Neural Information Processing Systems (NIPS), Montreal, Quebec, Canada, December 2014.', 'Liu, “Decoding with value networks for neural machine translation,” in Proceedings of the Advances in Neural Information Processing Systems, Long Beach, CA, USA, December 2017.', 'Over, “*e effects of human variation in DUC summarization evaluation, text summarization branches out,” Proceedings of the ACL-04 Workshop, vol.', 'Durme, “Annotated Gigaword,” in Proceedings of the AKBC-WEKEX, Montr´eal, Canada, 2012.', '28 Mathematical Problems in Engineering[71] K.', ', “Machines to read and comprehend,” in Proceedings of the Advances in Neural Information Processing Systems (NIPS), Montreal, Quebec, Canada, December 2015.', 'Artzi, “Newsroom: a dataset of 1.', '3 million summaries with diverse extractive strategies,” in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics, New Orleans, LA, USA, pp.', 'Lin, “ROUGE: a package for automatic evaluation of summaries,” in Proceedings of the 2004 ACL Workshop, Barcelona, Spain, July 2004.', 'Bagnell, “Improving multi-step prediction of learned time series models,” in Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp.', '3024–3030, Austin, TX, USA, 2015.', 'Bengio, Deep Learning, MIT Press, Cambridge, MA, USA, 2015.', 'Shazeer, “Scheduled sampling for sequence prediction with recurrent neural networks,” in Proceedings of the Annual Conference on Neural Information Processing Systems, pp.', '1171–1179, Montreal, Quebec, Canada, December 2015.', 'Denkowski, “*e Meteor metric for automatic evaluation of machine translation,” Machine Translation, vol.', 'Mathematical Problems in Engineering 29  Abstract—The security situation of the Internet of Things (IoT) is serious.', 'IoT encounters security problems more than traditional computer networks does.', 'The attributes of dispersity and mass of IoT require that approaches to IoT security should be dynamic.', 'Inspired by immunology, a novel approach to IoT security is proposed in this paper.', 'Traditional network security models are used for reference and special requests of IoT security are taken into account.', 'A dynamic defense frame for IoT security is formed in the proposed approach.', 'The links in the frame are correlated with relative data of IoT security.', 'Performance in biological immunology is applied into some links to make the proposed approach be adaptive to IoT environment.', 'The immunity-based antigen, self and detector in the real IoT environment are simulated.', 'They are adopted to imitate the mechanisms which are used to recognize pathogens in biological immune systems.', 'Simulation experiment results show that the proposed approach may provide a novel effective method to ensure IoT security.', 'Keywords- IoT Security; Immunology; Detection; Detector; Security Defense I.', 'INTRODUCTION The application of the Internet of Things (IoT) [1] increased sharply in recent years [2-5].', 'However, IoT has its special security requests other than the traditional problems harming computer, Internet and mobile communication network.', 'The terminals in the sense layer have wide varieties and are massive and widely distributed.', 'It makes that the security problems are amplified and affect IoT more severely [6].', 'Meanwhile, it causes that IoT is faced with more serious potential security threats than in the other networks.', 'Traditional security architectures can not fully satisfy the security requests.', 'A specifical approach to IoT security is in urgent need to cope with the complicated and changeful security situation of IoT.', 'In the interest of resolving the problems of information security, researchers introduced Artificial Immune System (AIS) [7-10] into the research field of information security.', 'AIS imitates the excellent mechanisms of Biological Immune System.', 'It has attracted much attention widely and been a research hotspot in the fields of bionics and computation intelligence.', 'Since 2002, International Conference on Artificial Immune Systems (ICARIS) has been held 12 times [11].', 'Researchers applied AIS into research fields of information security and network security and got many outstanding achievements [12-16].', 'In the following of this paper, the research background of IoT security is surveyed in the second section.', 'The state of security architecture research for traditional networks and IoT is discussed.', 'In the third section, this paper will propose a novel approach to cope with the complicated and changeful IoT environment based on immunology.', 'The proposed approach is expected to provide scientific reasons for the active defense strategy and effective defense measures.', 'RESEARCH BACKGROUND The current technologies for IoT security primarily come from the concepts of tradition network security.', 'Most of them focus on identity authentication, access control, privacy protection, encryption, security protocol, and etc [17-21].', 'The security ways are in the stage of passive defense.', 'To construct effective defense measures for IoT security, researchers proposed some methods and models of active defense.', 'Mirowski et al [22] developed a system of intrusion detection according to the past access frequency of thing labels.', 'Yang et al [23] presented a distributed intrusion detection method for nodes of wireless sense networks.', 'Wu et al [24] proposed a security transport model for IoT confidentiality.', 'The abovementioned system, method and model solved one aspect of the IoT security problems.', 'However, they don’t form an effective IoT security architecture and can not provide an appropriate approach to resolve problems of IoT security.', 'Standard and commercial organizations put forward a series of architectures and models for traditional network and information security.', 'These security architectures include ISO/OSI security system, TCP/IP security system, time-based PDR (Protection, Detection, Response) model, P2DR (Policy, Protection, Response, Recovery) model, P2DR2 (Policy, Protection, Detection, Response, Recovery) model, etc.', 'They realize the guarantee strategy of network security through the aspects of management and technology.', 'However, the traditional security architectures neglect entire construction of networks and information security system.', 'They emphasize simple protection measures and do not make full use of the other links of network security system [25].', 'They can not be copied blindly to construct the IoT security system because of the special attributes of IoT.', 'It is necessary to study a novel dynamical theoretical approach for IoT security to specially adapt the IoT network environment.', 'Dynamical approach to IoT security can use the traditional network security models for reference.', 'But it has to take the special requests of IoT security into account.', 'It must adopt effective ways to deal with the attributes of dispersity and mass in IoT.', 'In consideration of the excellent AIS attributes such as self-learning, self-adaptation, robustness, distribution, etc [10], it is valuable to use bionics principles of AIS to construct a novel dynamical approach to IoT security.', 'APPROACH TO IOT SECURITY The proposed approach adopts immune principles and mechanisms to simulate real defense environment of IoT security as an immune system.', 'It transforms its strategies of security defense along with the change of the security environment of IoT.', 'It makes the proposed approach be adaptive to real IoT.', 'The proposed approach is described in the following.', 'Frame of IoT Security To resolve the problems of static defense strategies, the proposed approach adopts dynamic and circular defense processes against security threats.', 'Its frame is shown in Fig.', 'It consists of five links.', 'The first link Security Threat Detection collects and analyzes original IoT network packets.', 'The other links perform based on the analysis results provided by the previous link.', 'All links serve IoT security.', 'Frame of proposed approach.', 'Simmulation of Immune Principles and Mechanisms The proposed approach captures original data from IoT traffic and analyzes the data to judge whether it contains security threats.', 'It simulates the principles and mechanisms of AIS to detect security threats in the original data.', 'The principles and mechanisms have the attributes of self-learning, self-adaptation, etc.', 'They make that the proposed approach can adapt the dynamic IoT security environments and discover mutated security threats.', 'The proposed approach simulates the following principles and mechanisms.', '1) Antigen simulation In immune systems, antigen is the original data to be recognized.', 'In the proposed approach, the original data of IoT traffic is simulated into antigen.', 'The original data comes from real-time IoT traffic.', 'The proposed approach captures IoT packets and gets their head data which is the signature.', 'Let the data set of antigen be A which is shown in (1).', 'The elements in A constitute antigen set to be recognized by the proposed approach.', 'A           a a l a toString Original Data Set ,    (1) Where, l is the length of an antigen, l N , N is the nature number set, toString( ) is a function to convert the original data set of IoT into binary strings.', 'A contains normal antigens and abnormal antigens which threaten IoT system.', 'Normal ones belong to self set which is defined as S.', 'Abnormal ones belongs to non-self set which is defined as N.', 'The antigens in N are abnormal and hide in all antigens.', 'The proposed approach uses immune principles and mechanisms to recognize the abnormal ones (non-self antigens).', 'Eventually, it maintains the IoT to be safe.', '2) Detector simulation To imitate the recognition mechanism in immune systems, detection elements of security threats are simulated into detectors.', 'Let the data set of detector be D which is shown in (2).', 'D ant ag cnt tp t fam     , , , , ,   (2) where, ant is the antibody string, ag is the living time, cnt is the amount of recognized antigens, tp is the class, t is the thickness.', '3) Match mechanism To simulate the match mechanism, a match method is needed to judge whether a antigen or detector matches another.', 'Presently, feasible matching methods include Hamming, Euclidean, r-Contiguous, etc.', 'The proposed approach adopts an improved match method of r-Contiguous.', 'It is shown in (3)    , , ,    , r r improved true f d a f d a false Otherwise       signature data of known IoT security threats.', 'They contain the precise signature information to recognize security threats.', '5) Self tolerance mechanism In immune systems, the mechanism of self tolerance is used to avoid that immune cells recognize self antigens.', 'In the proposed approach, new detectors may recognize self elements.', 'They can not be used to detect security threats directly.', 'They must accept the training process of the self-tolerance.', 'Let the function of the self-tolerance be which is shown in (4).', 'ftolerance ()       ( 1 ) 1 , .', '1 , tolerance I I r improved f D t r r D t r ag s S t f r s false       ,        (4) Where, t is the current moment,   is the period threshold of self-tolerance, DI is immature detector set, S is self set.', 'ftolerance ( ) returns the detector set which passed selftolerance.', 'The detectors in it do not match self antigens in a period of time.', 'They evolve to mature ones after they succeed to pass the self-tolerance.', 'In the process of the self-tolerance, new immature detectors are trained by all self elements.', 'If an immature one is matched by a self cell in a special period time, it fails to accept self-tolerance.', '6) Evolution mechanism of self Changeful IoT security environment cause that self set changes along with the dynamic security threats.', 'The initial self elements are gathered in a safe IoT environment.', 'When the proposed approach recognize new abnormal antigens, potential normal antigens are generated.', 'Therefore, self set should be expanded.', 'The expansion process is shown in (5).', 'It make that self set evolutes.', 'S S toSelf A     init Normal  (5) Where, is the data set of initial self elements, is the function to convert normal antigens into self elements, is the normal antigen set recognized by detectors.', 'Sinit toSelf ( ) ANormal C.', 'Dynamic Links of IoT Security 1) Detection of security threats Detectors and the simulative immune mechanisms are used to recognize abnormal antigens from real-time antigens in the proposed approach.', 'The detection process is shown in (6).', 'AHarm               a a A d D f d a true , , I r improved    Where, AHarm is a data set recognized by memory detectors.', '(6) A memory detector is activated when it detect a harmful antigen.', 'It begins to copy itself and accumulate its thickness.', '2) Danger computation In this link, the quantitative danger caused by security threats is computed.', \"It needs the elements of harmfulness of security threats, asset cost and memory detectors' thickness which is generated in the previous link.\", 'The danger computation process is shown in (7).', '2 illuminates the association relationship of IoT security threats, IoT asset, danger of threats, IoT security response grade and response policy.', 'The broken lines indicate how the proposed approach provides scientific reasons for policies and measures of IoT security defense.', 'The proposed approach produces security response grades and chooses security response polices.', 'Furthermore, it serves security defense.', 'SIMULATION EXPERIMENTS In the experiments, simulation software and devices were used to construct the simulation experiment environment which is shown in Fig.', 'The simulated network consisted of sense network, IoT gateway and a computer server which runs the proposed approach.', 'In the sense network, cloning attacks, mutated cloning attacks, replay attacks and mutated replay attacks were simulated.', 'The simulated server used AIS principles and mechanisms to detect security threats, compute danger, save security defense strategy library, respond to security threats and defend the security.', 'The sense network kept safe for three hours to collect initial self data set.', 'The simulated server detected simulated attacks and adopted defense measures.', 'Simulation experiment results are shown in Table I.', 'Table I shows that the proposed approach can detect security threats and change detectors to adapt the dynamic IoT environment.', 'Meanwhile, it indicates that appropriate defense strategies and measures can be adopted to deal with the relative security threats.', 'IoT has some special security requests.', 'Traditional security models for computer networks can not be applied in IoT security directly.', 'The proposed approach uses traditional security models for reference and adopts circular links to construct a dynamic defense system for IoT security.', 'The first link performs based on recognition of security threats.', 'The other links use the real data provided by its previous link.', 'The whole process for IoT security is dynamic.', 'Furthermore, principles in immunology are simulated and applied into the key links.', 'It makes that the proposed model can be adaptive to the IoT environment.', 'The proposed approach changes the defense ways for IoT security and provides a novel effective method to ensure IoT security.', '(3) Where, d D  , a A  ,   is a threshold, it meets 1 /               l r , r is the amount of binary chars of contiguous match, fr () is a single group match function of r-Contiguous.', 'If d matches a, fr () returns 1.', 'Otherwise, it returns 0.', '4) Evolution mechanism of detector To simulate the evolution process of immune cells, detectors are classified into immature detector DI, mature detector DM and memory detector DR.', 'The domain tp indicates which class the detector belongs to.', 'It is one of the class data set T which meets T i m r       , , .', 'Detectors adopt artificial immune mechanisms to evolve dynamically to adapt IoT security environment.', 'Immature detectors are generated with the mutation and recombination of gene sections or randomly.', 'They pass the detection of self-tolerance to evolve into mature detectors.', 'They can improve the diversity of detectors.', 'Mature detectors come from immature detectors.', 'They get the training of antigens.', 'If they do not match any harmful antigens in a period of time, they get the opportunity to evolve into memory detectors.', 'This embodies the process of self-learning and selfadaptation.', 'Memory detectors come from mature detectors and signature data of known IoT security threats.', 'They contain the precise signature information to recognize security threats.', '5) Self tolerance mechanism In immune systems, the mechanism of self tolerance is used to avoid that immune cells recognize self antigens.', 'In the proposed approach, new detectors may recognize self elements.', 'They can not be used to detect security threats directly.', 'They must accept the training process of the self-tolerance.', 'Let the function of the self-tolerance be which is shown in (4).', 'ftolerance ()  \\x03 \\x04  \\x03  \\x03  \\x03 \\x05 ( 1 ) 1 , .', '1 , tolerance I I r improved f D t r r D t r ag s S t f r s false \\t \\x06 \\x07 \\t   \\x15, \\x16 \\x07 \\t \\x17 \\x06 (4) Where, t is the current moment, \\x15 is the period threshold of self-tolerance, DI is immature detector set, S is self set.', 'ftolerance ( ) returns the detector set which passed selftolerance.', 'The detectors in it do not match self antigens in a period of time.', 'They evolve to mature ones after they succeed to pass the self-tolerance.', 'In the process of the self-tolerance, new immature detectors are trained by all self elements.', 'If an immature one is matched by a self cell in a special period time, it fails to accept self-tolerance.', '6) Evolution mechanism of self Changeful IoT security environment cause that self set changes along with the dynamic security threats.', 'The initial self elements are gathered in a safe IoT environment.', 'When the proposed approach recognize new abnormal antigens, potential normal antigens are generated.', 'Therefore, self set should be expanded.', 'The expansion process is shown in (5).', 'It make that self set evolutes.', 'S S toSelf A \\x06 \\x18 init  Normal \\x03 (5) Where, is the data set of initial self elements, is the function to convert normal antigens into self elements, is the normal antigen set recognized by detectors.', 'Sinit toSelf ( ) ANormal C.', 'Dynamic Links of IoT Security 1) Detection of security threats Detectors and the simulative immune mechanisms are used to recognize abnormal antigens from real-time antigens in the proposed approach.', 'The detection process is shown in (6).', 'AHarm \\x06 \\x16 \\x07 \\x19 \\x07 \\x17 \\x06 \\x04a a A d D f d a true , , I r improved  \\x03 \\x05 (6) Where, AHarm is a data set recognized by memory detectors.', 'A memory detector is activated when it detect a harmful antigen.', 'It begins to copy itself and accumulate its thickness.', '2) Danger computation In this link, the quantitative danger caused by security threats is computed.', \"It needs the elements of harmfulness of security threats, asset cost and memory detectors' thickness which is generated in the previous link.\", 'The danger computation process is shown in (7).', 'fdanger r f r t r h c \\x03 \\x06  .', ', \\x03 (7) Where, r is a memory detector, r.', 'h is the harmfulness of a security threat, c is the cost of IoT asset.', '3) Security Response and Defense The process of response and defense for IoT security is shown in Fig.', '2 illuminates the association relationship of IoT security threats, IoT asset, danger of threats, IoT security response grade and response policy.', 'The broken lines indicate how the proposed approach provides scientific reasons for policies and measures of IoT security defense.', 'The proposed approach produces security response grades and chooses security response polices.', 'Furthermore, it serves security defense.', 'SIMULATION EXPERIMENTS In the experiments, simulation software and devices were used to construct the simulation experiment environment which is shown in Fig.', 'The simulated network consisted of sense network, IoT gateway and a computer server which runs the proposed approach.', 'In the sense network, cloning attacks, mutated cloning attacks, replay attacks and mutated replay attacks were simulated.', 'The simulated server used AIS principles and mechanisms to detect security threats, compute danger, save security defense strategy library, respond to security threats and defend the security.', 'The sense network kept safe for three hours to collect initial self data set.', 'The simulated server detected simulated attacks and adopted defense measures.', 'Simulation experiment results are shown in Table I.', 'SIMULATION EXPERIMENT RESULTS ID Number Defense Measures Implementation Times 1 Logging 500 2 Alarm 309 3 Forensic 231 4 Modification 132 5 Part Deletion 95 6 Abandonment 53 Table I shows that the proposed approach can detect security threats and change detectors to adapt the dynamic IoT environment.', 'Meanwhile, it indicates that appropriate defense strategies and measures can be adopted to deal with the relative security threats.', 'CONCLUSION IoT has some special security requests.', 'Traditional security models for computer networks can not be applied in IoT security directly.', 'The proposed approach uses traditional security models for reference and adopts circular links to construct a dynamic defense system for IoT security.', 'The first link performs based on recognition of security threats.', 'The other links use the real data provided by its previous link.', 'The whole process for IoT security is dynamic.', 'Furthermore, principles in immunology are simulated and applied into the key links.', 'It makes that the proposed model can be adaptive to the IoT environment.', 'The proposed approach changes the defense ways for IoT security and provides a novel effective method to ensure IoT security.', 'ACKNOWLEDGMENT This work is supported by the National Natural Science Foundation of China (No.', '61103249), the Open Fund of Artificial Intelligence Key Laboratory of Sichuan Province (No.', '2011RYJ01), the Scientific Research Fund of Sichuan Provincial Education Department (No.', '13ZA0107, 13ZB0106 and 13TD0014) and the Construction Project of Science Research Innovation Team of Leshan Normal University.']\n",
            "time: 15.5 ms (started: 2022-01-18 18:57:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMDLOF_s0Jwh",
        "outputId": "eff38622-892c-4764-d9f9-b60e1fb15f77"
      },
      "source": [
        "#For NSS=4\n",
        "\n",
        "_c=text.strip()\n",
        "_c= _c.replace(\"\\n\",\" \") \n",
        "splitted = _c.split(\".\")\n",
        "splitted_c = []\n",
        "\n",
        "#take sentences that consist of more than 3 words\n",
        "for sentence in splitted:\n",
        "  if len(sentence.split(\" \")) > 3:\n",
        "    splitted_c.append(sentence)\n",
        "  \n",
        "\n",
        "#splitte_c = [sentence for sentence in splitted_c if len(sentence.split(\" \")) > 3]\n",
        "\n",
        "print(splitted_c)\n",
        "\n",
        "for x in range(4, len(splitted_c), 4):\n",
        "    splitted_c[x] = \"|\"+splitted_c[x].lstrip()\n",
        "_c = \".\".join(splitted_c)\n",
        "print(_c)\n",
        "\n",
        "splitted_by_bar_every_four_sentence = _c.split(\"|\")\n",
        "\n",
        "#print(splitted_by_bar_every_four_sentence)\n",
        "\n",
        "# Convert the corpus into a list of headlines\n",
        "#corpus=[i for i in _c.split('.') if i != '' and len(i.split(' '))>=4]\n",
        "corpus = splitted_by_bar_every_four_sentence\n",
        "print(corpus)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ABSTRACT  legality, sponsor public characters and, ultimately, lead to a bias within the public opinion [15,23]', ' Moreover, the plague of such spammers and bots leads to an ingenious and lucra- tive “underground economy”, where account vendors, their customers, and oblivious victims play a piece staging since the very introduction of social networks [33,34,37]', '  One of the most fascinating peculiarities of spambots is that they “evolve” over time, adopting sophisticated tech- niques to evade early-established detection approaches, such as those based on textual content of shared messages [24], posting patterns [35] and social relationships [17]', ' As evolv- ing spammers became clever in escaping detection, for in- stance by changing discussion topics and posting activities, researchers kept the pace and proposed complex models, such as those based on the interaction graphs of the accounts under investigation [20,43]', '  Noticeably, spambots evolution still goes on', ' Recent in- vestigations anecdotally highlight how new waves of social spambots are rising [15,47]', ' In this paper, we target these new waves, finding evidence of the difficulties for OSN users to distinguish between genuine and malicious accounts', ' We also highlight the difficulties for OSN administrators to take appropriate countermeasures against the takeover of evolv- ing spambots', ' Remarkably, a large number of tools and techniques have been proposed by Academia to detect OSN spambots [15,23]', ' Until recently, such tools have proved to be valid allies for spambots timely detection', ' Unfortunately, the characteristics of the new wave of social spambots are such that standard classification approaches, where a single account is evaluated according to a set of established features tested over known datasets, are no longer successful', ' In this work, we demonstrate this claim by investigating the per- formances of several state-of-the-art tools techniques when struggling against the latest wave of social spambots', ' The unsatisfactory results of the surveyed techniques call for new approaches capable of turning the tide of this long-lasting fight', '  Interestingly, we assist to a paradigm-shift in modeling and analyzing online accounts', ' Independently from each other, new research efforts were born, which leverage charac- teristics of groups of accounts – rather than those of a single account – as a red flag for anomalous behaviors', ' We provide a review of these prominent research directions, highlight- ing the new dimensions to sound out for successfully fighting  The paradigm-shift of social spambots  Evidence, theories, and tools for the arms race  Stefano Cresci † Roberto Di Pietro †‡§ Marinella Petrocchi † s', 'it  §  Department of Mathematics, University of Padua, Padua, Italy ¶ DTU Compute, Technical University of Denmark, Denmark  †  Institute for Informatics and Telematics, IIT-CNR, Pisa, Italy ‡ Nokia Bell Labs, Paris, France  Recent studies in social media spam and automation provide anecdotal argumentation of the rise of a new generation of spambots, so-called social spambots', ' Here, for the first time, we extensively study this novel phenomenon on Twitter and we provide quantitative evidence that a paradigm-shift ex- ists in spambot design', ' First, we measure current Twitter’s capabilities of detecting the new social spambots', ' Later, we assess the human performance in discriminating between genuine accounts, social spambots, and traditional spam- bots', ' Then, we benchmark several state-of-the-art tech- niques proposed by the academic literature', ' Results show that neither Twitter, nor humans, nor cutting-edge applica- tions are currently capable of accurately detecting the new social spambots', ' Our results call for new approaches capa- ble of turning the tide in the fight against this raising phe- nomenon', ' We conclude by reviewing the latest literature on spambots detection and we highlight an emerging common research trend based on the analysis of collective behaviors', ' Insights derived from both our extensive experimental cam- paign and survey shed light on the most promising directions of research and lay the foundations for the arms race against the novel social spambots', ' Finally, to foster research on this novel phenomenon, we make publicly available to the scien- tific community all the datasets used in this study', ' INTRODUCTION  The widespread availability and ease of use of Online So- cial Networks (OSN) have made them the ideal setting for the proliferation of fictitious and malicious accounts [28]', ' In- deed, recent work uncovered the existence of large numbers of OSN accounts that are purposely created to distribute unsolicited spam, advertise events and products of doubtful  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page', ' Copyrights for components of this work owned by others than ACM must be honored', ' Abstracting with credit is permitted', ' To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee', ' Request permissions from permissions@acm', '  WWW Web Science 2017, Perth, Australia  ⃝c 2017ACM', '475/123 4  Angelo Spognardi †¶ Maurizio Tesconi †  angsp@dtu', 'SI] 11 Jan 2017  ￼ dataset  genuine accountssocial spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 traditional spambots #3 traditional spambots #4 fake followers  test set #1 test set #2  description accounts  tweets year  8,377,522 2011 1,610,176 2012 428,542 2014 1,418,626 2011 145,094 2009 74,957 2014 5,794,931 2013 133,311 2009 196,027 2012  4,061,598 – 2,628,181 –  used in section  3', '1  4, 5 4, 5  statistics  ￼ ￼ verified accounts that are human-operated retweeters of an Italian political candidate spammers of paid apps for mobile devicesspammers of products on sale at Amazon', 'com training set of spammers used by Yang et al', ' in [43] spammers of scam URLs  automated accounts spamming job offersanother group of automated accounts spamming job offers simple accounts that inflate the number of followers of another account  mixed set of 50% genuine accounts + 50% social spambots #1 mixed set of 50% genuine accounts + 50% social spambots #3  3,474 991 3,457 464 1,000 100 433 1,128 3,351  1,982 928  ￼ ￼ against this novel generation of spambots', ' Our main contributions are:  \\t•\\tWe provide empirical evidence of the existence of a novel wave of Twitter spambots, which, up to now, has been just theorized [15]', '  \\t•\\tWe evaluate if, and to which extent, state-of-the-art detection techniques succeed in spotting such new spam- bots', '  \\t•\\tWe critically revise an emerging stream of research, which adopt features tied to groups of accounts rather than individual accounts features', '  \\t•\\tWe leverage results of a crowdsourcing spambot de- tection campaign for drawing new guidelines for the annotation of datasets comprising social spambots', '  \\t•\\tFinally, we publicly release to the scientific community an annotated dataset1, consisting of genuine accounts, traditional spambots, and —for the first time— the novel social spambots', ' DATASETS We describe the different Twitter datasets that constitute the real-world data used in our experiments', ' Table 1 re- ports the name of the datasets, their brief description, and the number of accounts and tweets they feature', ' The year represents the average of the creation years of the accounts that belong to the dataset', ' Thegenuine accountsdatasetisarandomsampleofgen- uine (human-operated) accounts', ' We randomly contacted Twitter users by asking a simple question in natural lan- guage', ' All the replies to our questions were manually veri- fied and all the 3,474 accounts that answered were certified as humans', ' The accounts that did not answer to our ques- tion were discarded and are not used in this study', ' The social spambots #1 dataset was created after ob- serving the activities of a novel group of social bots that we discovered on Twitter during the last Mayoral election in Rome, in 2014', ' One of the runners-up employed a so- cial media marketing firm for his electoral campaign, which made use of almost 1,000 automated accounts on Twitter to publicize his policies', ' Surprisingly, we found such automated 1 http://mib', 'html  accounts to be similar to genuine ones in every way', ' Every profile was accurately filled in with detailed – yet fake – personal information such as a (stolen) photo, (fake) short- bio, (fake) location, etc', ' Those accounts also represented credible sources of information since they all had thousands of followers and friends, the majority of which were gen- uine users2', ' Furthermore, the accounts showed a tweeting behavior that was apparently similar to those of genuine ac- counts, with a few tweets posted every day, mainly quotes from popular people, songs, and YouTube videos', ' However, every time the political candidate posted a new tweet from his official account, all the automated accounts retweeted it in a time span of just a few minutes', ' Thus, the political candidate was able to reach many more accounts in addition to his direct followers and managed to alter Twitter engage- ment metrics during the electoral campaign', ' Amazingly, we also found tens of human accounts who tried to engage in conversation with some of the spambots', ' The most com- mon form of such human-to-spambot interaction was rep- resented by a human reply to one of the spambot tweets quotes', ' We also discovered a second group of social bots, which we labeled social spambots #2, who spent several months promoting the #TALNTS hashtag', ' Specifically, Talnts is a mobile phone application for getting in touch with and hiring artists working in the fields of writing, digital photog- raphy, music, and more', ' The vast majority of tweets were harmless messages, occasionally interspersed by tweets men- tioning a specific genuine (human) account and suggesting him to buy the VIP version of the app from a Web store', '  Further, we uncovered a third group of social bots, social spambots #3, which advertise products on sale on Ama- zon', ' The deceitful activity was carried out by spam- ming URLs pointing to the advertised products', ' Similarly to the retweeters of the Italian political candidate, also this family of spambots interleaved spam tweets with harmless and genuine ones', '  We exploited a Twitter crawler to collect data about all the accounts we suspected to belong to the three groups of social spambots', ' All the accounts collected in this process have then undergone an internal manual verification phase to certify their automated nature', ' Among all the distinct retweeters of the Italian political candidate, 50', '05% (991 ac- counts) were certified as spambots', '50% (3,457  2This was made possible also by the adoption of social engineering techniques, such as the photo of a young attractive woman as the profile picture and the occasional posting of provocative tweets', '  Table 1: Statistics about the datasets used for this study', '  ￼ ￼  ￼ genuine accountssocial spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 fake followers  3,474 994 3,457 467 1,000 100 3,351  6 (0', '5%)  accounts  ￼ dataset total  alive  3,353 (96', '4%)  deleted  115 (3', '1%)  suspended  ￼ ￼ Table 2: Statistics about alive, deleted, and suspended accounts, for different groups of genuine and malicious accounts', '  accounts) of the accounts who tweeted the #TALNTS hash- tag resulted as spambots', '29% (464 accounts) of the accounts that tweeted suspicious Amazon', 'com URLs were also certified as spambots', ' The three sets of accounts represent our ground truth of novel social spambots', '  Our internal manual annotation has been carried out by comparing every account to all the others, in order to high- light possible similarities and common behaviors', ' This is in contrast with the typical annotation process where ac- counts are labeled one-by-one and by solely exploiting the characteristics of the account under investigation', '  In addition to genuine users and social spambots, we also collected several datasets of traditional spambots', ' Such datasets are used throughout the paper as a strong base- line', ' The traditional spambots #1 dataset is the training set used in [43], kindly provided to us by the authors of that work', ' In [43], the dataset has been used to train a machine learning classifier for the detection of evolving Twitter spam- bots', ' Accounts belonging to the traditional spambots #2 dataset are rather simplistic bots that repeatedly mention other users in tweets containing scam URLs', ' To lure users into clicking the malicious links, the content of their tweets invite the mentioned users to claim a monetary prize', ' The traditional spambots #3 and traditional spambots #4 datasets are related to 2 different groups of bots that re- peatedly tweet about open job positions and job offers', '  Fake followers are another kind of malicious accounts that recently gained interest both from platform administrators and from the scientific world [12]', ' Given that fake followers are rather simplistic in their design and functioning, they can serve as a weak baseline against which to compare so- cial spambots', ' In April, 2013, we bought 3,351 fake accounts from three different Twitter online markets, namely fastfol- lowerz', ' All the accounts acquired in this way have been merged in order to obtain the fake followers dataset used in this study', '  By considering a diverse set of spammer accounts we have captured many of the different dimensions currently exploited by spambots and tamperers to perpetrate their illicit activ- ities', ' In detail, we have considered (i) fake follower frauds, (ii) retweet frauds, (iii) hashtag promotion, (iv) URL spam- ming, (v) scamming, and (vi) spam of generic messages', '  RQ1 – To what extent is Twitter currently capable of de- tecting and removing social spambots?  Interesting insights can be gained by comparing the rate at which Twitter accounts are removed, for different types of malicious accounts', ' The intuition is that accounts that are easily identified as malicious can be rapidly removed by platform administrators', ' Thus, in this experiment, we let different types of accounts behave for a rather long amount of time (i', ' Then, we check whether Twitter man- aged to identify such accounts as malicious and to remove them from the platform', ' We perform this experiment on our set of genuine accounts, on our 3 groups of social spambots, on 2 groups of traditional spambots, and on the group of fake followers', '  In order to perform this experiment, we exploited Twit- ter’s responses to API calls and, particularly, the Twitter er- ror codes', ' Given a query to a specific account, Twitter’s API replies with information regarding the status of the queried account', ' Specifically, accounts that are suspected to perform malicious activities get suspended by Twitter', ' API queries to a suspended account result in Twitter responding with the error code 63', ' API queries to accounts that have been deleted by their original owner result in Twitter respond- ing with the error code 50', ' Instead, for accounts that are neither suspended nor deleted, Twitter replies with the full metadata information of the account, without issuing error codes', ' By exploiting this response mechanism, we were able to measure the survivability of the different groups of ac- counts', ' Results of this experiment are reported in Table 2 and are pictorially depicted in Figure 1', '  As shown in Table 2, genuine accounts feature a very high survival rate (96', ' In addition, among the no longer available accounts, the vast majority have been deleted by the original owner, rather than suspended by Twitter', ' These results are quite intuitive, by considering that legitimate ac- counts rarely perform any kind of malicious activity', ' Con- versely, the simplest kind of malicious accounts, fake fol- lowers, have mostly been detected and suspended by Twit- ter', ' The same also applies to one of the two groups of traditional spambots, identified as traditional spambots #2 in Table 2, which features a suspension rate as high as 99%', ' The most interesting results are however related to those kinds of malicious accounts that better mimic human behaviors', ' So far, traditional spambots #1 have largely managed to evade suspension, despite dating back to 2009', ' Indeed, only 8', '6% of the bots have been suspended, while 88', '9% of them are still alive', ' This seems to suggest that Twitter’s spambot detection mechanisms are still unable to accurately identify such accounts, while recent solutions pro- posed by Academia have succeeded in this task [43]', ' Twit-  3', '1  REAL-WORLD EXPERIMENTATION Twitter monitoring  A first assessment of the extent and the severity of Twit- ter social spambots problem can be obtained by measuring Twitter’s capacity of detecting and removing them from the platform', ' This section thus answers the research question:  ￼ ￼ ￼ dataset  social spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 fake followers  alive −1', '1%***  accounts  deleted suspended −3', '4%***  ￼ ￼ Figure 1: Survival rates for different types of accounts', '  ter’s performance in suspending malicious accounts is even worse if we consider social spambots', ' All the 3 groups of so- cial spambots feature very high survival rates, respectively 95', ' Even if the difference between the survival rate of social spambots and that of traditional spambots #1 is marginal, these results nonetheless suggest an increased difficulty for the detection of social spambots', ' Table 3 also reports the results of a comparison between the ratios of alive, deleted, and suspended accounts between spambots and genuine accounts', ' As shown, social spam- bots feature very small differences with respect to genuine accounts (∼ ±3%)', ' Some of these differences are not even statistically significant, according to a chi-square test', ' Tra- ditional spambots #1 have differences ∼ ±8% that are highly significant (p < 0', '01) for alive and suspended ac- counts', ' Instead, traditional spambots #2 and fake fol- lowers show massive differences: ∼ ±96% and ∼ ±72%, respectively', '  Figure 1 shows results of the survivability experiment, with respect to the account age3', ' This can allow to un- derstand if temporal patterns exist in the way malicious accounts are created, and if Twitter’s mechanisms for sus- pending malicious accounts are related to an account’s age', ' For instance, Twitter might be better in detecting and sus- pending older accounts than newer ones', ' However, this hy- pothesis can be ruled out by considering that 99% of tradi- tional spambots #2 accounts have been suspended despite being younger than most of the social spambots', ' Overall, an analysis of Figure 1 shows that account suspensions seem to depend on the type of the account, its design and behavior, rather than on its age', '  Results reported in this first experiment already reveal interesting differences between social spambots, traditional spambots, and fake followers', ' Notably, social spambots ap- pear to be more similar to genuine accounts than to tradi- tional spambots, with regards to Twitter suspensions', '  3Account age is computed as the number of days between the ac- count’s creation date and the day we performed the experiment', '  Table 3: Effect size and statistical significance of the difference between the survivability results of malicious accounts with respect to those of genuine accounts', '2 Crowdsourcing: tasks and results  This section addresses the following research questions:  RQ2 – Do humans succeed in detecting social spambots in the wild?  RQ3 – Do they succeed in discriminating between tradi- tional spambots, social spambots, and genuine accounts?  Even if Twitter users were generally capable of distin- guishing between traditional spambots and genuine accounts, they might still find it difficult to spot social spambots in the wild', ' If confirmed, this would provide additional evi- dence of the evolutionary step characterizing the new social spambots with respect to traditional ones', '  Figure 2: Dataset for the crowdsourcing experiment', '  To answer these research questions, we asked a large set of real-world users to classify the accounts in our datasets', ' To obtain a large and diverse set of users, we recruited con- tributors from the CrowdFlower4 crowdsourcing platform', ' Figure 2 shows the distribution of the 4,428 accounts that we have employed for this crowdsourcing experiment, picked up from the datasets in Section 2', ' Contributors were asked to assign to each account one of the following classes: (i) spambot, (ii) genuine, and (iii) unable to classify', ' The lat- ter class (iii) has been inserted to deal with Twitter accounts possibly getting deleted, suspended, or protected5 while our crowdsourcing task was ongoing', '  Notably, our experiment marks a difference with those typically carried out with crowdsourcing', ' In fact, crowd- sourcing tasks are typically aimed at creating a ground truth (i', ', labeled) dataset for later use', ' For instance, crowdsourc- ing is often used to create large training-sets for machine learning algorithms', ' Here, instead, the datasets are labeled in advance', ' Thus, by asking contributors to (re-)classify our datasets, we are actually evaluating their ability to spot the different types of accounts', '  4 https://www', 'com/  5Protected accounts are those accounts whose tweets and timeline are not publicly visible', '  ***p < 0', '01, **p < 0', '05, *p < 0', '1  ￼ ￼ ￼  ￼ 1,385 328 0  0 0  0 0 1,267 110  detection results  ￼ type  traditional spambots social spambots genuine accounts  accounts♯ 1,516  1,393 1,377  TP TN FP  FN  131 1,065 0  Accuracy  0', '9201  Fleiss’ kappa (κ) 0', '410  got  4', '7/5  ￼ ￼ ♯: The total number of accounts considered is 4,286 instead deleted, suspended, or protected during our campaign', '  of 4,428 because 142 accounts (3', '2%)  Table 4: Results of the crowdsourcing campaign on spambots detection', '  ￼ Enforcing results reliability', ' We only recruited contrib- utors who were tech-savvy and Twitter users themselves, in order to be reasonably sure about their knowledge of Twitter and its dynamics', ' Furthermore, we required each account to be classified by at least 3 different contributors, with the final class decided by majority voting', ' We also fixed to 100 the upper threshold of the number of accounts that a single contributor could classify', ' In this way, we have obtained re- dundant results from a broad set of contributors', ' Then, in order to further guarantee the reliability of our crowdsourc- ing results, we designed a set of “test” (or “gold”) questions aimed at evaluating the quality of contributors’ answers', ' A test question is one for which the correct answer is already known by the system', ' Within the crowdsourcing platform, such questions are indistinguishable from standard ones and are randomly mixed among all the questions, so that contrib- utors cannot know whether they are answering to a test or to a standard question', ' Contributors’ answers to test questions were checked against the known correct answers', ' Only the trusted contributors who answered correctly to more than the 70% of the test questions have been considered in our study', ' Our test questions consist of accounts whose nature is “easily” recognizable, and specifically: (i) a set of traditional spambots sampled from the dataset of Yang et al', ' [43], (ii) a subset of genuine accounts, and (iii) a set of suspended, deleted, and protected accounts', ' Notably, by designing test questions with traditional spambots and genuine accounts, and by enforcing the policy of at least 70% correct answers, we can guarantee that all our trusted contributors are typi- cally able to detect traditional spambots and to distinguish them from genuine accounts', ' This further strengthens the results of their classification of the novel social spambots', '  Table 5 shows a recap of the settings used in our crowd- sourcing campaign', ' The thorough description of our cam- paign, with the complete set of instructions, a list of exam- ple accounts, and the task preview, is available online6', ' The campaign completed when each of the 4,428 accounts was classified by 3 different trusted contributors', '  Instructions clear Test questions fair Ease of jobPay  Overall  ￼ ￼ Num', ' accounts to classify Min', ' contributors per account Max', ' answers per contributor Num', ' accuracy threshold Reward  4,428 3 100 25 70% 0', '1 US$ per 5 accounts classified  Figure 3: Crowdsourcing campaign details', '  contributors the possibility to evaluate crowdsourcing cam- paigns for: (i) clarity of instructions, (ii) fairness of the test questions, (iii) ease of the task, and (iv) appropriateness of the payment', ' Out of the 247 participating contributors, 60 of them (∼ 24%) evaluated our campaign, leading to a convincing aggregated score of 3', '7/5, as shown in detail in Table 6', ' Our campaign costed us 410 US$ in total', '  The most interesting results of our crowdsourcing cam- paign are undoubtedly related to the detection performance of our human contributors', ' As reported in Table 4, overall, the human annotators obtained an accuracy of less than 0', '24 on the social spambots, with more than 1,000 False Nega- tives (FN), meaning that contributors classified more than 1,000 accounts as genuine, when they actually belonged to the dataset of the last generation of spambots', ' Human de- tection performances for the two other groups of accounts, namely traditional spambots and genuine accounts, are in- stead quite satisfactory, with an accuracy of 0', ' These important results further highlight the existence of a striking difference between traditional and so- cial spambots', ' More worryingly, they also suggest that hu- mans might not be able to detect social spambots in the wild, and to distinguish them from genuine accounts', '  Given that each account under investigation has been clas- sified by at least 3 different contributors, we have also com- puted the Fleiss’ kappa (κ) inter-rater agreement metric [19]', ' All inter-rater agreement metrics measure the level of agree- ment of different annotators on a task', ' The level of agree- ment can be also interpreted as a proxy for the difficulty of a task', ' In our experiment, human contributors showed a  Table 6: Contributors’ evaluation of our campaign', '  ￼ ￼ (a) Top 20 countries', '  (b) Answers per contributor', '  ￼ ￼ Table 5: Crowdsourcing campaign settings', '  Results of the crowdsourcing campaign', ' Overall, we collected 13,284 answers given by 247 trusted contributors from 42 different countries', ' Figure 3(a) shows the distribu- tion of answers per country', ' Figure 3(b) depicts the distri- bution of answers per contributor', ' CrowdFlower also gives  6 http://wafi', 'it/fake/fake/crowdflower/instructions/  ￼  ￼ ￼ technique  test set #1  Twitter countermeasures Human annotators BotOrNot? [14]C', ' Yang et al', ' [43] Miller et al', ' [30]  Ahmed et al', ' [2]♯ Cresci et al', ' [13]  test set #2  Twitter countermeasures Human annotators BotOrNot? [14]C', ' Yang et al', ' [43] Miller et al', ' [30]  Ahmed et al', ' [2]♯ Cresci et al', ' [13]  type Precision mixed 1', '000  detection results  Recall Specificity Accuracy  0', '929  F-Measure  0', '923  MCC  0', '867  ￼ manual supervised supervised unsupervised unsupervised unsupervised  mixed manual supervised supervised unsupervised unsupervised unsupervised  0', '000  ￼ ￼ ♯: Modified by employing fastgreedy instead of MCL for the graph clustering step', '  Table 7: Comparison among the spambot detection techniques, tools, and algorithms surveyed in this study', ' For each test set, the highest values in each evaluation metric are shown in bold', '  decent agreement for the classification of genuine accounts, with κ = 0', ' Instead, they showed very little agree- ment while classifying traditional spambots, as represented by κ = 0', ' This interesting result shows that, overall, the human contributors were able to correctly detect tra- ditional spambots, as shown by the 0', '91 accuracy, but also that contributors rarely agreed on the class', ' Surprisingly, we measured a slightly higher agreement for the classification of social spambots than for traditional ones, with κ = 0', ' These results imply that humans generally failed in classify- ing social spambots (accuracy = 0', '2355) and, furthermore, that they also were more in agreement on this mistake than they were when (correctly) classifying traditional spambots', '  Annotation guidelines for spambots detection', ' De- spite the recent advances in machine learning-based detec- tion systems, manual verification of accounts to assess their degree of automation is still carried out by platform adminis- trators [15]', ' In [41], it is reported that human experts “con- sistently produced near-optimal results” on a dataset of tra- ditional spambots', ' However, the results of our crowdsourc- ing experiment confirmed that the traditional “account-by- account” annotation process used by human workers to eval- uate social media accounts is no longer viable when applied to the detection of the novel wave of social spambots', ' Given the importance of manual annotation for the creation of ground-truth datasets and for double-checking suspicious ac- counts on social networking platforms, we call for the adop- tion of new annotation methodologies that take into account the similarities and synchronized behaviors of the accounts', ' We have adopted a practical implementation of this method- ology to annotate our datasets of social spambots', ' In par- ticular, we have compared the timelines of large groups of accounts, in order to highlight tweeting similarities among them', ' By comparing the behaviors of different accounts, rather than by analyzing them one by one, we were able to spot the social spambots among all the collected accounts, as thoroughly described in Section 2', ' Therefore, we envisage the possibility to adopt this methodology, as well as similar ones, in order to safeguard the manual annotation process from elusive social spambots', ' ESTABLISHED TECHNIQUES  So far, we have demonstrated that neither Twitter nor human operators are currently capable of identifying novel social spambots', ' Here, we investigate whether established tools and techniques are able to succeed in this task', ' Thus, our research question is:  RQ4 – Are state-of-the-art scientific applications and tech- niques able to detect social spambots?  The BotOrNot? service', ' BotOrNot? is a publicly- available service7 to evaluate the similarity of a Twitter ac- count with the known characteristics of social spambots [14]', ' It has been developed by the Indiana University at Bloom- ington and it was released in May 2014', ' Claimed capable of detecting social spambots [15], at the time of writing it was the only publicly-available social spambot detection system', ' BotOrNot? leverages a supervised machine-learning classi- fier that exploits more than 1,000 features of the Twitter account under investigation', ' Specifically, it employs off-the- shelf supervised learning algorithms trained with examples of both humans and bots behaviors, based on the Texas A&M dataset [25] with 15,000 examples of each class and millions of tweets', ' Similarly to most already established techniques, BotOrNot? performs its analyses on an account- by-account basis', ' Despite being specifically designed for the detection of social spambots, authors state that the detec- tion performances of BotOrNot? against evolved spambots might be worse than those reported in [14]', ' Here, we aim at evaluating this point by querying the BotOrNot? service with our sets of genuine and social spambot accounts', ' As shown in Table 7, BotOrNot? achieves rather unsatisfactory results for the accounts of both test set #1 and test set #2 (such datasets are described in Table 1)', ' Its detection performances are particularly bad for the accounts of test set #1 – where the spambots are from the social spambots #1 group', ' The low values of F-Measure and Mathews Corre- lation Coefficient (MCC), respectively 0', '174, are mainly due to the low Recall', ' In turn, this represents a ten- dency of labeling social spambots #1 as genuine accounts', '  7 http://truthy', 'edu/botornot/  ￼  ￼ Supervised spambot classification', ' Among the many supervised classification approaches to spambot detection proposed in recent years by Academia, we decided to ex- periment with the one presented by C', ' Yang et al', ' in [43], since it focuses on the detection of evolving Twitter spam- bots', ' Thus, it is interesting to evaluate if the system recently presented in [43] is actually able to detect the sophisticated social spambots', ' This supervised system provides a machine learning classifier that infers whether a Twitter account is genuine or spambot by relying on account’s relationships, tweeting timing and level of automation', ' We have repro- duced such a classifier by implementing and computing all the features proposed in [43], and by training the classifier with its original dataset', ' Results in Table 7 show that the system fails to correctly classify the novel social spambots', ' Similarly to the results of the BotOrNot? service, the worst results of this system in both test set #1 and test set #2 are related to the Recall metric', ' This means that also this classifier labeled social spambots as genuine accounts', '  Unsupervised spambot detection via Twitter stream clustering', ' Our initial claim, supported by preliminary work [15,47], is that social spambots might be so sophisti- catedly designed to make it very difficult to distinguish them from genuine accounts, if observed one by one', ' If demon- strated, this claim would imply that supervised classifica- tion approaches are intrinsically worse than unsupervised ones for the detection of social spambots', ' For this reason, we have also experimented with unsupervised approaches for spambot detection', ' The approach in [30] considers vec- tors made of 126 features extracted from both accounts and tweets as input of modified versions of the DenStream [7] and StreamKM++ [1] clustering algorithms, to cluster fea- ture vectors of a set of unlabeled accounts', ' We have imple- mented the system proposed in [30] to cluster the accounts of our 2 test sets', ' As shown in Table 7, this achieved the worst performances among all those that we have benchmarked in this study', ' Low values of both Precision and Recall mean incomplete and unreliable spambot detection', ' Among the 126 features, 95 are based on the textual content of tweets', ' However, novel social spambots tweet contents similar to that of genuine accounts (e', ', retweets of genuine tweets and famous quotes)', ' For this reason, an approach almost solely based on tweet content will not be able to achieve satisfactory results', '  Unsupervised spambot detection via graph cluster- ing', ' The approach in [2] exploits statistical features related to URLs, hashtags, mentions and retweets', ' Feature vectors generated in this way are then compared with one another via an Euclidean distance measure', ' Distances between ac- counts are organized in an adjacency matrix, which is later used to construct an undirected weighted graph of the ac- counts', ' Then, graph clustering and community detection algorithms are applied in order to identify groups of similar accounts', ' Graph clustering is done by employing the Markov cluster algorithm (MCL) [38]', ' We fully implemented this so- lution and we experimented with our datasets', ' However, the approach failed to identify 2 distinct clusters, since accounts of both our test-sets were assigned to a single cluster', ' We also performed a grid search simulation in order to test the best parameter configuration for MCL8, but to no avail', ' To achieve effective detection results, instead of the MCL, we  8MCL admits 2 fundamental parameters: inflation and expansion', '  established work  Yardi, Romero et al', ' [45] 2009  Benevenuto et al', ' [3, 4, 17] 2009-12 K', ' Lee, Caverlee 2010-11 et al', ' [24,25]Stringhini et al', ' [33–35] 2010-13  Viswanath, Mislove, 2011 Gummadi et al', ' [39]Stein et al', ' [32] 2011 Thomas et al', ' [36] 2011 Gao et al', ' [16] 2012 Cao et al', ' [8] 2012 Xie et al', ' [42] 2012 C', ' Yang et al', ' [43] 2013 Wang et al', ' [41] 2013 S', ' Lee et al', ' [26,27] 2013-14 Z', ' Yang et al', ' [44] 2014 Liu et al', ' [29] 2014 Paradise et al', ' [31] 2014 Cresci et al', ' [11,12] 2014-15 Ferrara et al', ' [6, 14] 2014-16  emerging trends  ￼ This table does not aim to be complete, but rather to testify the emergence of a new trend of research', '  Table 8: Recent work in spambot detection', '  adopted the fastgreedy community detection algorithm [10]', ' As reported in Table 7, our modified implementation proved effective in detecting social spambots, with an MCC = 0', '886 for test set #1 and MCC = 0', '847 for test set #2', ' EMERGING TRENDS  As shown in Table 7, the established works benchmarked in Section 4 largely failed to detect the new wave of so- cial spambots', ' In turn, these results call for novel analytic tools able to keep pace with the latest evolutionary step of spambots', ' Thus, in this section we revise the most recent literature on spambots detection with the aim of answering the research question:  RQ5 – Is it possible to find a new dimension over which to fight and overcome the novel social spambots?  Traditional spambot detection systems typically rely on the application of well-known machine learning algorithms on the accounts under investigation', ' However, since 2013, a number of research teams independently started to for- malize new approaches for detecting the coordinated and synchronized behavior that characterizes groups of auto- mated malicious accounts [5]', ' Table 8 groups such tech- niques as emerging trends', ' Despite being based on different key concepts, these studies investigate groups of accounts as a whole, marking a significant difference with the previous literature', ' Table 9 reports on the new concepts introduced by these emerging works', ' Focusing on groups has the ad- vantage that, no matter how sophisticated a single spambot can be, a large enough group of spambots will still leave traces of automation, since they do have a common goal (e', ', increasing someone’s reputation score)', ' By perform- ing analyses at the group level, this emerging trend might be able to significantly raise the bar for social spambots to evade detection', ' To support the claim, we experiment with work in [13,40]', '  Tamper Detection in Crowd Computations', ' The con- tribution by Viswanath et al', ' in [40] checks whether a given group of accounts (e', ', retweeters of another account, re- viewers of a venue on Yelp) contains a subset of malicious  Beutel, Faloutsos,et al', ' [5,18,21,22]Cao et al', ' [9] 2014 Yu et al', ' [46] 2015  Viswanath, Mislove, 2015 Gummadi et al', ' [40]Cresci et al', ' [13] 2016  2013-16  ￼ ￼  ￼ work  key concept  detection of lockstep behaviors  anomalies in synchronicity and normality  detection of loosely synchro- nized actions  detection of latent group anomalies in graphs  distance between distributions of reputation scores  similarity between digital DNA sequences  computed in [13] by measuring the Longest Common Sub- string (LCS)—that is, the longest DNA substring shared by all the accounts of the group', ' Accounts that share a suspi- ciously long DNA substring are then labeled as spambots', ' Notably, although working at group level, [13] is capable of spotting single spambot accounts', ' For this reason, we have been able to compare this technique with the ones previ- ously benchmarked in Table 7', ' Applying the technique to our datasets, the similarity curve of genuine accounts is sig- nificantly different from that of social spambots, as shown in Figure 4(c)', ' More specifically, as measured by the LCS metric, social spambots #1 and #3 feature a level of sim- ilarity much higher than that of genuine accounts', ' Results reported in Table 7 demonstrate that the digital DNA-based technique [13] achieves excellent detection performances', '  The compelling features of the emerging techniques listed in this section represent a fertile ground for fighting the novel social spambots', ' We can observe a paradigm-shift for research and development of spambot detection systems, which may exploit such new concepts to achieve better re- silience and robustness, to withstand the next evolution of social media spambots', ' CONCLUDING REMARKS  Our long-lasting experiment on malicious accounts sur- vival rate in Twitter demonstrated that spambot detection is still an open issue', ' Moreover, the already difficult prob- lem to detect spambots in social media is bound to worsen, since the emergence of a new wave of so-called social spam- bots', ' By accurately mimicking the characteristics of gen- uine users, these spambots are intrinsically harder to detect than those studied by Academia in the past years', ' In our experiments, neither humans nor state-of-the-art spambot detection applications managed to accurately detect the ac- counts belonging to this new wave of spambots', ' Indeed, our experiments highlighted that the majority of existing au- tomated systems, as well as crowdsourcing, erroneously la- bel social spambots as genuine (human-operated) accounts', ' We demonstrated the need for novel analytic tools capable of turning the tide in the arms race against such sophis- ticated spambots', ' One promising research direction stems from the analysis of collective behaviors', ' We highlighted a few emerging approaches that analyze groups as a whole, rather than individuals', ' The promising outcome of these novel approaches clearly indicates that this is a favorable research avenue']\n",
            "ABSTRACT  legality, sponsor public characters and, ultimately, lead to a bias within the public opinion [15,23]. Moreover, the plague of such spammers and bots leads to an ingenious and lucra- tive “underground economy”, where account vendors, their customers, and oblivious victims play a piece staging since the very introduction of social networks [33,34,37].  One of the most fascinating peculiarities of spambots is that they “evolve” over time, adopting sophisticated tech- niques to evade early-established detection approaches, such as those based on textual content of shared messages [24], posting patterns [35] and social relationships [17]. As evolv- ing spammers became clever in escaping detection, for in- stance by changing discussion topics and posting activities, researchers kept the pace and proposed complex models, such as those based on the interaction graphs of the accounts under investigation [20,43].|Noticeably, spambots evolution still goes on. Recent in- vestigations anecdotally highlight how new waves of social spambots are rising [15,47]. In this paper, we target these new waves, finding evidence of the difficulties for OSN users to distinguish between genuine and malicious accounts. We also highlight the difficulties for OSN administrators to take appropriate countermeasures against the takeover of evolv- ing spambots.|Remarkably, a large number of tools and techniques have been proposed by Academia to detect OSN spambots [15,23]. Until recently, such tools have proved to be valid allies for spambots timely detection. Unfortunately, the characteristics of the new wave of social spambots are such that standard classification approaches, where a single account is evaluated according to a set of established features tested over known datasets, are no longer successful. In this work, we demonstrate this claim by investigating the per- formances of several state-of-the-art tools techniques when struggling against the latest wave of social spambots.|The unsatisfactory results of the surveyed techniques call for new approaches capable of turning the tide of this long-lasting fight.  Interestingly, we assist to a paradigm-shift in modeling and analyzing online accounts. Independently from each other, new research efforts were born, which leverage charac- teristics of groups of accounts – rather than those of a single account – as a red flag for anomalous behaviors. We provide a review of these prominent research directions, highlight- ing the new dimensions to sound out for successfully fighting  The paradigm-shift of social spambots  Evidence, theories, and tools for the arms race  Stefano Cresci † Roberto Di Pietro †‡§ Marinella Petrocchi † s.|it  §  Department of Mathematics, University of Padua, Padua, Italy ¶ DTU Compute, Technical University of Denmark, Denmark  †  Institute for Informatics and Telematics, IIT-CNR, Pisa, Italy ‡ Nokia Bell Labs, Paris, France  Recent studies in social media spam and automation provide anecdotal argumentation of the rise of a new generation of spambots, so-called social spambots. Here, for the first time, we extensively study this novel phenomenon on Twitter and we provide quantitative evidence that a paradigm-shift ex- ists in spambot design. First, we measure current Twitter’s capabilities of detecting the new social spambots. Later, we assess the human performance in discriminating between genuine accounts, social spambots, and traditional spam- bots.|Then, we benchmark several state-of-the-art tech- niques proposed by the academic literature. Results show that neither Twitter, nor humans, nor cutting-edge applica- tions are currently capable of accurately detecting the new social spambots. Our results call for new approaches capa- ble of turning the tide in the fight against this raising phe- nomenon. We conclude by reviewing the latest literature on spambots detection and we highlight an emerging common research trend based on the analysis of collective behaviors.|Insights derived from both our extensive experimental cam- paign and survey shed light on the most promising directions of research and lay the foundations for the arms race against the novel social spambots. Finally, to foster research on this novel phenomenon, we make publicly available to the scien- tific community all the datasets used in this study. INTRODUCTION  The widespread availability and ease of use of Online So- cial Networks (OSN) have made them the ideal setting for the proliferation of fictitious and malicious accounts [28]. In- deed, recent work uncovered the existence of large numbers of OSN accounts that are purposely created to distribute unsolicited spam, advertise events and products of doubtful  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page.|Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.|WWW Web Science 2017, Perth, Australia  ⃝c 2017ACM.475/123 4  Angelo Spognardi †¶ Maurizio Tesconi †  angsp@dtu.SI] 11 Jan 2017  ￼ dataset  genuine accountssocial spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 traditional spambots #3 traditional spambots #4 fake followers  test set #1 test set #2  description accounts  tweets year  8,377,522 2011 1,610,176 2012 428,542 2014 1,418,626 2011 145,094 2009 74,957 2014 5,794,931 2013 133,311 2009 196,027 2012  4,061,598 – 2,628,181 –  used in section  3.1  4, 5 4, 5  statistics  ￼ ￼ verified accounts that are human-operated retweeters of an Italian political candidate spammers of paid apps for mobile devicesspammers of products on sale at Amazon.|com training set of spammers used by Yang et al. in [43] spammers of scam URLs  automated accounts spamming job offersanother group of automated accounts spamming job offers simple accounts that inflate the number of followers of another account  mixed set of 50% genuine accounts + 50% social spambots #1 mixed set of 50% genuine accounts + 50% social spambots #3  3,474 991 3,457 464 1,000 100 433 1,128 3,351  1,982 928  ￼ ￼ against this novel generation of spambots. Our main contributions are:  \t•\tWe provide empirical evidence of the existence of a novel wave of Twitter spambots, which, up to now, has been just theorized [15].  \t•\tWe evaluate if, and to which extent, state-of-the-art detection techniques succeed in spotting such new spam- bots.|•\tWe critically revise an emerging stream of research, which adopt features tied to groups of accounts rather than individual accounts features.  \t•\tWe leverage results of a crowdsourcing spambot de- tection campaign for drawing new guidelines for the annotation of datasets comprising social spambots.  \t•\tFinally, we publicly release to the scientific community an annotated dataset1, consisting of genuine accounts, traditional spambots, and —for the first time— the novel social spambots. DATASETS We describe the different Twitter datasets that constitute the real-world data used in our experiments.|Table 1 re- ports the name of the datasets, their brief description, and the number of accounts and tweets they feature. The year represents the average of the creation years of the accounts that belong to the dataset. Thegenuine accountsdatasetisarandomsampleofgen- uine (human-operated) accounts. We randomly contacted Twitter users by asking a simple question in natural lan- guage.|All the replies to our questions were manually veri- fied and all the 3,474 accounts that answered were certified as humans. The accounts that did not answer to our ques- tion were discarded and are not used in this study. The social spambots #1 dataset was created after ob- serving the activities of a novel group of social bots that we discovered on Twitter during the last Mayoral election in Rome, in 2014. One of the runners-up employed a so- cial media marketing firm for his electoral campaign, which made use of almost 1,000 automated accounts on Twitter to publicize his policies.|Surprisingly, we found such automated 1 http://mib.html  accounts to be similar to genuine ones in every way. Every profile was accurately filled in with detailed – yet fake – personal information such as a (stolen) photo, (fake) short- bio, (fake) location, etc. Those accounts also represented credible sources of information since they all had thousands of followers and friends, the majority of which were gen- uine users2.|Furthermore, the accounts showed a tweeting behavior that was apparently similar to those of genuine ac- counts, with a few tweets posted every day, mainly quotes from popular people, songs, and YouTube videos. However, every time the political candidate posted a new tweet from his official account, all the automated accounts retweeted it in a time span of just a few minutes. Thus, the political candidate was able to reach many more accounts in addition to his direct followers and managed to alter Twitter engage- ment metrics during the electoral campaign. Amazingly, we also found tens of human accounts who tried to engage in conversation with some of the spambots.|The most com- mon form of such human-to-spambot interaction was rep- resented by a human reply to one of the spambot tweets quotes. We also discovered a second group of social bots, which we labeled social spambots #2, who spent several months promoting the #TALNTS hashtag. Specifically, Talnts is a mobile phone application for getting in touch with and hiring artists working in the fields of writing, digital photog- raphy, music, and more. The vast majority of tweets were harmless messages, occasionally interspersed by tweets men- tioning a specific genuine (human) account and suggesting him to buy the VIP version of the app from a Web store.|Further, we uncovered a third group of social bots, social spambots #3, which advertise products on sale on Ama- zon. The deceitful activity was carried out by spam- ming URLs pointing to the advertised products. Similarly to the retweeters of the Italian political candidate, also this family of spambots interleaved spam tweets with harmless and genuine ones.  We exploited a Twitter crawler to collect data about all the accounts we suspected to belong to the three groups of social spambots.|All the accounts collected in this process have then undergone an internal manual verification phase to certify their automated nature. Among all the distinct retweeters of the Italian political candidate, 50.05% (991 ac- counts) were certified as spambots.50% (3,457  2This was made possible also by the adoption of social engineering techniques, such as the photo of a young attractive woman as the profile picture and the occasional posting of provocative tweets.|Table 1: Statistics about the datasets used for this study.  ￼ ￼  ￼ genuine accountssocial spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 fake followers  3,474 994 3,457 467 1,000 100 3,351  6 (0.5%)  accounts  ￼ dataset total  alive  3,353 (96.4%)  deleted  115 (3.|1%)  suspended  ￼ ￼ Table 2: Statistics about alive, deleted, and suspended accounts, for different groups of genuine and malicious accounts.  accounts) of the accounts who tweeted the #TALNTS hash- tag resulted as spambots.29% (464 accounts) of the accounts that tweeted suspicious Amazon.com URLs were also certified as spambots.|The three sets of accounts represent our ground truth of novel social spambots.  Our internal manual annotation has been carried out by comparing every account to all the others, in order to high- light possible similarities and common behaviors. This is in contrast with the typical annotation process where ac- counts are labeled one-by-one and by solely exploiting the characteristics of the account under investigation.  In addition to genuine users and social spambots, we also collected several datasets of traditional spambots.|Such datasets are used throughout the paper as a strong base- line. The traditional spambots #1 dataset is the training set used in [43], kindly provided to us by the authors of that work. In [43], the dataset has been used to train a machine learning classifier for the detection of evolving Twitter spam- bots. Accounts belonging to the traditional spambots #2 dataset are rather simplistic bots that repeatedly mention other users in tweets containing scam URLs.|To lure users into clicking the malicious links, the content of their tweets invite the mentioned users to claim a monetary prize. The traditional spambots #3 and traditional spambots #4 datasets are related to 2 different groups of bots that re- peatedly tweet about open job positions and job offers.  Fake followers are another kind of malicious accounts that recently gained interest both from platform administrators and from the scientific world [12]. Given that fake followers are rather simplistic in their design and functioning, they can serve as a weak baseline against which to compare so- cial spambots.|In April, 2013, we bought 3,351 fake accounts from three different Twitter online markets, namely fastfol- lowerz. All the accounts acquired in this way have been merged in order to obtain the fake followers dataset used in this study.  By considering a diverse set of spammer accounts we have captured many of the different dimensions currently exploited by spambots and tamperers to perpetrate their illicit activ- ities. In detail, we have considered (i) fake follower frauds, (ii) retweet frauds, (iii) hashtag promotion, (iv) URL spam- ming, (v) scamming, and (vi) spam of generic messages.|RQ1 – To what extent is Twitter currently capable of de- tecting and removing social spambots?  Interesting insights can be gained by comparing the rate at which Twitter accounts are removed, for different types of malicious accounts. The intuition is that accounts that are easily identified as malicious can be rapidly removed by platform administrators. Thus, in this experiment, we let different types of accounts behave for a rather long amount of time (i. Then, we check whether Twitter man- aged to identify such accounts as malicious and to remove them from the platform.|We perform this experiment on our set of genuine accounts, on our 3 groups of social spambots, on 2 groups of traditional spambots, and on the group of fake followers.  In order to perform this experiment, we exploited Twit- ter’s responses to API calls and, particularly, the Twitter er- ror codes. Given a query to a specific account, Twitter’s API replies with information regarding the status of the queried account. Specifically, accounts that are suspected to perform malicious activities get suspended by Twitter.|API queries to a suspended account result in Twitter responding with the error code 63. API queries to accounts that have been deleted by their original owner result in Twitter respond- ing with the error code 50. Instead, for accounts that are neither suspended nor deleted, Twitter replies with the full metadata information of the account, without issuing error codes. By exploiting this response mechanism, we were able to measure the survivability of the different groups of ac- counts.|Results of this experiment are reported in Table 2 and are pictorially depicted in Figure 1.  As shown in Table 2, genuine accounts feature a very high survival rate (96. In addition, among the no longer available accounts, the vast majority have been deleted by the original owner, rather than suspended by Twitter. These results are quite intuitive, by considering that legitimate ac- counts rarely perform any kind of malicious activity.|Con- versely, the simplest kind of malicious accounts, fake fol- lowers, have mostly been detected and suspended by Twit- ter. The same also applies to one of the two groups of traditional spambots, identified as traditional spambots #2 in Table 2, which features a suspension rate as high as 99%. The most interesting results are however related to those kinds of malicious accounts that better mimic human behaviors. So far, traditional spambots #1 have largely managed to evade suspension, despite dating back to 2009.|Indeed, only 8.6% of the bots have been suspended, while 88.9% of them are still alive. This seems to suggest that Twitter’s spambot detection mechanisms are still unable to accurately identify such accounts, while recent solutions pro- posed by Academia have succeeded in this task [43].|Twit-  3.1  REAL-WORLD EXPERIMENTATION Twitter monitoring  A first assessment of the extent and the severity of Twit- ter social spambots problem can be obtained by measuring Twitter’s capacity of detecting and removing them from the platform. This section thus answers the research question:  ￼ ￼ ￼ dataset  social spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 fake followers  alive −1.1%***  accounts  deleted suspended −3.|4%***  ￼ ￼ Figure 1: Survival rates for different types of accounts.  ter’s performance in suspending malicious accounts is even worse if we consider social spambots. All the 3 groups of so- cial spambots feature very high survival rates, respectively 95. Even if the difference between the survival rate of social spambots and that of traditional spambots #1 is marginal, these results nonetheless suggest an increased difficulty for the detection of social spambots.|Table 3 also reports the results of a comparison between the ratios of alive, deleted, and suspended accounts between spambots and genuine accounts. As shown, social spam- bots feature very small differences with respect to genuine accounts (∼ ±3%). Some of these differences are not even statistically significant, according to a chi-square test. Tra- ditional spambots #1 have differences ∼ ±8% that are highly significant (p < 0.|01) for alive and suspended ac- counts. Instead, traditional spambots #2 and fake fol- lowers show massive differences: ∼ ±96% and ∼ ±72%, respectively.  Figure 1 shows results of the survivability experiment, with respect to the account age3. This can allow to un- derstand if temporal patterns exist in the way malicious accounts are created, and if Twitter’s mechanisms for sus- pending malicious accounts are related to an account’s age.|For instance, Twitter might be better in detecting and sus- pending older accounts than newer ones. However, this hy- pothesis can be ruled out by considering that 99% of tradi- tional spambots #2 accounts have been suspended despite being younger than most of the social spambots. Overall, an analysis of Figure 1 shows that account suspensions seem to depend on the type of the account, its design and behavior, rather than on its age.  Results reported in this first experiment already reveal interesting differences between social spambots, traditional spambots, and fake followers.|Notably, social spambots ap- pear to be more similar to genuine accounts than to tradi- tional spambots, with regards to Twitter suspensions.  3Account age is computed as the number of days between the ac- count’s creation date and the day we performed the experiment.  Table 3: Effect size and statistical significance of the difference between the survivability results of malicious accounts with respect to those of genuine accounts.2 Crowdsourcing: tasks and results  This section addresses the following research questions:  RQ2 – Do humans succeed in detecting social spambots in the wild?  RQ3 – Do they succeed in discriminating between tradi- tional spambots, social spambots, and genuine accounts?  Even if Twitter users were generally capable of distin- guishing between traditional spambots and genuine accounts, they might still find it difficult to spot social spambots in the wild.|If confirmed, this would provide additional evi- dence of the evolutionary step characterizing the new social spambots with respect to traditional ones.  Figure 2: Dataset for the crowdsourcing experiment.  To answer these research questions, we asked a large set of real-world users to classify the accounts in our datasets. To obtain a large and diverse set of users, we recruited con- tributors from the CrowdFlower4 crowdsourcing platform.|Figure 2 shows the distribution of the 4,428 accounts that we have employed for this crowdsourcing experiment, picked up from the datasets in Section 2. Contributors were asked to assign to each account one of the following classes: (i) spambot, (ii) genuine, and (iii) unable to classify. The lat- ter class (iii) has been inserted to deal with Twitter accounts possibly getting deleted, suspended, or protected5 while our crowdsourcing task was ongoing.  Notably, our experiment marks a difference with those typically carried out with crowdsourcing.|In fact, crowd- sourcing tasks are typically aimed at creating a ground truth (i., labeled) dataset for later use. For instance, crowdsourc- ing is often used to create large training-sets for machine learning algorithms. Here, instead, the datasets are labeled in advance.|Thus, by asking contributors to (re-)classify our datasets, we are actually evaluating their ability to spot the different types of accounts.  4 https://www.com/  5Protected accounts are those accounts whose tweets and timeline are not publicly visible.  ***p < 0.|01, **p < 0.05, *p < 0.1  ￼ ￼ ￼  ￼ 1,385 328 0  0 0  0 0 1,267 110  detection results  ￼ type  traditional spambots social spambots genuine accounts  accounts♯ 1,516  1,393 1,377  TP TN FP  FN  131 1,065 0  Accuracy  0.9201  Fleiss’ kappa (κ) 0.|410  got  4.7/5  ￼ ￼ ♯: The total number of accounts considered is 4,286 instead deleted, suspended, or protected during our campaign.  of 4,428 because 142 accounts (3.2%)  Table 4: Results of the crowdsourcing campaign on spambots detection.|￼ Enforcing results reliability. We only recruited contrib- utors who were tech-savvy and Twitter users themselves, in order to be reasonably sure about their knowledge of Twitter and its dynamics. Furthermore, we required each account to be classified by at least 3 different contributors, with the final class decided by majority voting. We also fixed to 100 the upper threshold of the number of accounts that a single contributor could classify.|In this way, we have obtained re- dundant results from a broad set of contributors. Then, in order to further guarantee the reliability of our crowdsourc- ing results, we designed a set of “test” (or “gold”) questions aimed at evaluating the quality of contributors’ answers. A test question is one for which the correct answer is already known by the system. Within the crowdsourcing platform, such questions are indistinguishable from standard ones and are randomly mixed among all the questions, so that contrib- utors cannot know whether they are answering to a test or to a standard question.|Contributors’ answers to test questions were checked against the known correct answers. Only the trusted contributors who answered correctly to more than the 70% of the test questions have been considered in our study. Our test questions consist of accounts whose nature is “easily” recognizable, and specifically: (i) a set of traditional spambots sampled from the dataset of Yang et al. [43], (ii) a subset of genuine accounts, and (iii) a set of suspended, deleted, and protected accounts.|Notably, by designing test questions with traditional spambots and genuine accounts, and by enforcing the policy of at least 70% correct answers, we can guarantee that all our trusted contributors are typi- cally able to detect traditional spambots and to distinguish them from genuine accounts. This further strengthens the results of their classification of the novel social spambots.  Table 5 shows a recap of the settings used in our crowd- sourcing campaign. The thorough description of our cam- paign, with the complete set of instructions, a list of exam- ple accounts, and the task preview, is available online6.|The campaign completed when each of the 4,428 accounts was classified by 3 different trusted contributors.  Instructions clear Test questions fair Ease of jobPay  Overall  ￼ ￼ Num. accounts to classify Min. contributors per account Max.|answers per contributor Num. accuracy threshold Reward  4,428 3 100 25 70% 0.1 US$ per 5 accounts classified  Figure 3: Crowdsourcing campaign details.  contributors the possibility to evaluate crowdsourcing cam- paigns for: (i) clarity of instructions, (ii) fairness of the test questions, (iii) ease of the task, and (iv) appropriateness of the payment.|Out of the 247 participating contributors, 60 of them (∼ 24%) evaluated our campaign, leading to a convincing aggregated score of 3.7/5, as shown in detail in Table 6. Our campaign costed us 410 US$ in total.  The most interesting results of our crowdsourcing cam- paign are undoubtedly related to the detection performance of our human contributors.|As reported in Table 4, overall, the human annotators obtained an accuracy of less than 0.24 on the social spambots, with more than 1,000 False Nega- tives (FN), meaning that contributors classified more than 1,000 accounts as genuine, when they actually belonged to the dataset of the last generation of spambots. Human de- tection performances for the two other groups of accounts, namely traditional spambots and genuine accounts, are in- stead quite satisfactory, with an accuracy of 0. These important results further highlight the existence of a striking difference between traditional and so- cial spambots.|More worryingly, they also suggest that hu- mans might not be able to detect social spambots in the wild, and to distinguish them from genuine accounts.  Given that each account under investigation has been clas- sified by at least 3 different contributors, we have also com- puted the Fleiss’ kappa (κ) inter-rater agreement metric [19]. All inter-rater agreement metrics measure the level of agree- ment of different annotators on a task. The level of agree- ment can be also interpreted as a proxy for the difficulty of a task.|In our experiment, human contributors showed a  Table 6: Contributors’ evaluation of our campaign.  ￼ ￼ (a) Top 20 countries.  (b) Answers per contributor.  ￼ ￼ Table 5: Crowdsourcing campaign settings.|Results of the crowdsourcing campaign. Overall, we collected 13,284 answers given by 247 trusted contributors from 42 different countries. Figure 3(a) shows the distribu- tion of answers per country. Figure 3(b) depicts the distri- bution of answers per contributor.|CrowdFlower also gives  6 http://wafi.it/fake/fake/crowdflower/instructions/  ￼  ￼ ￼ technique  test set #1  Twitter countermeasures Human annotators BotOrNot? [14]C. Yang et al. [43] Miller et al.|[30]  Ahmed et al. [2]♯ Cresci et al. [13]  test set #2  Twitter countermeasures Human annotators BotOrNot? [14]C. Yang et al.|[43] Miller et al. [30]  Ahmed et al. [2]♯ Cresci et al. [13]  type Precision mixed 1.|000  detection results  Recall Specificity Accuracy  0.929  F-Measure  0.923  MCC  0.867  ￼ manual supervised supervised unsupervised unsupervised unsupervised  mixed manual supervised supervised unsupervised unsupervised unsupervised  0.|000  ￼ ￼ ♯: Modified by employing fastgreedy instead of MCL for the graph clustering step.  Table 7: Comparison among the spambot detection techniques, tools, and algorithms surveyed in this study. For each test set, the highest values in each evaluation metric are shown in bold.  decent agreement for the classification of genuine accounts, with κ = 0.|Instead, they showed very little agree- ment while classifying traditional spambots, as represented by κ = 0. This interesting result shows that, overall, the human contributors were able to correctly detect tra- ditional spambots, as shown by the 0.91 accuracy, but also that contributors rarely agreed on the class. Surprisingly, we measured a slightly higher agreement for the classification of social spambots than for traditional ones, with κ = 0.|These results imply that humans generally failed in classify- ing social spambots (accuracy = 0.2355) and, furthermore, that they also were more in agreement on this mistake than they were when (correctly) classifying traditional spambots.  Annotation guidelines for spambots detection. De- spite the recent advances in machine learning-based detec- tion systems, manual verification of accounts to assess their degree of automation is still carried out by platform adminis- trators [15].|In [41], it is reported that human experts “con- sistently produced near-optimal results” on a dataset of tra- ditional spambots. However, the results of our crowdsourc- ing experiment confirmed that the traditional “account-by- account” annotation process used by human workers to eval- uate social media accounts is no longer viable when applied to the detection of the novel wave of social spambots. Given the importance of manual annotation for the creation of ground-truth datasets and for double-checking suspicious ac- counts on social networking platforms, we call for the adop- tion of new annotation methodologies that take into account the similarities and synchronized behaviors of the accounts. We have adopted a practical implementation of this method- ology to annotate our datasets of social spambots.|In par- ticular, we have compared the timelines of large groups of accounts, in order to highlight tweeting similarities among them. By comparing the behaviors of different accounts, rather than by analyzing them one by one, we were able to spot the social spambots among all the collected accounts, as thoroughly described in Section 2. Therefore, we envisage the possibility to adopt this methodology, as well as similar ones, in order to safeguard the manual annotation process from elusive social spambots. ESTABLISHED TECHNIQUES  So far, we have demonstrated that neither Twitter nor human operators are currently capable of identifying novel social spambots.|Here, we investigate whether established tools and techniques are able to succeed in this task. Thus, our research question is:  RQ4 – Are state-of-the-art scientific applications and tech- niques able to detect social spambots?  The BotOrNot? service. BotOrNot? is a publicly- available service7 to evaluate the similarity of a Twitter ac- count with the known characteristics of social spambots [14]. It has been developed by the Indiana University at Bloom- ington and it was released in May 2014.|Claimed capable of detecting social spambots [15], at the time of writing it was the only publicly-available social spambot detection system. BotOrNot? leverages a supervised machine-learning classi- fier that exploits more than 1,000 features of the Twitter account under investigation. Specifically, it employs off-the- shelf supervised learning algorithms trained with examples of both humans and bots behaviors, based on the Texas A&M dataset [25] with 15,000 examples of each class and millions of tweets. Similarly to most already established techniques, BotOrNot? performs its analyses on an account- by-account basis.|Despite being specifically designed for the detection of social spambots, authors state that the detec- tion performances of BotOrNot? against evolved spambots might be worse than those reported in [14]. Here, we aim at evaluating this point by querying the BotOrNot? service with our sets of genuine and social spambot accounts. As shown in Table 7, BotOrNot? achieves rather unsatisfactory results for the accounts of both test set #1 and test set #2 (such datasets are described in Table 1). Its detection performances are particularly bad for the accounts of test set #1 – where the spambots are from the social spambots #1 group.|The low values of F-Measure and Mathews Corre- lation Coefficient (MCC), respectively 0.174, are mainly due to the low Recall. In turn, this represents a ten- dency of labeling social spambots #1 as genuine accounts.  7 http://truthy.|edu/botornot/  ￼  ￼ Supervised spambot classification. Among the many supervised classification approaches to spambot detection proposed in recent years by Academia, we decided to ex- periment with the one presented by C. Yang et al. in [43], since it focuses on the detection of evolving Twitter spam- bots.|Thus, it is interesting to evaluate if the system recently presented in [43] is actually able to detect the sophisticated social spambots. This supervised system provides a machine learning classifier that infers whether a Twitter account is genuine or spambot by relying on account’s relationships, tweeting timing and level of automation. We have repro- duced such a classifier by implementing and computing all the features proposed in [43], and by training the classifier with its original dataset. Results in Table 7 show that the system fails to correctly classify the novel social spambots.|Similarly to the results of the BotOrNot? service, the worst results of this system in both test set #1 and test set #2 are related to the Recall metric. This means that also this classifier labeled social spambots as genuine accounts.  Unsupervised spambot detection via Twitter stream clustering. Our initial claim, supported by preliminary work [15,47], is that social spambots might be so sophisti- catedly designed to make it very difficult to distinguish them from genuine accounts, if observed one by one.|If demon- strated, this claim would imply that supervised classifica- tion approaches are intrinsically worse than unsupervised ones for the detection of social spambots. For this reason, we have also experimented with unsupervised approaches for spambot detection. The approach in [30] considers vec- tors made of 126 features extracted from both accounts and tweets as input of modified versions of the DenStream [7] and StreamKM++ [1] clustering algorithms, to cluster fea- ture vectors of a set of unlabeled accounts. We have imple- mented the system proposed in [30] to cluster the accounts of our 2 test sets.|As shown in Table 7, this achieved the worst performances among all those that we have benchmarked in this study. Low values of both Precision and Recall mean incomplete and unreliable spambot detection. Among the 126 features, 95 are based on the textual content of tweets. However, novel social spambots tweet contents similar to that of genuine accounts (e.|, retweets of genuine tweets and famous quotes). For this reason, an approach almost solely based on tweet content will not be able to achieve satisfactory results.  Unsupervised spambot detection via graph cluster- ing. The approach in [2] exploits statistical features related to URLs, hashtags, mentions and retweets.|Feature vectors generated in this way are then compared with one another via an Euclidean distance measure. Distances between ac- counts are organized in an adjacency matrix, which is later used to construct an undirected weighted graph of the ac- counts. Then, graph clustering and community detection algorithms are applied in order to identify groups of similar accounts. Graph clustering is done by employing the Markov cluster algorithm (MCL) [38].|We fully implemented this so- lution and we experimented with our datasets. However, the approach failed to identify 2 distinct clusters, since accounts of both our test-sets were assigned to a single cluster. We also performed a grid search simulation in order to test the best parameter configuration for MCL8, but to no avail. To achieve effective detection results, instead of the MCL, we  8MCL admits 2 fundamental parameters: inflation and expansion.|established work  Yardi, Romero et al. [45] 2009  Benevenuto et al. [3, 4, 17] 2009-12 K. Lee, Caverlee 2010-11 et al.|[24,25]Stringhini et al. [33–35] 2010-13  Viswanath, Mislove, 2011 Gummadi et al. [39]Stein et al. [32] 2011 Thomas et al.|[36] 2011 Gao et al. [16] 2012 Cao et al. [8] 2012 Xie et al. [42] 2012 C.|Yang et al. [43] 2013 Wang et al. [41] 2013 S. Lee et al.|[26,27] 2013-14 Z. Yang et al. [44] 2014 Liu et al. [29] 2014 Paradise et al.|[31] 2014 Cresci et al. [11,12] 2014-15 Ferrara et al. [6, 14] 2014-16  emerging trends  ￼ This table does not aim to be complete, but rather to testify the emergence of a new trend of research.  Table 8: Recent work in spambot detection.|adopted the fastgreedy community detection algorithm [10]. As reported in Table 7, our modified implementation proved effective in detecting social spambots, with an MCC = 0.886 for test set #1 and MCC = 0.847 for test set #2.|EMERGING TRENDS  As shown in Table 7, the established works benchmarked in Section 4 largely failed to detect the new wave of so- cial spambots. In turn, these results call for novel analytic tools able to keep pace with the latest evolutionary step of spambots. Thus, in this section we revise the most recent literature on spambots detection with the aim of answering the research question:  RQ5 – Is it possible to find a new dimension over which to fight and overcome the novel social spambots?  Traditional spambot detection systems typically rely on the application of well-known machine learning algorithms on the accounts under investigation. However, since 2013, a number of research teams independently started to for- malize new approaches for detecting the coordinated and synchronized behavior that characterizes groups of auto- mated malicious accounts [5].|Table 8 groups such tech- niques as emerging trends. Despite being based on different key concepts, these studies investigate groups of accounts as a whole, marking a significant difference with the previous literature. Table 9 reports on the new concepts introduced by these emerging works. Focusing on groups has the ad- vantage that, no matter how sophisticated a single spambot can be, a large enough group of spambots will still leave traces of automation, since they do have a common goal (e.|, increasing someone’s reputation score). By perform- ing analyses at the group level, this emerging trend might be able to significantly raise the bar for social spambots to evade detection. To support the claim, we experiment with work in [13,40].  Tamper Detection in Crowd Computations.|The con- tribution by Viswanath et al. in [40] checks whether a given group of accounts (e., retweeters of another account, re- viewers of a venue on Yelp) contains a subset of malicious  Beutel, Faloutsos,et al. [5,18,21,22]Cao et al.|[9] 2014 Yu et al. [46] 2015  Viswanath, Mislove, 2015 Gummadi et al. [40]Cresci et al. [13] 2016  2013-16  ￼ ￼  ￼ work  key concept  detection of lockstep behaviors  anomalies in synchronicity and normality  detection of loosely synchro- nized actions  detection of latent group anomalies in graphs  distance between distributions of reputation scores  similarity between digital DNA sequences  computed in [13] by measuring the Longest Common Sub- string (LCS)—that is, the longest DNA substring shared by all the accounts of the group.|Accounts that share a suspi- ciously long DNA substring are then labeled as spambots. Notably, although working at group level, [13] is capable of spotting single spambot accounts. For this reason, we have been able to compare this technique with the ones previ- ously benchmarked in Table 7. Applying the technique to our datasets, the similarity curve of genuine accounts is sig- nificantly different from that of social spambots, as shown in Figure 4(c).|More specifically, as measured by the LCS metric, social spambots #1 and #3 feature a level of sim- ilarity much higher than that of genuine accounts. Results reported in Table 7 demonstrate that the digital DNA-based technique [13] achieves excellent detection performances.  The compelling features of the emerging techniques listed in this section represent a fertile ground for fighting the novel social spambots. We can observe a paradigm-shift for research and development of spambot detection systems, which may exploit such new concepts to achieve better re- silience and robustness, to withstand the next evolution of social media spambots.|CONCLUDING REMARKS  Our long-lasting experiment on malicious accounts sur- vival rate in Twitter demonstrated that spambot detection is still an open issue. Moreover, the already difficult prob- lem to detect spambots in social media is bound to worsen, since the emergence of a new wave of so-called social spam- bots. By accurately mimicking the characteristics of gen- uine users, these spambots are intrinsically harder to detect than those studied by Academia in the past years. In our experiments, neither humans nor state-of-the-art spambot detection applications managed to accurately detect the ac- counts belonging to this new wave of spambots.|Indeed, our experiments highlighted that the majority of existing au- tomated systems, as well as crowdsourcing, erroneously la- bel social spambots as genuine (human-operated) accounts. We demonstrated the need for novel analytic tools capable of turning the tide in the arms race against such sophis- ticated spambots. One promising research direction stems from the analysis of collective behaviors. We highlighted a few emerging approaches that analyze groups as a whole, rather than individuals.|The promising outcome of these novel approaches clearly indicates that this is a favorable research avenue\n",
            "['ABSTRACT  legality, sponsor public characters and, ultimately, lead to a bias within the public opinion [15,23]. Moreover, the plague of such spammers and bots leads to an ingenious and lucra- tive “underground economy”, where account vendors, their customers, and oblivious victims play a piece staging since the very introduction of social networks [33,34,37].  One of the most fascinating peculiarities of spambots is that they “evolve” over time, adopting sophisticated tech- niques to evade early-established detection approaches, such as those based on textual content of shared messages [24], posting patterns [35] and social relationships [17]. As evolv- ing spammers became clever in escaping detection, for in- stance by changing discussion topics and posting activities, researchers kept the pace and proposed complex models, such as those based on the interaction graphs of the accounts under investigation [20,43].', 'Noticeably, spambots evolution still goes on. Recent in- vestigations anecdotally highlight how new waves of social spambots are rising [15,47]. In this paper, we target these new waves, finding evidence of the difficulties for OSN users to distinguish between genuine and malicious accounts. We also highlight the difficulties for OSN administrators to take appropriate countermeasures against the takeover of evolv- ing spambots.', 'Remarkably, a large number of tools and techniques have been proposed by Academia to detect OSN spambots [15,23]. Until recently, such tools have proved to be valid allies for spambots timely detection. Unfortunately, the characteristics of the new wave of social spambots are such that standard classification approaches, where a single account is evaluated according to a set of established features tested over known datasets, are no longer successful. In this work, we demonstrate this claim by investigating the per- formances of several state-of-the-art tools techniques when struggling against the latest wave of social spambots.', 'The unsatisfactory results of the surveyed techniques call for new approaches capable of turning the tide of this long-lasting fight.  Interestingly, we assist to a paradigm-shift in modeling and analyzing online accounts. Independently from each other, new research efforts were born, which leverage charac- teristics of groups of accounts – rather than those of a single account – as a red flag for anomalous behaviors. We provide a review of these prominent research directions, highlight- ing the new dimensions to sound out for successfully fighting  The paradigm-shift of social spambots  Evidence, theories, and tools for the arms race  Stefano Cresci † Roberto Di Pietro †‡§ Marinella Petrocchi † s.', 'it  §  Department of Mathematics, University of Padua, Padua, Italy ¶ DTU Compute, Technical University of Denmark, Denmark  †  Institute for Informatics and Telematics, IIT-CNR, Pisa, Italy ‡ Nokia Bell Labs, Paris, France  Recent studies in social media spam and automation provide anecdotal argumentation of the rise of a new generation of spambots, so-called social spambots. Here, for the first time, we extensively study this novel phenomenon on Twitter and we provide quantitative evidence that a paradigm-shift ex- ists in spambot design. First, we measure current Twitter’s capabilities of detecting the new social spambots. Later, we assess the human performance in discriminating between genuine accounts, social spambots, and traditional spam- bots.', 'Then, we benchmark several state-of-the-art tech- niques proposed by the academic literature. Results show that neither Twitter, nor humans, nor cutting-edge applica- tions are currently capable of accurately detecting the new social spambots. Our results call for new approaches capa- ble of turning the tide in the fight against this raising phe- nomenon. We conclude by reviewing the latest literature on spambots detection and we highlight an emerging common research trend based on the analysis of collective behaviors.', 'Insights derived from both our extensive experimental cam- paign and survey shed light on the most promising directions of research and lay the foundations for the arms race against the novel social spambots. Finally, to foster research on this novel phenomenon, we make publicly available to the scien- tific community all the datasets used in this study. INTRODUCTION  The widespread availability and ease of use of Online So- cial Networks (OSN) have made them the ideal setting for the proliferation of fictitious and malicious accounts [28]. In- deed, recent work uncovered the existence of large numbers of OSN accounts that are purposely created to distribute unsolicited spam, advertise events and products of doubtful  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page.', 'Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.', 'WWW Web Science 2017, Perth, Australia  ⃝c 2017ACM.475/123 4  Angelo Spognardi †¶ Maurizio Tesconi †  angsp@dtu.SI] 11 Jan 2017  ￼ dataset  genuine accountssocial spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 traditional spambots #3 traditional spambots #4 fake followers  test set #1 test set #2  description accounts  tweets year  8,377,522 2011 1,610,176 2012 428,542 2014 1,418,626 2011 145,094 2009 74,957 2014 5,794,931 2013 133,311 2009 196,027 2012  4,061,598 – 2,628,181 –  used in section  3.1  4, 5 4, 5  statistics  ￼ ￼ verified accounts that are human-operated retweeters of an Italian political candidate spammers of paid apps for mobile devicesspammers of products on sale at Amazon.', 'com training set of spammers used by Yang et al. in [43] spammers of scam URLs  automated accounts spamming job offersanother group of automated accounts spamming job offers simple accounts that inflate the number of followers of another account  mixed set of 50% genuine accounts + 50% social spambots #1 mixed set of 50% genuine accounts + 50% social spambots #3  3,474 991 3,457 464 1,000 100 433 1,128 3,351  1,982 928  ￼ ￼ against this novel generation of spambots. Our main contributions are:  \\t•\\tWe provide empirical evidence of the existence of a novel wave of Twitter spambots, which, up to now, has been just theorized [15].  \\t•\\tWe evaluate if, and to which extent, state-of-the-art detection techniques succeed in spotting such new spam- bots.', '•\\tWe critically revise an emerging stream of research, which adopt features tied to groups of accounts rather than individual accounts features.  \\t•\\tWe leverage results of a crowdsourcing spambot de- tection campaign for drawing new guidelines for the annotation of datasets comprising social spambots.  \\t•\\tFinally, we publicly release to the scientific community an annotated dataset1, consisting of genuine accounts, traditional spambots, and —for the first time— the novel social spambots. DATASETS We describe the different Twitter datasets that constitute the real-world data used in our experiments.', 'Table 1 re- ports the name of the datasets, their brief description, and the number of accounts and tweets they feature. The year represents the average of the creation years of the accounts that belong to the dataset. Thegenuine accountsdatasetisarandomsampleofgen- uine (human-operated) accounts. We randomly contacted Twitter users by asking a simple question in natural lan- guage.', 'All the replies to our questions were manually veri- fied and all the 3,474 accounts that answered were certified as humans. The accounts that did not answer to our ques- tion were discarded and are not used in this study. The social spambots #1 dataset was created after ob- serving the activities of a novel group of social bots that we discovered on Twitter during the last Mayoral election in Rome, in 2014. One of the runners-up employed a so- cial media marketing firm for his electoral campaign, which made use of almost 1,000 automated accounts on Twitter to publicize his policies.', 'Surprisingly, we found such automated 1 http://mib.html  accounts to be similar to genuine ones in every way. Every profile was accurately filled in with detailed – yet fake – personal information such as a (stolen) photo, (fake) short- bio, (fake) location, etc. Those accounts also represented credible sources of information since they all had thousands of followers and friends, the majority of which were gen- uine users2.', 'Furthermore, the accounts showed a tweeting behavior that was apparently similar to those of genuine ac- counts, with a few tweets posted every day, mainly quotes from popular people, songs, and YouTube videos. However, every time the political candidate posted a new tweet from his official account, all the automated accounts retweeted it in a time span of just a few minutes. Thus, the political candidate was able to reach many more accounts in addition to his direct followers and managed to alter Twitter engage- ment metrics during the electoral campaign. Amazingly, we also found tens of human accounts who tried to engage in conversation with some of the spambots.', 'The most com- mon form of such human-to-spambot interaction was rep- resented by a human reply to one of the spambot tweets quotes. We also discovered a second group of social bots, which we labeled social spambots #2, who spent several months promoting the #TALNTS hashtag. Specifically, Talnts is a mobile phone application for getting in touch with and hiring artists working in the fields of writing, digital photog- raphy, music, and more. The vast majority of tweets were harmless messages, occasionally interspersed by tweets men- tioning a specific genuine (human) account and suggesting him to buy the VIP version of the app from a Web store.', 'Further, we uncovered a third group of social bots, social spambots #3, which advertise products on sale on Ama- zon. The deceitful activity was carried out by spam- ming URLs pointing to the advertised products. Similarly to the retweeters of the Italian political candidate, also this family of spambots interleaved spam tweets with harmless and genuine ones.  We exploited a Twitter crawler to collect data about all the accounts we suspected to belong to the three groups of social spambots.', 'All the accounts collected in this process have then undergone an internal manual verification phase to certify their automated nature. Among all the distinct retweeters of the Italian political candidate, 50.05% (991 ac- counts) were certified as spambots.50% (3,457  2This was made possible also by the adoption of social engineering techniques, such as the photo of a young attractive woman as the profile picture and the occasional posting of provocative tweets.', 'Table 1: Statistics about the datasets used for this study.  ￼ ￼  ￼ genuine accountssocial spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 fake followers  3,474 994 3,457 467 1,000 100 3,351  6 (0.5%)  accounts  ￼ dataset total  alive  3,353 (96.4%)  deleted  115 (3.', '1%)  suspended  ￼ ￼ Table 2: Statistics about alive, deleted, and suspended accounts, for different groups of genuine and malicious accounts.  accounts) of the accounts who tweeted the #TALNTS hash- tag resulted as spambots.29% (464 accounts) of the accounts that tweeted suspicious Amazon.com URLs were also certified as spambots.', 'The three sets of accounts represent our ground truth of novel social spambots.  Our internal manual annotation has been carried out by comparing every account to all the others, in order to high- light possible similarities and common behaviors. This is in contrast with the typical annotation process where ac- counts are labeled one-by-one and by solely exploiting the characteristics of the account under investigation.  In addition to genuine users and social spambots, we also collected several datasets of traditional spambots.', 'Such datasets are used throughout the paper as a strong base- line. The traditional spambots #1 dataset is the training set used in [43], kindly provided to us by the authors of that work. In [43], the dataset has been used to train a machine learning classifier for the detection of evolving Twitter spam- bots. Accounts belonging to the traditional spambots #2 dataset are rather simplistic bots that repeatedly mention other users in tweets containing scam URLs.', 'To lure users into clicking the malicious links, the content of their tweets invite the mentioned users to claim a monetary prize. The traditional spambots #3 and traditional spambots #4 datasets are related to 2 different groups of bots that re- peatedly tweet about open job positions and job offers.  Fake followers are another kind of malicious accounts that recently gained interest both from platform administrators and from the scientific world [12]. Given that fake followers are rather simplistic in their design and functioning, they can serve as a weak baseline against which to compare so- cial spambots.', 'In April, 2013, we bought 3,351 fake accounts from three different Twitter online markets, namely fastfol- lowerz. All the accounts acquired in this way have been merged in order to obtain the fake followers dataset used in this study.  By considering a diverse set of spammer accounts we have captured many of the different dimensions currently exploited by spambots and tamperers to perpetrate their illicit activ- ities. In detail, we have considered (i) fake follower frauds, (ii) retweet frauds, (iii) hashtag promotion, (iv) URL spam- ming, (v) scamming, and (vi) spam of generic messages.', 'RQ1 – To what extent is Twitter currently capable of de- tecting and removing social spambots?  Interesting insights can be gained by comparing the rate at which Twitter accounts are removed, for different types of malicious accounts. The intuition is that accounts that are easily identified as malicious can be rapidly removed by platform administrators. Thus, in this experiment, we let different types of accounts behave for a rather long amount of time (i. Then, we check whether Twitter man- aged to identify such accounts as malicious and to remove them from the platform.', 'We perform this experiment on our set of genuine accounts, on our 3 groups of social spambots, on 2 groups of traditional spambots, and on the group of fake followers.  In order to perform this experiment, we exploited Twit- ter’s responses to API calls and, particularly, the Twitter er- ror codes. Given a query to a specific account, Twitter’s API replies with information regarding the status of the queried account. Specifically, accounts that are suspected to perform malicious activities get suspended by Twitter.', 'API queries to a suspended account result in Twitter responding with the error code 63. API queries to accounts that have been deleted by their original owner result in Twitter respond- ing with the error code 50. Instead, for accounts that are neither suspended nor deleted, Twitter replies with the full metadata information of the account, without issuing error codes. By exploiting this response mechanism, we were able to measure the survivability of the different groups of ac- counts.', 'Results of this experiment are reported in Table 2 and are pictorially depicted in Figure 1.  As shown in Table 2, genuine accounts feature a very high survival rate (96. In addition, among the no longer available accounts, the vast majority have been deleted by the original owner, rather than suspended by Twitter. These results are quite intuitive, by considering that legitimate ac- counts rarely perform any kind of malicious activity.', 'Con- versely, the simplest kind of malicious accounts, fake fol- lowers, have mostly been detected and suspended by Twit- ter. The same also applies to one of the two groups of traditional spambots, identified as traditional spambots #2 in Table 2, which features a suspension rate as high as 99%. The most interesting results are however related to those kinds of malicious accounts that better mimic human behaviors. So far, traditional spambots #1 have largely managed to evade suspension, despite dating back to 2009.', 'Indeed, only 8.6% of the bots have been suspended, while 88.9% of them are still alive. This seems to suggest that Twitter’s spambot detection mechanisms are still unable to accurately identify such accounts, while recent solutions pro- posed by Academia have succeeded in this task [43].', 'Twit-  3.1  REAL-WORLD EXPERIMENTATION Twitter monitoring  A first assessment of the extent and the severity of Twit- ter social spambots problem can be obtained by measuring Twitter’s capacity of detecting and removing them from the platform. This section thus answers the research question:  ￼ ￼ ￼ dataset  social spambots #1 social spambots #2 social spambots #3 traditional spambots #1 traditional spambots #2 fake followers  alive −1.1%***  accounts  deleted suspended −3.', '4%***  ￼ ￼ Figure 1: Survival rates for different types of accounts.  ter’s performance in suspending malicious accounts is even worse if we consider social spambots. All the 3 groups of so- cial spambots feature very high survival rates, respectively 95. Even if the difference between the survival rate of social spambots and that of traditional spambots #1 is marginal, these results nonetheless suggest an increased difficulty for the detection of social spambots.', 'Table 3 also reports the results of a comparison between the ratios of alive, deleted, and suspended accounts between spambots and genuine accounts. As shown, social spam- bots feature very small differences with respect to genuine accounts (∼ ±3%). Some of these differences are not even statistically significant, according to a chi-square test. Tra- ditional spambots #1 have differences ∼ ±8% that are highly significant (p < 0.', '01) for alive and suspended ac- counts. Instead, traditional spambots #2 and fake fol- lowers show massive differences: ∼ ±96% and ∼ ±72%, respectively.  Figure 1 shows results of the survivability experiment, with respect to the account age3. This can allow to un- derstand if temporal patterns exist in the way malicious accounts are created, and if Twitter’s mechanisms for sus- pending malicious accounts are related to an account’s age.', 'For instance, Twitter might be better in detecting and sus- pending older accounts than newer ones. However, this hy- pothesis can be ruled out by considering that 99% of tradi- tional spambots #2 accounts have been suspended despite being younger than most of the social spambots. Overall, an analysis of Figure 1 shows that account suspensions seem to depend on the type of the account, its design and behavior, rather than on its age.  Results reported in this first experiment already reveal interesting differences between social spambots, traditional spambots, and fake followers.', 'Notably, social spambots ap- pear to be more similar to genuine accounts than to tradi- tional spambots, with regards to Twitter suspensions.  3Account age is computed as the number of days between the ac- count’s creation date and the day we performed the experiment.  Table 3: Effect size and statistical significance of the difference between the survivability results of malicious accounts with respect to those of genuine accounts.2 Crowdsourcing: tasks and results  This section addresses the following research questions:  RQ2 – Do humans succeed in detecting social spambots in the wild?  RQ3 – Do they succeed in discriminating between tradi- tional spambots, social spambots, and genuine accounts?  Even if Twitter users were generally capable of distin- guishing between traditional spambots and genuine accounts, they might still find it difficult to spot social spambots in the wild.', 'If confirmed, this would provide additional evi- dence of the evolutionary step characterizing the new social spambots with respect to traditional ones.  Figure 2: Dataset for the crowdsourcing experiment.  To answer these research questions, we asked a large set of real-world users to classify the accounts in our datasets. To obtain a large and diverse set of users, we recruited con- tributors from the CrowdFlower4 crowdsourcing platform.', 'Figure 2 shows the distribution of the 4,428 accounts that we have employed for this crowdsourcing experiment, picked up from the datasets in Section 2. Contributors were asked to assign to each account one of the following classes: (i) spambot, (ii) genuine, and (iii) unable to classify. The lat- ter class (iii) has been inserted to deal with Twitter accounts possibly getting deleted, suspended, or protected5 while our crowdsourcing task was ongoing.  Notably, our experiment marks a difference with those typically carried out with crowdsourcing.', 'In fact, crowd- sourcing tasks are typically aimed at creating a ground truth (i., labeled) dataset for later use. For instance, crowdsourc- ing is often used to create large training-sets for machine learning algorithms. Here, instead, the datasets are labeled in advance.', 'Thus, by asking contributors to (re-)classify our datasets, we are actually evaluating their ability to spot the different types of accounts.  4 https://www.com/  5Protected accounts are those accounts whose tweets and timeline are not publicly visible.  ***p < 0.', '01, **p < 0.05, *p < 0.1  ￼ ￼ ￼  ￼ 1,385 328 0  0 0  0 0 1,267 110  detection results  ￼ type  traditional spambots social spambots genuine accounts  accounts♯ 1,516  1,393 1,377  TP TN FP  FN  131 1,065 0  Accuracy  0.9201  Fleiss’ kappa (κ) 0.', '410  got  4.7/5  ￼ ￼ ♯: The total number of accounts considered is 4,286 instead deleted, suspended, or protected during our campaign.  of 4,428 because 142 accounts (3.2%)  Table 4: Results of the crowdsourcing campaign on spambots detection.', '￼ Enforcing results reliability. We only recruited contrib- utors who were tech-savvy and Twitter users themselves, in order to be reasonably sure about their knowledge of Twitter and its dynamics. Furthermore, we required each account to be classified by at least 3 different contributors, with the final class decided by majority voting. We also fixed to 100 the upper threshold of the number of accounts that a single contributor could classify.', 'In this way, we have obtained re- dundant results from a broad set of contributors. Then, in order to further guarantee the reliability of our crowdsourc- ing results, we designed a set of “test” (or “gold”) questions aimed at evaluating the quality of contributors’ answers. A test question is one for which the correct answer is already known by the system. Within the crowdsourcing platform, such questions are indistinguishable from standard ones and are randomly mixed among all the questions, so that contrib- utors cannot know whether they are answering to a test or to a standard question.', 'Contributors’ answers to test questions were checked against the known correct answers. Only the trusted contributors who answered correctly to more than the 70% of the test questions have been considered in our study. Our test questions consist of accounts whose nature is “easily” recognizable, and specifically: (i) a set of traditional spambots sampled from the dataset of Yang et al. [43], (ii) a subset of genuine accounts, and (iii) a set of suspended, deleted, and protected accounts.', 'Notably, by designing test questions with traditional spambots and genuine accounts, and by enforcing the policy of at least 70% correct answers, we can guarantee that all our trusted contributors are typi- cally able to detect traditional spambots and to distinguish them from genuine accounts. This further strengthens the results of their classification of the novel social spambots.  Table 5 shows a recap of the settings used in our crowd- sourcing campaign. The thorough description of our cam- paign, with the complete set of instructions, a list of exam- ple accounts, and the task preview, is available online6.', 'The campaign completed when each of the 4,428 accounts was classified by 3 different trusted contributors.  Instructions clear Test questions fair Ease of jobPay  Overall  ￼ ￼ Num. accounts to classify Min. contributors per account Max.', 'answers per contributor Num. accuracy threshold Reward  4,428 3 100 25 70% 0.1 US$ per 5 accounts classified  Figure 3: Crowdsourcing campaign details.  contributors the possibility to evaluate crowdsourcing cam- paigns for: (i) clarity of instructions, (ii) fairness of the test questions, (iii) ease of the task, and (iv) appropriateness of the payment.', 'Out of the 247 participating contributors, 60 of them (∼ 24%) evaluated our campaign, leading to a convincing aggregated score of 3.7/5, as shown in detail in Table 6. Our campaign costed us 410 US$ in total.  The most interesting results of our crowdsourcing cam- paign are undoubtedly related to the detection performance of our human contributors.', 'As reported in Table 4, overall, the human annotators obtained an accuracy of less than 0.24 on the social spambots, with more than 1,000 False Nega- tives (FN), meaning that contributors classified more than 1,000 accounts as genuine, when they actually belonged to the dataset of the last generation of spambots. Human de- tection performances for the two other groups of accounts, namely traditional spambots and genuine accounts, are in- stead quite satisfactory, with an accuracy of 0. These important results further highlight the existence of a striking difference between traditional and so- cial spambots.', 'More worryingly, they also suggest that hu- mans might not be able to detect social spambots in the wild, and to distinguish them from genuine accounts.  Given that each account under investigation has been clas- sified by at least 3 different contributors, we have also com- puted the Fleiss’ kappa (κ) inter-rater agreement metric [19]. All inter-rater agreement metrics measure the level of agree- ment of different annotators on a task. The level of agree- ment can be also interpreted as a proxy for the difficulty of a task.', 'In our experiment, human contributors showed a  Table 6: Contributors’ evaluation of our campaign.  ￼ ￼ (a) Top 20 countries.  (b) Answers per contributor.  ￼ ￼ Table 5: Crowdsourcing campaign settings.', 'Results of the crowdsourcing campaign. Overall, we collected 13,284 answers given by 247 trusted contributors from 42 different countries. Figure 3(a) shows the distribu- tion of answers per country. Figure 3(b) depicts the distri- bution of answers per contributor.', 'CrowdFlower also gives  6 http://wafi.it/fake/fake/crowdflower/instructions/  ￼  ￼ ￼ technique  test set #1  Twitter countermeasures Human annotators BotOrNot? [14]C. Yang et al. [43] Miller et al.', '[30]  Ahmed et al. [2]♯ Cresci et al. [13]  test set #2  Twitter countermeasures Human annotators BotOrNot? [14]C. Yang et al.', '[43] Miller et al. [30]  Ahmed et al. [2]♯ Cresci et al. [13]  type Precision mixed 1.', '000  detection results  Recall Specificity Accuracy  0.929  F-Measure  0.923  MCC  0.867  ￼ manual supervised supervised unsupervised unsupervised unsupervised  mixed manual supervised supervised unsupervised unsupervised unsupervised  0.', '000  ￼ ￼ ♯: Modified by employing fastgreedy instead of MCL for the graph clustering step.  Table 7: Comparison among the spambot detection techniques, tools, and algorithms surveyed in this study. For each test set, the highest values in each evaluation metric are shown in bold.  decent agreement for the classification of genuine accounts, with κ = 0.', 'Instead, they showed very little agree- ment while classifying traditional spambots, as represented by κ = 0. This interesting result shows that, overall, the human contributors were able to correctly detect tra- ditional spambots, as shown by the 0.91 accuracy, but also that contributors rarely agreed on the class. Surprisingly, we measured a slightly higher agreement for the classification of social spambots than for traditional ones, with κ = 0.', 'These results imply that humans generally failed in classify- ing social spambots (accuracy = 0.2355) and, furthermore, that they also were more in agreement on this mistake than they were when (correctly) classifying traditional spambots.  Annotation guidelines for spambots detection. De- spite the recent advances in machine learning-based detec- tion systems, manual verification of accounts to assess their degree of automation is still carried out by platform adminis- trators [15].', 'In [41], it is reported that human experts “con- sistently produced near-optimal results” on a dataset of tra- ditional spambots. However, the results of our crowdsourc- ing experiment confirmed that the traditional “account-by- account” annotation process used by human workers to eval- uate social media accounts is no longer viable when applied to the detection of the novel wave of social spambots. Given the importance of manual annotation for the creation of ground-truth datasets and for double-checking suspicious ac- counts on social networking platforms, we call for the adop- tion of new annotation methodologies that take into account the similarities and synchronized behaviors of the accounts. We have adopted a practical implementation of this method- ology to annotate our datasets of social spambots.', 'In par- ticular, we have compared the timelines of large groups of accounts, in order to highlight tweeting similarities among them. By comparing the behaviors of different accounts, rather than by analyzing them one by one, we were able to spot the social spambots among all the collected accounts, as thoroughly described in Section 2. Therefore, we envisage the possibility to adopt this methodology, as well as similar ones, in order to safeguard the manual annotation process from elusive social spambots. ESTABLISHED TECHNIQUES  So far, we have demonstrated that neither Twitter nor human operators are currently capable of identifying novel social spambots.', 'Here, we investigate whether established tools and techniques are able to succeed in this task. Thus, our research question is:  RQ4 – Are state-of-the-art scientific applications and tech- niques able to detect social spambots?  The BotOrNot? service. BotOrNot? is a publicly- available service7 to evaluate the similarity of a Twitter ac- count with the known characteristics of social spambots [14]. It has been developed by the Indiana University at Bloom- ington and it was released in May 2014.', 'Claimed capable of detecting social spambots [15], at the time of writing it was the only publicly-available social spambot detection system. BotOrNot? leverages a supervised machine-learning classi- fier that exploits more than 1,000 features of the Twitter account under investigation. Specifically, it employs off-the- shelf supervised learning algorithms trained with examples of both humans and bots behaviors, based on the Texas A&M dataset [25] with 15,000 examples of each class and millions of tweets. Similarly to most already established techniques, BotOrNot? performs its analyses on an account- by-account basis.', 'Despite being specifically designed for the detection of social spambots, authors state that the detec- tion performances of BotOrNot? against evolved spambots might be worse than those reported in [14]. Here, we aim at evaluating this point by querying the BotOrNot? service with our sets of genuine and social spambot accounts. As shown in Table 7, BotOrNot? achieves rather unsatisfactory results for the accounts of both test set #1 and test set #2 (such datasets are described in Table 1). Its detection performances are particularly bad for the accounts of test set #1 – where the spambots are from the social spambots #1 group.', 'The low values of F-Measure and Mathews Corre- lation Coefficient (MCC), respectively 0.174, are mainly due to the low Recall. In turn, this represents a ten- dency of labeling social spambots #1 as genuine accounts.  7 http://truthy.', 'edu/botornot/  ￼  ￼ Supervised spambot classification. Among the many supervised classification approaches to spambot detection proposed in recent years by Academia, we decided to ex- periment with the one presented by C. Yang et al. in [43], since it focuses on the detection of evolving Twitter spam- bots.', 'Thus, it is interesting to evaluate if the system recently presented in [43] is actually able to detect the sophisticated social spambots. This supervised system provides a machine learning classifier that infers whether a Twitter account is genuine or spambot by relying on account’s relationships, tweeting timing and level of automation. We have repro- duced such a classifier by implementing and computing all the features proposed in [43], and by training the classifier with its original dataset. Results in Table 7 show that the system fails to correctly classify the novel social spambots.', 'Similarly to the results of the BotOrNot? service, the worst results of this system in both test set #1 and test set #2 are related to the Recall metric. This means that also this classifier labeled social spambots as genuine accounts.  Unsupervised spambot detection via Twitter stream clustering. Our initial claim, supported by preliminary work [15,47], is that social spambots might be so sophisti- catedly designed to make it very difficult to distinguish them from genuine accounts, if observed one by one.', 'If demon- strated, this claim would imply that supervised classifica- tion approaches are intrinsically worse than unsupervised ones for the detection of social spambots. For this reason, we have also experimented with unsupervised approaches for spambot detection. The approach in [30] considers vec- tors made of 126 features extracted from both accounts and tweets as input of modified versions of the DenStream [7] and StreamKM++ [1] clustering algorithms, to cluster fea- ture vectors of a set of unlabeled accounts. We have imple- mented the system proposed in [30] to cluster the accounts of our 2 test sets.', 'As shown in Table 7, this achieved the worst performances among all those that we have benchmarked in this study. Low values of both Precision and Recall mean incomplete and unreliable spambot detection. Among the 126 features, 95 are based on the textual content of tweets. However, novel social spambots tweet contents similar to that of genuine accounts (e.', ', retweets of genuine tweets and famous quotes). For this reason, an approach almost solely based on tweet content will not be able to achieve satisfactory results.  Unsupervised spambot detection via graph cluster- ing. The approach in [2] exploits statistical features related to URLs, hashtags, mentions and retweets.', 'Feature vectors generated in this way are then compared with one another via an Euclidean distance measure. Distances between ac- counts are organized in an adjacency matrix, which is later used to construct an undirected weighted graph of the ac- counts. Then, graph clustering and community detection algorithms are applied in order to identify groups of similar accounts. Graph clustering is done by employing the Markov cluster algorithm (MCL) [38].', 'We fully implemented this so- lution and we experimented with our datasets. However, the approach failed to identify 2 distinct clusters, since accounts of both our test-sets were assigned to a single cluster. We also performed a grid search simulation in order to test the best parameter configuration for MCL8, but to no avail. To achieve effective detection results, instead of the MCL, we  8MCL admits 2 fundamental parameters: inflation and expansion.', 'established work  Yardi, Romero et al. [45] 2009  Benevenuto et al. [3, 4, 17] 2009-12 K. Lee, Caverlee 2010-11 et al.', '[24,25]Stringhini et al. [33–35] 2010-13  Viswanath, Mislove, 2011 Gummadi et al. [39]Stein et al. [32] 2011 Thomas et al.', '[36] 2011 Gao et al. [16] 2012 Cao et al. [8] 2012 Xie et al. [42] 2012 C.', 'Yang et al. [43] 2013 Wang et al. [41] 2013 S. Lee et al.', '[26,27] 2013-14 Z. Yang et al. [44] 2014 Liu et al. [29] 2014 Paradise et al.', '[31] 2014 Cresci et al. [11,12] 2014-15 Ferrara et al. [6, 14] 2014-16  emerging trends  ￼ This table does not aim to be complete, but rather to testify the emergence of a new trend of research.  Table 8: Recent work in spambot detection.', 'adopted the fastgreedy community detection algorithm [10]. As reported in Table 7, our modified implementation proved effective in detecting social spambots, with an MCC = 0.886 for test set #1 and MCC = 0.847 for test set #2.', 'EMERGING TRENDS  As shown in Table 7, the established works benchmarked in Section 4 largely failed to detect the new wave of so- cial spambots. In turn, these results call for novel analytic tools able to keep pace with the latest evolutionary step of spambots. Thus, in this section we revise the most recent literature on spambots detection with the aim of answering the research question:  RQ5 – Is it possible to find a new dimension over which to fight and overcome the novel social spambots?  Traditional spambot detection systems typically rely on the application of well-known machine learning algorithms on the accounts under investigation. However, since 2013, a number of research teams independently started to for- malize new approaches for detecting the coordinated and synchronized behavior that characterizes groups of auto- mated malicious accounts [5].', 'Table 8 groups such tech- niques as emerging trends. Despite being based on different key concepts, these studies investigate groups of accounts as a whole, marking a significant difference with the previous literature. Table 9 reports on the new concepts introduced by these emerging works. Focusing on groups has the ad- vantage that, no matter how sophisticated a single spambot can be, a large enough group of spambots will still leave traces of automation, since they do have a common goal (e.', ', increasing someone’s reputation score). By perform- ing analyses at the group level, this emerging trend might be able to significantly raise the bar for social spambots to evade detection. To support the claim, we experiment with work in [13,40].  Tamper Detection in Crowd Computations.', 'The con- tribution by Viswanath et al. in [40] checks whether a given group of accounts (e., retweeters of another account, re- viewers of a venue on Yelp) contains a subset of malicious  Beutel, Faloutsos,et al. [5,18,21,22]Cao et al.', '[9] 2014 Yu et al. [46] 2015  Viswanath, Mislove, 2015 Gummadi et al. [40]Cresci et al. [13] 2016  2013-16  ￼ ￼  ￼ work  key concept  detection of lockstep behaviors  anomalies in synchronicity and normality  detection of loosely synchro- nized actions  detection of latent group anomalies in graphs  distance between distributions of reputation scores  similarity between digital DNA sequences  computed in [13] by measuring the Longest Common Sub- string (LCS)—that is, the longest DNA substring shared by all the accounts of the group.', 'Accounts that share a suspi- ciously long DNA substring are then labeled as spambots. Notably, although working at group level, [13] is capable of spotting single spambot accounts. For this reason, we have been able to compare this technique with the ones previ- ously benchmarked in Table 7. Applying the technique to our datasets, the similarity curve of genuine accounts is sig- nificantly different from that of social spambots, as shown in Figure 4(c).', 'More specifically, as measured by the LCS metric, social spambots #1 and #3 feature a level of sim- ilarity much higher than that of genuine accounts. Results reported in Table 7 demonstrate that the digital DNA-based technique [13] achieves excellent detection performances.  The compelling features of the emerging techniques listed in this section represent a fertile ground for fighting the novel social spambots. We can observe a paradigm-shift for research and development of spambot detection systems, which may exploit such new concepts to achieve better re- silience and robustness, to withstand the next evolution of social media spambots.', 'CONCLUDING REMARKS  Our long-lasting experiment on malicious accounts sur- vival rate in Twitter demonstrated that spambot detection is still an open issue. Moreover, the already difficult prob- lem to detect spambots in social media is bound to worsen, since the emergence of a new wave of so-called social spam- bots. By accurately mimicking the characteristics of gen- uine users, these spambots are intrinsically harder to detect than those studied by Academia in the past years. In our experiments, neither humans nor state-of-the-art spambot detection applications managed to accurately detect the ac- counts belonging to this new wave of spambots.', 'Indeed, our experiments highlighted that the majority of existing au- tomated systems, as well as crowdsourcing, erroneously la- bel social spambots as genuine (human-operated) accounts. We demonstrated the need for novel analytic tools capable of turning the tide in the arms race against such sophis- ticated spambots. One promising research direction stems from the analysis of collective behaviors. We highlighted a few emerging approaches that analyze groups as a whole, rather than individuals.', 'The promising outcome of these novel approaches clearly indicates that this is a favorable research avenue']\n",
            "time: 22.1 ms (started: 2022-01-01 19:44:57 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmyLvgC2iVI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8302481-6018-49d5-8c60-560a0ac72450"
      },
      "source": [
        "# Get a vector for each sentence in the corpus\n",
        "corpus_embeddings = model.encode(corpus)\n",
        "print(corpus_embeddings.shape)\n",
        "\n",
        "print(\"\\n\\n======================\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2840, 512)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "time: 12.1 s (started: 2022-01-18 18:59:59 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBpeVPIoAz6Q",
        "outputId": "81677105-b207-4a4d-9a70-2b7764a16eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-d5cd58f0-e4c2-fcc0-fe5c-34f9583ef29b)\n",
            "time: 188 ms (started: 2022-01-18 18:55:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "h87q_g0Hxhsr",
        "outputId": "1037ffb2-d3b5-4fde-e8ee-5a3fa7178fd1"
      },
      "source": [
        "import pandas as pd\n",
        "seq_len = [len(i.split()) for i in corpus]\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb456459690>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 361 ms (started: 2021-09-07 08:35:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg4Wmr-Kif1o",
        "outputId": "accf860a-50f8-42a9-bda6-33c08ad91ef5"
      },
      "source": [
        "\n",
        "# For each search term return 5 closest sentences\n",
        "import pandas as pd\n",
        "list_of_sentence_and_score = []\n",
        "closest_n = 3\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        "    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n",
        "\n",
        "    results = zip(range(len(distances)), distances)\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    print(\"\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query[0:90])\n",
        "    print(\"\\n\")\n",
        "    print(query[90:])\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    #print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for idx, distance in results[0:closest_n]:\n",
        "        temp = []\n",
        "        temp.append(corpus[idx].strip()[0:295]+\"....\")\n",
        "        temp.append(round((1-distance),4))\n",
        "        list_of_sentence_and_score.append(temp)\n",
        "        #print(corpus[idx].strip(), \"Score:{:.4f}\".format(1-distance))\n",
        "    \n",
        "    df=pd.DataFrame(list_of_sentence_and_score, columns=['Sentence', 'Score'])\n",
        "    print(df.to_markdown())\n",
        "    print(\"\\n\\n======================\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: IoT devices are more prone to security than traditional computers.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "|    | Sentence                                                                                                                                            |   Score |\n",
            "|---:|:----------------------------------------------------------------------------------------------------------------------------------------------------|--------:|\n",
            "|  0 | IoT encounters security problems more than traditional computer networks does.....                                                                  |  0.8313 |\n",
            "|  1 | Meanwhile, it causes that IoT is faced with more serious potential security threats than in the other networks.....                                 |  0.7279 |\n",
            "|  2 | However, IoT has its special security requests other than the traditional problems harming computer, Internet and mobile communication network..... |  0.7197 |\n",
            "|  3 | IoT has some special security requests.....                                                                                                         |  0.7014 |\n",
            "|  4 | CONCLUSION IoT has some special security requests.....                                                                                              |  0.6992 |\n",
            "|  5 | They can not be copied blindly to construct the IoT security system because of the special attributes of IoT.....                                   |  0.6668 |\n",
            "|  6 | The attributes of dispersity and mass of IoT require that approaches to IoT security should be dynamic.....                                         |  0.6302 |\n",
            "|  7 | A specifical approach to IoT security is in urgent need to cope with the complicated and changeful security situation of IoT.....                   |  0.6104 |\n",
            "|  8 | But it has to take the special requests of IoT security into account.....                                                                           |  0.6075 |\n",
            "|  9 | It transforms its strategies of security defense along with the change of the security environment of IoT.....                                      |  0.599  |\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "time: 29.1 ms (started: 2022-01-18 19:04:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "u5g7i1uQi7Sk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}